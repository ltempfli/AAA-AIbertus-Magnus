{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Load-Data\" data-toc-modified-id=\"Imports-and-Load-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and Load Data</a></span></li><li><span><a href=\"#Inspect-Meta-Data\" data-toc-modified-id=\"Inspect-Meta-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Inspect Meta Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Weather_data_raw\" data-toc-modified-id=\"Weather_data_raw-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Weather_data_raw</a></span></li><li><span><a href=\"#Trip-Data-Raw\" data-toc-modified-id=\"Trip-Data-Raw-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Trip Data Raw</a></span></li></ul></li><li><span><a href=\"#Prepare-Weather-Data,-POI-Data-and-supplemental-Data\" data-toc-modified-id=\"Prepare-Weather-Data,-POI-Data-and-supplemental-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare Weather Data, POI Data and supplemental Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-Weather-Data\" data-toc-modified-id=\"Prepare-Weather-Data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Prepare Weather Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Select-relevant-columns\" data-toc-modified-id=\"Select-relevant-columns-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Select relevant columns</a></span></li><li><span><a href=\"#Unpack-/-Format-Columns\" data-toc-modified-id=\"Unpack-/-Format-Columns-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Unpack / Format Columns</a></span></li><li><span><a href=\"#Check-for-Outliers-/-Validity\" data-toc-modified-id=\"Check-for-Outliers-/-Validity-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Check for Outliers / Validity</a></span></li><li><span><a href=\"#Visualize-Data\" data-toc-modified-id=\"Visualize-Data-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Visualize Data</a></span></li></ul></li><li><span><a href=\"#Prepare-POI-Data\" data-toc-modified-id=\"Prepare-POI-Data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Prepare POI Data</a></span></li><li><span><a href=\"#Prepare-Supplemental-Data\" data-toc-modified-id=\"Prepare-Supplemental-Data-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Prepare Supplemental Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-sport-event-data\" data-toc-modified-id=\"Prepare-sport-event-data-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Prepare sport event data</a></span></li></ul></li></ul></li><li><span><a href=\"#Prepare-Trip-Data\" data-toc-modified-id=\"Prepare-Trip-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Prepare Trip Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-spatial-Columns\" data-toc-modified-id=\"Prepare-spatial-Columns-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Prepare spatial Columns</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-granularity\" data-toc-modified-id=\"Check-granularity-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Check granularity</a></span></li><li><span><a href=\"#Check-for-missing-data\" data-toc-modified-id=\"Check-for-missing-data-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Check for missing data</a></span></li><li><span><a href=\"#Convert-to-geopandas-column\" data-toc-modified-id=\"Convert-to-geopandas-column-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Convert to geopandas column</a></span></li><li><span><a href=\"#Create-H3-discretization\" data-toc-modified-id=\"Create-H3-discretization-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Create H3 discretization</a></span></li><li><span><a href=\"#Visualize-Data\" data-toc-modified-id=\"Visualize-Data-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>Visualize Data</a></span></li></ul></li><li><span><a href=\"#Check-for-duplicates\" data-toc-modified-id=\"Check-for-duplicates-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Check for duplicates</a></span></li><li><span><a href=\"#Handling-missing-values\" data-toc-modified-id=\"Handling-missing-values-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Handling missing values</a></span></li><li><span><a href=\"#Check-validity/-handle-outliers\" data-toc-modified-id=\"Check-validity/-handle-outliers-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Check validity/ handle outliers</a></span></li><li><span><a href=\"#Join-Datasets\" data-toc-modified-id=\"Join-Datasets-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Join Datasets</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering-&amp;-Selection\" data-toc-modified-id=\"Feature-Engineering-&amp;-Selection-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Engineering &amp; Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prediction-Task\" data-toc-modified-id=\"Prediction-Task-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Prediction Task</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-different-Time-Bins\" data-toc-modified-id=\"Create-different-Time-Bins-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Create different Time Bins</a></span></li><li><span><a href=\"#Create-Cyclical-Time-Features-with-Sine-and-Cosine-Transformation\" data-toc-modified-id=\"Create-Cyclical-Time-Features-with-Sine-and-Cosine-Transformation-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Create Cyclical Time Features with Sine and Cosine Transformation</a></span></li><li><span><a href=\"#Create-Lagged-Time-Features\" data-toc-modified-id=\"Create-Lagged-Time-Features-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Create Lagged Time Features</a></span></li><li><span><a href=\"#Visualize-Relationship-between-Predictors-and-dependent-Variable\" data-toc-modified-id=\"Visualize-Relationship-between-Predictors-and-dependent-Variable-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Visualize Relationship between Predictors and dependent Variable</a></span></li></ul></li></ul></li><li><span><a href=\"#Export-new-Datasets\" data-toc-modified-id=\"Export-new-Datasets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Export new Datasets</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from h3 import h3\n",
    "#import census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/chicago_taxi_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trip_data_raw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/chicago_taxi_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300000\u001b[39m,\n\u001b[0;32m      5\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayment_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m     },\n\u001b[0;32m      9\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrip_start_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrip_end_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/chicago_taxi_data.csv'"
     ]
    }
   ],
   "source": [
    "trip_data_raw = pd.read_csv(\n",
    "    r'../data/chicago_taxi_data.csv',\n",
    "    index_col=None,\n",
    "    nrows=300000,\n",
    "    dtype={\n",
    "        'payment_type': 'category',\n",
    "        'company': 'category'\n",
    "    },\n",
    "    parse_dates=['trip_start_timestamp', 'trip_end_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_raw = pd.read_csv(r'../data/weather_raw.csv',\n",
    "                               dtype={},\n",
    "                               index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Meta Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Data Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trip_data_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trip_data_raw.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Prepare Weather Data, POI Data and supplemental Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* select relevant columns\n",
    "* unpack/format columns\n",
    "* check for outliers/ validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_data_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_raw.drop(\n",
    "    ['Dew Point', 'Humidity', 'Wind', 'Wind Gust', 'Pressure'],\n",
    "    axis=1,\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_raw.head(25)\n",
    "# Problem row 23 should be 2018-01-02 -> lists first of the next day in table of the previous day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack / Format Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_columns_weather(row):\n",
    "\n",
    "    return pd.Series(data={\n",
    "        'date': extract_time_weather(row['date'], row['Time']),\n",
    "        'temperature': format_temperature(row['Temperature']),\n",
    "        'wind_speed': format_wind(row['Wind Speed']),\n",
    "        'condition': row['Condition'],\n",
    "        'precipitation': format_precipitation(row['Precip.'])\n",
    "    },\n",
    "                     index=None)\n",
    "\n",
    "\n",
    "def extract_time_weather(date_cell, Time_cell):\n",
    "\n",
    "    date = pd.to_datetime(date_cell + \"-\" + Time_cell,\n",
    "                          format='%Y-%m-%d-%I:%M %p')\n",
    "\n",
    "    return date\n",
    "\n",
    "\n",
    "def format_temperature(temp_cell):\n",
    "\n",
    "    temp_cell = temp_cell.replace('°F', \"\")\n",
    "    temp_cell = temp_cell.strip()\n",
    "\n",
    "    return (int(temp_cell) - 32) * 5 / 9  # transform Fahrenheit into celsius\n",
    "\n",
    "\n",
    "def format_wind(wind_cell):\n",
    "\n",
    "    return int(wind_cell.replace(\"mph\", \"\").strip())\n",
    "\n",
    "\n",
    "def format_precipitation(precip_cell):\n",
    "    return float(precip_cell.replace(\"in\", \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.DataFrame(weather_data_raw.apply(\n",
    "    lambda row: format_columns_weather(row), axis=1),\n",
    "                            index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Outliers / Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sicherstellen, dass die 'date'-Spalte datetime-Objekte enthält\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Erstellung der Unterplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 16))\n",
    "\n",
    "# Erster Plot: Temperatur-Zeitreihe\n",
    "ax[0].grid(True, color='gray')\n",
    "sns.lineplot(data=weather_data, x='date', y='temperature', ax=ax[0])\n",
    "ax[0].set_xlabel('Date')\n",
    "ax[0].set_ylabel('Temperature (Celsius)')\n",
    "\n",
    "# Zweiter Plot: Wöchentliche Niederschlagsmittelwerte\n",
    "# Sicherstellen, dass die 'precipitation'-Spalte numerisch ist\n",
    "visualize_precipitation = weather_data[['date', 'precipitation']].set_index('date').resample('W').mean().reset_index()\n",
    "\n",
    "sns.barplot(data=visualize_precipitation, x='date', y='precipitation', ax=ax[1])\n",
    "ax[1].set_ylabel('Average Precipitation per Week')\n",
    "ax[1].set_xlabel('Date')\n",
    "ax[1].set_xticks(ax[1].get_xticks()[::4])\n",
    "ax[1].set_xticklabels(visualize_precipitation['date'].dt.strftime('%Y-%m-%d')[::4], rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare POI Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Supplemental Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* select relevant columns\n",
    "* unpack/format columns\n",
    "* join datasets & create features from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sport event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import from data/Sports_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Trip Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* decide which geographical column(s) for spatial analysis/ feature\n",
    "* handle missing values\n",
    "* check for outliers\n",
    "* check for data validity\n",
    "* prepare spatial data\n",
    "* normalize data\n",
    "* cut into taskspecific datasets & export new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare spatial Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=4>Pickup/Dropoff_census_tract</font>**\n",
    "\n",
    "**Definition of ChatGPT:** \"A census tract is a geographic area defined by the United States Census Bureau for the purpose of taking the census. Census tracts are relatively small, permanent subdivisions of a county or equivalent entity that are updated by local participants prior to each decennial census as part of the Census Bureau's Participant Statistical Areas Program.\"\n",
    "\n",
    "* **Demographic Analysis:** Census tracts are often associated with specific demographic characteristics, such as income levels, population density, and racial composition. Analyzing taxi trip data in conjunction with census tract demographics can provide insights into the socioeconomic factors influencing travel patterns.\n",
    "\n",
    "* --> install CensusData package to obtain demographic data\n",
    "\n",
    "* --> demographic data interesting for prediction task? --> a lot of missing data compared to other spatial columns, so that other for prediction\n",
    "\n",
    "**<font size=4>Longitude/ Latitude Columns</font>**\n",
    "\n",
    "* containts the same data as the location columns but in more columns\n",
    "* --> are more efficient in storing \n",
    "* --> however, more missing values compared to the location column\n",
    "* --> discrepancy between location and longitude / latitude column\n",
    "     * --> maybe other granualrity?\n",
    "     \n",
    "**<font size=4>Centroid Location Column</font>**\n",
    "\n",
    "* contains a point which consists for the longitude and latitude -> has the least missing values\n",
    "* -> candidate for prediciton/ cluster task?\n",
    "\n",
    "\n",
    "-> see if spatial data is on the same granularity: if yes fill missing values of the other coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trip_data_raw['pickup_centroid_latitude'].nunique())\n",
    "print(trip_data_raw['pickup_centroid_location'].nunique())\n",
    "print(trip_data_raw['pickup_census_tract'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census tract data seem to be of a finer grid than the rest of the data -> difficult to map from the locaton_data to the census tract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(trip_data_raw.loc[\n",
    "        ~pd.isna(trip_data_raw['pickup_centroid_latitude'])\n",
    "        ^ ~pd.isna(trip_data_raw['pickup_centroid_longitude'])]),\n",
    "    \"number of columns were either pickup_centroid_latitude xor pickup_centroid_longitude is filled\\n\"\n",
    ")\n",
    "print(\n",
    "    len(trip_data_raw.loc[\n",
    "        ~pd.isna(trip_data_raw['pickup_centroid_latitude'])\n",
    "        ^ ~pd.isna(trip_data_raw['pickup_centroid_location'])]),\n",
    "    \"number of columns were either pickup_centroid_latitude xor pickup_centroid_location is filled\\n\"\n",
    ")\n",
    "print(\n",
    "    len(trip_data_raw.loc[\n",
    "        ~pd.isna(trip_data_raw['pickup_centroid_latitude'])\n",
    "        & ~pd.isna(trip_data_raw['pickup_centroid_location'])]),\n",
    "    \"number of columns were either pickup_centroid_latitude and pickup_centroid_longitude and pickup_centroid_location are filled\\n\"\n",
    ")\n",
    "print(\n",
    "    len(trip_data_raw.loc[pd.isna(trip_data_raw['pickup_centroid_latitude'])\n",
    "                          & ~pd.isna(trip_data_raw['pickup_census_tract'])]),\n",
    "    \"number of columns were pickup_centroid_latitude and pickup_centroid_longitude and pickup_centroid_location are not filled but census trackt\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    len(trip_data_raw.loc[~pd.isna(trip_data_raw['pickup_centroid_latitude'])\n",
    "                          & pd.isna(trip_data_raw['pickup_census_tract'])]),\n",
    "    \"number of columns were pickup_centroid_latitude and pickup_centroid_longitude and pickup_centroid_location are filled but census trackt not\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(trip_data_raw.loc[\n",
    "        ~pd.isna(trip_data_raw['dropoff_centroid_latitude'])\n",
    "        ^ ~pd.isna(trip_data_raw['dropoff_centroid_longitude'])]),\n",
    "    \"number of columns were either dropoff_centroid_latitude xor dropoff_centroid_longitude is filled\\n\"\n",
    ")\n",
    "print(\n",
    "    len(trip_data_raw.loc[\n",
    "        ~pd.isna(trip_data_raw['dropoff_centroid_latitude'])\n",
    "        ^ ~pd.isna(trip_data_raw['dropoff_centroid_location'])]),\n",
    "    \"number of columns were either dropoff_centroid_latitude xor dropoff_centroid_location is filled\\n\"\n",
    ")\n",
    "print(\n",
    "    len(trip_data_raw.loc[\n",
    "        ~pd.isna(trip_data_raw['dropoff_centroid_latitude'])\n",
    "        & ~pd.isna(trip_data_raw['dropoff_centroid_location'])]),\n",
    "    \"number of columns were either dropoff_centroid_latitude and dropoff_centroid_longitude and dropoff_centroid_location are filled\\n\"\n",
    ")\n",
    "print(\n",
    "    len(trip_data_raw.loc[pd.isna(trip_data_raw['dropoff_centroid_latitude'])\n",
    "                          & ~pd.isna(trip_data_raw['dropoff_census_tract'])]),\n",
    "    \"number of columns were dropoff_centroid_latitude and dropoff_centroid_longitude and dropoff_centroid_location are not filled but census trackt\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    len(trip_data_raw.loc[~pd.isna(trip_data_raw['dropoff_centroid_latitude'])\n",
    "                          & pd.isna(trip_data_raw['dropoff_census_tract'])]),\n",
    "    \"number of columns were dropoff_centroid_latitude and dropoff_centroid_longitude and dropoff_centroid_location are filled but census trackt %notebook\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "* If centroid_location is NaN so are centroid_latitude and centroide_longitude and the other way around\n",
    "* However, there is census tract data where latitude, longitude and location data is NaN. But is not much so that can be    neglected and the NaN rows can be deleted\n",
    "* there is more pickoff data than dropoff -> dropoff is not need for demand prediction task\n",
    "* around ~2 Millionen rows have missing location data: 10% of the overall dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop missing geographical data**\n",
    "\n",
    "as described above in section 4.1 we cannot recover the missing data (expect for the ~40000) \n",
    "so we decided to drop the NaN columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as described above in section 4.1 we cannot recover the missing data (expect for the ~40000)\n",
    "# so we decided to drop the NaN columns\n",
    "\n",
    "trip_data_raw.dropna(subset=['dropoff_centroid_location'], inplace=True)\n",
    "trip_data_raw.dropna(subset=['pickup_centroid_location'], inplace=True)\n",
    "print(len(trip_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_raw.drop([\n",
    "    'dropoff_centroid_latitude', 'pickup_centroid_latitude',\n",
    "    'pickup_centroid_longitude', 'dropoff_centroid_longitude'\n",
    "],\n",
    "                   axis=1,\n",
    "                   inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to geopandas column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_raw['dropoff_centroid_location'] = trip_data_raw[\n",
    "    'dropoff_centroid_location'].apply(\n",
    "        lambda x: Point(ast.literal_eval(x).get('coordinates')))\n",
    "trip_data_raw['pickup_centroid_location'] = trip_data_raw[\n",
    "    'pickup_centroid_location'].apply(\n",
    "        lambda x: Point(ast.literal_eval(x).get('coordinates')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_raw_geo = gpd.GeoDataFrame(trip_data_raw,\n",
    "                                     geometry='pickup_centroid_location')\n",
    "trip_data_raw_geo.set_geometry('dropoff_centroid_location', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create H3 discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_raw_geo['h3_res_6'] = trip_data_raw_geo.apply(\n",
    "    lambda row: h3.geo_to_h3(lat=row['pickup_centroid_location'].y,\n",
    "                             lng=row['pickup_centroid_location'].x,\n",
    "                             resolution=6),\n",
    "    axis=1)\n",
    "\n",
    "trip_data_raw_geo['h3_res_4'] = trip_data_raw_geo.apply(\n",
    "    lambda row: h3.geo_to_h3(lat=row['pickup_centroid_location'].y,\n",
    "                             lng=row['pickup_centroid_location'].x,\n",
    "                             resolution=4),\n",
    "    axis=1)\n",
    "\n",
    "trip_data_raw_geo['h3_res_8'] = trip_data_raw_geo.apply(\n",
    "    lambda row: h3.geo_to_h3(lat=row['pickup_centroid_location'].y,\n",
    "                             lng=row['pickup_centroid_location'].x,\n",
    "                             resolution=8),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trip_data_raw_geo['h3_res_4'].nunique())\n",
    "print(trip_data_raw_geo['h3_res_6'].nunique())\n",
    "print(trip_data_raw_geo['h3_res_8'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "figure_pickup_location = trip_data_raw_geo[['pickup_centroid_location']]\n",
    "figure_dropoff_location = trip_data_raw_geo[['dropoff_centroid_location']]\n",
    "\n",
    "figure_pickup_location = figure_pickup_location.set_geometry(\n",
    "    'pickup_centroid_location')\n",
    "figure_dropoff_location = figure_dropoff_location.set_geometry(\n",
    "    'dropoff_centroid_location')\n",
    "\n",
    "figure_pickup_location.crs = 'epsg:4326'\n",
    "figure_dropoff_location.crs = 'epsg:4326'\n",
    "\n",
    "figure_pickup_location = figure_pickup_location.to_crs(epsg=3857)\n",
    "figure_dropoff_location = figure_dropoff_location.to_crs(epsg=3857)\n",
    "\n",
    "figure_pickup_location.plot(color='green', ax=ax, marker='o', markersize=7)\n",
    "figure_dropoff_location.plot(color='blue',\n",
    "                             ax=ax,\n",
    "                             marker='o',\n",
    "                             markersize=7,\n",
    "                             alpha=0.01)\n",
    "\n",
    "#ctx.add_basemap(ax=ax, zoom=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_hexagons_6 = trip_data_raw_geo[['h3_res_6']].copy()\n",
    "visualize_hexagons_6['number'] = 1\n",
    "\n",
    "visualize_hexagons_6 = visualize_hexagons_6.groupby(['h3_res_6'],\n",
    "                                                    as_index=False).sum()\n",
    "\n",
    "visualize_hexagons_4 = trip_data_raw_geo[['h3_res_4']].copy()\n",
    "visualize_hexagons_4['number'] = 1\n",
    "\n",
    "visualize_hexagons_4 = visualize_hexagons_4.groupby(['h3_res_4'],\n",
    "                                                    as_index=False).sum()\n",
    "\n",
    "visualize_hexagons_8 = trip_data_raw_geo[['h3_res_8']].copy()\n",
    "visualize_hexagons_8['number'] = 1\n",
    "\n",
    "visualize_hexagons_8 = visualize_hexagons_8.groupby(['h3_res_8'],\n",
    "                                                    as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_hexagons_6['h3_res_6_geo'] = visualize_hexagons_6['h3_res_6'].apply(\n",
    "    lambda x: shape({\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': [h3.h3_to_geo_boundary(x, geo_json=True)],\n",
    "        'properties': ''\n",
    "    }))\n",
    "visualize_hexagons_4['h3_res_4_geo'] = visualize_hexagons_4['h3_res_4'].apply(\n",
    "    lambda x: shape({\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': [h3.h3_to_geo_boundary(x, geo_json=True)],\n",
    "        'properties': ''\n",
    "    }))\n",
    "visualize_hexagons_8['h3_res_8_geo'] = visualize_hexagons_8['h3_res_8'].apply(\n",
    "    lambda x: shape({\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': [h3.h3_to_geo_boundary(x, geo_json=True)],\n",
    "        'properties': ''\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_hexagons_6 = visualize_hexagons_6.set_geometry('h3_res_6_geo')\n",
    "visualize_hexagons_6.crs = 'epsg:4326'\n",
    "\n",
    "visualize_hexagons_6 = visualize_hexagons_6.to_crs(epsg=3857)\n",
    "\n",
    "visualize_hexagons_4 = visualize_hexagons_4.set_geometry('h3_res_4_geo')\n",
    "visualize_hexagons_4.crs = 'epsg:4326'\n",
    "\n",
    "visualize_hexagons_4 = visualize_hexagons_4.to_crs(epsg=3857)\n",
    "\n",
    "visualize_hexagons_8 = visualize_hexagons_8.set_geometry('h3_res_8_geo')\n",
    "visualize_hexagons_8.crs = 'epsg:4326'\n",
    "\n",
    "visualize_hexagons_8 = visualize_hexagons_8.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
    "\n",
    "visualize_hexagons_6.set_geometry('h3_res_6_geo').plot(ax=ax[0, 0],\n",
    "                                                       edgecolor='black',\n",
    "                                                       linewidth=1.0,\n",
    "                                                       alpha=0.3)\n",
    "\n",
    "figure_pickup_location.plot(color='green',\n",
    "                            ax=ax[0, 0],\n",
    "                            marker='o',\n",
    "                            markersize=7)\n",
    "\n",
    "#ctx.add_basemap(ax=ax[0,0], zoom=12)\n",
    "\n",
    "visualize_hexagons_6.set_geometry('h3_res_6_geo').plot(ax=ax[0, 1],\n",
    "                                                       edgecolor='black',\n",
    "                                                       linewidth=1.0,\n",
    "                                                       column='number',\n",
    "                                                       cmap='Reds',\n",
    "                                                       alpha=0.6)\n",
    "\n",
    "#ctx.add_basemap(ax=ax[0,1], zoom=12)\n",
    "\n",
    "visualize_hexagons_8.set_geometry('h3_res_8_geo').plot(ax=ax[1, 0],\n",
    "                                                       edgecolor='black',\n",
    "                                                       linewidth=1.0,\n",
    "                                                       alpha=0.3)\n",
    "\n",
    "#ctx.add_basemap(ax=ax[1,0], zoom=12)\n",
    "\n",
    "visualize_hexagons_4.set_geometry('h3_res_4_geo').plot(ax=ax[1, 1],\n",
    "                                                       edgecolor='black',\n",
    "                                                       linewidth=1.0,\n",
    "                                                       alpha=0.3)\n",
    "\n",
    "#ctx.add_basemap(ax=ax[1,1], zoom=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resolution of 6 seems reasonable, a resolution of 4 maybe to coarse and a resolution of 8 almost to fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trip_data_raw_geo[\"trip_id\"].nunique()\n",
    "      )  #20695512 entries in whole dataset -> no duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Trip stimestamp**\n",
    "only 349 rows so they can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in trip_data_raw_geo.columns:\n",
    "\n",
    "    print(f\"NaN Values in {element}:\",\n",
    "          trip_data_raw_geo[element].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check validity/ handle outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_raw_geo.sort_values(by='trip_start_timestamp', inplace=True)\n",
    "weather_data.sort_values(by='date', inplace=True)\n",
    "\n",
    "merged_data = pd.merge_asof(left=trip_data_raw_geo,\n",
    "                            right=weather_data.loc[:, ('date', 'temperature',\n",
    "                                                       'precipitation')],\n",
    "                            left_on='trip_start_timestamp',\n",
    "                            right_on='date',\n",
    "                            direction='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the relevant features\n",
    "prediction_data_raw = merged_data.loc[:, ('trip_start_timestamp', 'h3_res_4',\n",
    "                                          'h3_res_6', 'h3_res_8',\n",
    "                                          'temperature', 'precipitation')]\n",
    "prediction_data_raw[\"number_of_trips\"] = 1\n",
    "prediction_data_raw['weekday'] = prediction_data_raw[\n",
    "    'trip_start_timestamp'].apply(lambda x: x.day)\n",
    "prediction_data_raw['month'] = prediction_data_raw[\n",
    "    'trip_start_timestamp'].apply(lambda x: x.month)\n",
    "\n",
    "time_periods = [1, 2, 6, 24] # time bins we want to predict the demand for\n",
    "resolution = ['h3_res_4', 'h3_res_6', 'h3_res_8'] # spatial resolution we want to predict the demand for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create different Time Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = {}\n",
    "for period in time_periods:\n",
    "    dict_time_bucket = {}\n",
    "    for res in resolution:\n",
    "        df = prediction_data_raw.loc[:, ('trip_start_timestamp', 'temperature',\n",
    "                                         'precipitation', 'number_of_trips',\n",
    "                                         'weekday', 'month', res)]\n",
    "        df['trip_start_timestamp'] = df['trip_start_timestamp'].apply(\n",
    "            lambda x: x.replace(minute=0, hour=(x.hour // period) * period))\n",
    "        df['hour'] = df['trip_start_timestamp'].apply(lambda x: x.hour)\n",
    "        df = df.groupby(['trip_start_timestamp', res], as_index=True).agg({\n",
    "            'temperature':\n",
    "            'mean',\n",
    "            'precipitation':\n",
    "            'mean',\n",
    "            'number_of_trips':\n",
    "            'sum',\n",
    "            'weekday':\n",
    "            'mean',\n",
    "            'month':\n",
    "            'mean',\n",
    "            'hour':\n",
    "            'mean'\n",
    "        })\n",
    "        df.reset_index(level=res, inplace=True)\n",
    "        dict_time_bucket[res] = df\n",
    "\n",
    "    prediction_data[period] = dict_time_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data.get(1).get('h3_res_4').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cyclical Time Features with Sine and Cosine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for periods in prediction_data.keys():\n",
    "    for res in prediction_data.get(periods).keys():\n",
    "        df = prediction_data.get(periods).get(res)\n",
    "        if (periods != 24):\n",
    "            df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / df[\"hour\"].max())\n",
    "            df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / df[\"hour\"].max())\n",
    "\n",
    "        df[\"weekday_sin\"] = np.sin(2 * np.pi * df[\"weekday\"] /\n",
    "                                   df[\"weekday\"].max())\n",
    "        df[\"weekday_cos\"] = np.cos(2 * np.pi * df[\"weekday\"] /\n",
    "                                   df[\"weekday\"].max())\n",
    "\n",
    "        df[\"weekday_sin\"] = np.sin(2 * np.pi * df[\"month\"] / df[\"month\"].max())\n",
    "        df[\"weekday_cos\"] = np.cos(2 * np.pi * df[\"month\"] / df[\"month\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data.get(1).get('h3_res_4').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lagged Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lagged_column(row_index, area, area_column):\n",
    "    if row_index in df.index:\n",
    "        trips = df.loc[(df.index == row_index) & (df[area_column] == area),\n",
    "                       'number_of_trips']\n",
    "        if len(trips) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return trips[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for periods in prediction_data.keys():\n",
    "    for res in prediction_data.get(periods).keys():\n",
    "        df = prediction_data.get(periods).get(res)\n",
    "        if periods != 24:\n",
    "            df[\"lagged_\" + str(periods) +\n",
    "               \"h\"] = df.apply(lambda row: get_lagged_column(\n",
    "                   row.name - pd.Timedelta(hours=periods), row[res], res),\n",
    "                               axis=1)\n",
    "        df[\"lagged_1day\"] = df.apply(lambda row: get_lagged_column(\n",
    "            row.name - pd.Timedelta(days=1), row[res], res),\n",
    "                                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data.get(1).get('h3_res_4').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Relationship between Predictors and dependent Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prediction_data.get(1).get('h3_res_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)\n",
    "features.remove('number_of_trips')\n",
    "features.remove( 'h3_res_4')\n",
    "fig, ax = plt.subplots(len(features),2, figsize=(14,99))\n",
    "for i in range(0, len(features)):\n",
    "    features_grouped = df[[features[i],'number_of_trips']].groupby([features[i]], as_index=False).mean()\n",
    "    sns.scatterplot(x=df[features[i]], y=df['number_of_trips'], ax=ax[i,0])\n",
    "    sns.lineplot(x=features_grouped[features[i]], y=features_grouped['number_of_trips'], ax=ax[i,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export new Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for periods in prediction_data.keys():\n",
    "    for res in prediction_data.get(period).keys():\n",
    "        prediction_data.get(periods).get(res).to_csv(f'../data/{periods}hours_{res}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "870.398px",
    "left": "41.9801px",
    "top": "110.795px",
    "width": "431.804px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
