{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tasks\" data-toc-modified-id=\"Tasks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Tasks</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#List-available-GPUs\" data-toc-modified-id=\"List-available-GPUs-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>List available GPUs</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Transformations\" data-toc-modified-id=\"Data-Transformations-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Data Transformations</a></span></li><li><span><a href=\"#Test/Train-split-Techniques\" data-toc-modified-id=\"Test/Train-split-Techniques-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Test/Train split Techniques</a></span></li></ul></li><li><span><a href=\"#Additional-Pre-Processing\" data-toc-modified-id=\"Additional-Pre-Processing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Additional Pre-Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deploy-One-Hot-Encoding\" data-toc-modified-id=\"Deploy-One-Hot-Encoding-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Deploy One-Hot-Encoding</a></span></li><li><span><a href=\"#Create-Feature_Sets\" data-toc-modified-id=\"Create-Feature_Sets-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Create Feature_Sets</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Support Vector Machines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Perform-Grid-search\" data-toc-modified-id=\"Perform-Grid-search-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Perform Grid search</a></span></li><li><span><a href=\"#Train-the-models-with-different-time/spatial\" data-toc-modified-id=\"Train-the-models-with-different-time/spatial-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Train the models with different time/spatial</a></span><ul class=\"toc-item\"><li><span><a href=\"#Batch-Split-k-fold-Cross-Validation\" data-toc-modified-id=\"Batch-Split-k-fold-Cross-Validation-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Batch Split k-fold Cross Validation</a></span></li><li><span><a href=\"#Sliding-Window-cross-Validation\" data-toc-modified-id=\"Sliding-Window-cross-Validation-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Sliding Window cross Validation</a></span></li><li><span><a href=\"#Expanding-Window-Split\" data-toc-modified-id=\"Expanding-Window-Split-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>Expanding Window Split</a></span></li><li><span><a href=\"#Start-End-Split\" data-toc-modified-id=\"Start-End-Split-7.2.4\"><span class=\"toc-item-num\">7.2.4&nbsp;&nbsp;</span>Start End Split</a></span></li></ul></li><li><span><a href=\"#Compare-the-results-of-the-different-time/spaitla-bin-combinations\" data-toc-modified-id=\"Compare-the-results-of-the-different-time/spaitla-bin-combinations-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Compare the results of the different time/spaitla bin combinations</a></span></li><li><span><a href=\"#Analyize-the-models-with-XAI-methods\" data-toc-modified-id=\"Analyize-the-models-with-XAI-methods-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Analyize the models with XAI methods</a></span></li></ul></li><li><span><a href=\"#Neural-Networks\" data-toc-modified-id=\"Neural-Networks-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Grid Search</a></span></li><li><span><a href=\"#Train-the-models-with-different-time/spatial-bins-and-different-test/train-spilt-techniques\" data-toc-modified-id=\"Train-the-models-with-different-time/spatial-bins-and-different-test/train-spilt-techniques-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Train the models with different time/spatial bins and different test/train spilt techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Batch-Split-k-fold-Cross-Validation\" data-toc-modified-id=\"Batch-Split-k-fold-Cross-Validation-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>Batch Split k-fold Cross Validation</a></span></li><li><span><a href=\"#Sliding-Window-cross-Validation\" data-toc-modified-id=\"Sliding-Window-cross-Validation-8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;</span>Sliding Window cross Validation</a></span></li><li><span><a href=\"#Expanding-Window-Split\" data-toc-modified-id=\"Expanding-Window-Split-8.2.3\"><span class=\"toc-item-num\">8.2.3&nbsp;&nbsp;</span>Expanding Window Split</a></span></li><li><span><a href=\"#Start-End-Split\" data-toc-modified-id=\"Start-End-Split-8.2.4\"><span class=\"toc-item-num\">8.2.4&nbsp;&nbsp;</span>Start End Split</a></span></li></ul></li><li><span><a href=\"#Compare-the-results-of--the-different-time/spaitla-bin-combinations\" data-toc-modified-id=\"Compare-the-results-of--the-different-time/spaitla-bin-combinations-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Compare the results of  the different time/spaitla bin combinations</a></span></li><li><span><a href=\"#Compare-the-different-test/train-split-techniques\" data-toc-modified-id=\"Compare-the-different-test/train-split-techniques-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Compare the different test/train split techniques</a></span></li><li><span><a href=\"#Aanalyize-the-models-with-XAI-methods\" data-toc-modified-id=\"Aanalyize-the-models-with-XAI-methods-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Aanalyize the models with XAI methods</a></span></li><li><span><a href=\"#Apply-different-NN-architectures\" data-toc-modified-id=\"Apply-different-NN-architectures-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>Apply different NN architectures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convolutional-NN-(CNN)\" data-toc-modified-id=\"Convolutional-NN-(CNN)-8.6.1\"><span class=\"toc-item-num\">8.6.1&nbsp;&nbsp;</span>Convolutional NN (CNN)</a></span></li><li><span><a href=\"#Recurrent-NN-(RNN)\" data-toc-modified-id=\"Recurrent-NN-(RNN)-8.6.2\"><span class=\"toc-item-num\">8.6.2&nbsp;&nbsp;</span>Recurrent NN (RNN)</a></span></li><li><span><a href=\"#Long-Short-Term-Memory-Networks-(LSTM)\" data-toc-modified-id=\"Long-Short-Term-Memory-Networks-(LSTM)-8.6.3\"><span class=\"toc-item-num\">8.6.3&nbsp;&nbsp;</span>Long Short-Term Memory Networks (LSTM)</a></span></li><li><span><a href=\"#Kernalized-NN\" data-toc-modified-id=\"Kernalized-NN-8.6.4\"><span class=\"toc-item-num\">8.6.4&nbsp;&nbsp;</span>Kernalized NN</a></span></li><li><span><a href=\"#Comparison-of-the-results\" data-toc-modified-id=\"Comparison-of-the-results-8.6.5\"><span class=\"toc-item-num\">8.6.5&nbsp;&nbsp;</span>Comparison of the results</a></span></li></ul></li></ul></li><li><span><a href=\"#Compare-SVM-and-NN-models-in-terms-of-predictive-performance-and-computation-time\" data-toc-modified-id=\"Compare-SVM-and-NN-models-in-terms-of-predictive-performance-and-computation-time-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Compare SVM and NN models in terms of predictive performance and computation time</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "* Load and train NN Grid Seach params\n",
    "* properly save best params\n",
    "* rearrange methods\n",
    "* measure time when using GridSearch\n",
    "* create Matrix Graph from the produced scores\n",
    "* free memory when creating objects in Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from dask_ml.model_selection import GridSearchCV as DaskGridSearchCV\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import delayed\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import time\n",
    "\n",
    "#import shap\n",
    "#import lime\n",
    "\n",
    "import joblib\n",
    "\n",
    "import math\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "#print(\"Num GPUs Available:\", len([x for x in device_lib.list_local_devices() if x.device_type == 'GPU']))\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "WARNING:tensorflow:From C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_25912\\2177123679.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods = [1, 2, 6, 24] # time bins we want to predict the demand for\n",
    "resolution = ['h3_res_4', 'h3_res_6', 'h3_res_8'] # spatial resolution we want to predict the demand for\n",
    "\n",
    "prediction_data={}\n",
    "for period in time_periods:\n",
    "    res_data={}\n",
    "    for res in resolution:\n",
    "        res_data[res]=pd.read_csv(f'../data/{period}hours_{res}.csv', \n",
    "                                  parse_dates=['trip_start_timestamp'],\n",
    "                                  #index_col=\"trip_start_timestamp\"\n",
    "                                 )\n",
    "    prediction_data[period]=res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>h3_res_4</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>number_of_trips</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>poi_h3_res_4</th>\n",
       "      <th>sports_event_h3</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>lagged_1day</th>\n",
       "      <th>lagged_1h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02 13:00:00</td>\n",
       "      <td>8426641ffffffff</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.398401</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03 08:00:00</td>\n",
       "      <td>8426641ffffffff</td>\n",
       "      <td>-9.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.816970</td>\n",
       "      <td>-0.576680</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03 14:00:00</td>\n",
       "      <td>8426641ffffffff</td>\n",
       "      <td>-8.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-03 17:00:00</td>\n",
       "      <td>8426641ffffffff</td>\n",
       "      <td>-8.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.997669</td>\n",
       "      <td>-0.068242</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-04 06:00:00</td>\n",
       "      <td>8426641ffffffff</td>\n",
       "      <td>-16.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.997669</td>\n",
       "      <td>-0.068242</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trip_start_timestamp         h3_res_4  temperature  precipitation  \\\n",
       "0  2018-01-02 13:00:00  8426641ffffffff   -15.000000            0.0   \n",
       "1  2018-01-03 08:00:00  8426641ffffffff    -9.444444            0.0   \n",
       "2  2018-01-03 14:00:00  8426641ffffffff    -8.333333            0.0   \n",
       "3  2018-01-03 17:00:00  8426641ffffffff    -8.888889            0.0   \n",
       "4  2018-01-04 06:00:00  8426641ffffffff   -16.111111            0.0   \n",
       "\n",
       "   number_of_trips  weekday  month  hour  poi_h3_res_4  sports_event_h3  \\\n",
       "0                1      1.0    1.0  13.0             9              0.0   \n",
       "1                1      2.0    1.0   8.0             9              0.0   \n",
       "2                1      2.0    1.0  14.0             9              0.0   \n",
       "3                1      2.0    1.0  17.0             9              0.0   \n",
       "4                1      3.0    1.0   6.0             9              0.0   \n",
       "\n",
       "   hour_sin  hour_cos   weekday_sin  weekday_cos  month_sin  month_cos  \\\n",
       "0 -0.398401 -0.917211  8.660254e-01          0.5        0.5   0.866025   \n",
       "1  0.816970 -0.576680  8.660254e-01         -0.5        0.5   0.866025   \n",
       "2 -0.631088 -0.775711  8.660254e-01         -0.5        0.5   0.866025   \n",
       "3 -0.997669 -0.068242  8.660254e-01         -0.5        0.5   0.866025   \n",
       "4  0.997669 -0.068242  1.224647e-16         -1.0        0.5   0.866025   \n",
       "\n",
       "   lagged_1day  lagged_1h  \n",
       "0            0          0  \n",
       "1            0          1  \n",
       "2            0          1  \n",
       "3            0          1  \n",
       "4            0          1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data.get(1).get('h3_res_4').head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, Y_train, X_test, Y_test):\n",
    "    '''\n",
    "    Method that prepares the data for training (split the data/)\n",
    "    param X: feature data to be prepared\n",
    "    param Y: target data to be prepared\n",
    "    param train_index: index that defines the split for trainig data\n",
    "    param val_index: index that defines the split for target data\n",
    "    returns X_train, Y_train, X_val, Y_val: prepared training & validation data\n",
    "    '''\n",
    "    Scaler=StandardScaler()\n",
    "    \n",
    "    X_train = Scaler.fit_transform(X_train)\n",
    "    Y_train = Scaler.fit_transform(Y_train)\n",
    "    \n",
    "    X_test = Scaler.fit_transform(X_test)\n",
    "    Y_test = Scaler.fit_transform(Y_test)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def train_lsvr(X_train, Y_train, X_val, Y_val, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a SVM with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   SVM to be trained\n",
    "    param params:  Parameters to train the SVM\n",
    "    returns:       Nothing    \n",
    "    '''\n",
    "    model.set_params(\n",
    "              epsilon = params.get('regressor__model__epsilon'),\n",
    "              C = params.get('regressor__model__C'),\n",
    "              max_iter = 5000\n",
    "    )\n",
    "    model.fit(X_train, \n",
    "              Y_train.reshape(len(Y_train))\n",
    "             )\n",
    "    Y_pred = model.predict(X_val)\n",
    "    r2 = r2_score(Y_val, Y_pred)\n",
    "    MAE = mean_absolute_error(Y_val, Y_pred)\n",
    "    print(f\"R-squared {r2}\")\n",
    "    print(f\"Mean Squared Error {MAE}\")\n",
    "\n",
    "    return r2, MAE\n",
    "    \n",
    "    \n",
    "def train_ksvr(X_train, Y_train, X_val, Y_val, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a SVM with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   SVM to be trained\n",
    "    param params:  Parameters to train the SVM\n",
    "    returns:       Nothing    \n",
    "    '''\n",
    "    print(params)\n",
    "    model.set_params(\n",
    "              epsilon = params.get('regressor__model__epsilon'),\n",
    "              C = params.get('regressor__model__C'),\n",
    "              kernel = params.get('regressor__model__kernel'),\n",
    "              gamma = params.get('regressor__model__gamma'),\n",
    "              coef0 = params.get('regressor__model__coef0')\n",
    "    )\n",
    "    model.fit(X_train, \n",
    "              Y_train.reshape(len(Y_train))\n",
    "             )\n",
    "    Y_pred = model.predict(X_val)\n",
    "    r2 = r2_score(Y_val, Y_pred)\n",
    "    MAE = mean_absolute_error(Y_val, Y_pred)\n",
    "    print(f\"R-squared {r2}\")\n",
    "    print(f\"Mean Squared Error {MAE}\")\n",
    "\n",
    "    return r2, MAE\n",
    "\n",
    "\n",
    "def create_date_list(delta):\n",
    "    '''\n",
    "    This method creates a list of dates (2018-2019) \n",
    "    depending on the delta\n",
    "    params delta: determines break between dates\n",
    "    returns:      list of dates \n",
    "    '''\n",
    "    # create a startdate\n",
    "    start = pd.to_datetime(\"2018-01-01\", format=\"%Y-%m-%d\")\n",
    "    # create the enddate\n",
    "    #end = pd.to_datetime(\"2018-12-31\", format=\"%Y-%m-%d\")\n",
    "    end = pd.to_datetime(\"2018-01-20\", format=\"%Y-%m-%d\")\n",
    "    # create timedelta to increase days\n",
    "    next_date = timedelta(days=delta)\n",
    "    list_dates=[]\n",
    "    \n",
    "    while start <= end:\n",
    "        \n",
    "        # add date to list\n",
    "        list_dates.append(start)\n",
    "        # increase date by one day\n",
    "        start = start + next_date\n",
    "    list_dates.append(end)\n",
    "    \n",
    "    return list_dates\n",
    "\n",
    "\n",
    "def create_batch_split(X,Y, date_list, arr):\n",
    "    '''\n",
    "    '''\n",
    "    X_train = X.copy()\n",
    "    Y_train = Y.copy()\n",
    "    X_test = pd.DataFrame()\n",
    "    Y_test = pd.DataFrame()\n",
    "    # Sort the arr to be able to delete entries with the index\n",
    "    arr = sorted(arr, reverse=True)\n",
    "    for element in arr:\n",
    "        # extract batch\n",
    "        batch_x = X.loc[(X.index < date_list[element]) & (X.index >= date_list[element] - timedelta(days=7))]\n",
    "        batch_y = Y.loc[(Y.index < date_list[element]) & (Y.index >= date_list[element] - timedelta(days=7))]\n",
    "        # add to the test sets\n",
    "        X_test = pd.concat([batch_x, X_test])\n",
    "        Y_test = pd.concat([batch_y, Y_test])\n",
    "        # delete from the training data set\n",
    "        X_train.drop(index=batch_x.index, inplace=True)\n",
    "        Y_train.drop(index=batch_y.index, inplace=True)\n",
    "        # Delete the elements from the date_list that have been already used for the test set\n",
    "        del date_list[element]\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def get_columns(df, subset):\n",
    "    features = list(df.columns)\n",
    "    if subset == \"cos\":\n",
    "        features.remove(\"weekday\")\n",
    "        features.remove(\"month\")\n",
    "        features.remove(\"hour\")\n",
    "        features.remove(\"number_of_trips\")\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Train split Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_k_fold_validation(k, X, Y, model, train_model, params):\n",
    "    '''\n",
    "    Method that trains and validate the model using k-fold validation\n",
    "    However, this method can disrupt the temporal patterns and data leakage occurs \n",
    "    due to the lagged time features\n",
    "    param k:      Number of folds (iterations)\n",
    "    param x:      Feature data\n",
    "    param y:      Target data\n",
    "    param model:  Model to be trained ()\n",
    "    returns:      Nothing\n",
    "    '''\n",
    "    # initialize the folds\n",
    "    k_fold = KFold(n_splits= k, random_state=47, shuffle=True)\n",
    "    # iteratre through all folds\n",
    "    for train_index, val_index in k_fold.split(X,Y):\n",
    "        # normalize data\n",
    "        X_train, Y_train, X_val, Y_val = normalize_data(\n",
    "            X.iloc[train_index],\n",
    "            Y.iloc[train_index].values.reshape(-1,1), \n",
    "            X.iloc[val_index], \n",
    "            Y.iloc[val_index].values.reshape(-1,1)\n",
    "        )\n",
    "        # train & validate the model\n",
    "        train_model(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=model, params=params)\n",
    "\n",
    "        \n",
    "def batch_split_cross_validation(k, time_bin, X, Y, model, train_model, params):\n",
    "    '''\n",
    "    This method split the entire dataset into batches. Direct Data Leakage is avoided by disrupting\n",
    "    the chain of the lagged time feature between test and training set. Batches are cut on week and day level \n",
    "    depending on the granularity of the time bins.\n",
    "    param X:              feature data \n",
    "    param Y:              target data\n",
    "    param k_folds:        number of folds for k-fold validation\n",
    "    param time_bin:       granularity of the time bins\n",
    "    returns index_dict:   contains indicies to split for cross validation\n",
    "    '''\n",
    "    # if 24 split batches by weeks\n",
    "    if time_bin == 24:\n",
    "        date_list = create_date_list(7)    \n",
    "    # else split by days\n",
    "    else:\n",
    "        date_list = create_date_list(1)\n",
    "    # number of batches that are included in the test set\n",
    "    size_test_split = len(date_list) // k\n",
    "    for fold in range(0, k):\n",
    "        print(fold)\n",
    "        # pick random choices for the test data batches\n",
    "        if fold==k-1:\n",
    "            arr = np.random.choice(range(1, len(date_list)), len(date_list)-1, replace=False)\n",
    "        else: \n",
    "            arr = np.random.choice(range(1, len(date_list)), size_test_split, replace=False)   \n",
    "        # create training and test set with the batches\n",
    "        X_train, Y_train, X_test, Y_test = create_batch_split(X,Y, date_list, arr)\n",
    "        # normalize the data\n",
    "        X_train, Y_train, X_test, Y_test = normalize_data(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test\n",
    "        )\n",
    "        # train & validate the model\n",
    "        train_model(X_train=X_train, Y_train= Y_train, X_val=X_test, Y_val=Y_test,  model=model, params=params)\n",
    "\n",
    "def sorted_train_test_split(X, Y, test_size, model, train_model, params):\n",
    "    '''\n",
    "    '''\n",
    "    # sort the entries in ascending order\n",
    "    X.sort_index(inplace=True)\n",
    "    Y.sort_index(inplace=True)\n",
    "    # get split index\n",
    "    test_index = int(len(X)*(1-test_size))-1\n",
    "    # normalize data\n",
    "    X_train, Y_train, X_val, Y_val = normalize_data(\n",
    "        X.iloc[:test_index],\n",
    "        Y.iloc[:test_index].values.reshape(-1,1), \n",
    "        X.iloc[test_index:], \n",
    "        Y.iloc[test_index:].values.reshape(-1,1)\n",
    "    )\n",
    "    # train & validate the model\n",
    "    r2, MAE = train_model(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=model, params=params)\n",
    "    \n",
    "    return r2, MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# encode the hexagons in dummy variables\n",
    "\n",
    "for period in time_periods:\n",
    "    for res in resolution:\n",
    "        df = prediction_data.get(period).get(res)\n",
    "        \n",
    "        encoded_data = encoder.fit_transform(df[[res]])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out([res]))\n",
    "\n",
    "        #df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "        df = pd.concat([df, encoded_df], axis=1).drop(res, axis=1)\n",
    "        df.set_index(\"trip_start_timestamp\", inplace=True);\n",
    "        \n",
    "        prediction_data[period][res] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_ksvr=joblib.load(f'Models/gs_results_ksvr')\n",
    "gs_results_lsvr=joblib.load(f'Models/gs_results_lsvr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(gs_results_lsvr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(gs_results_lsvr[(1, 'h3_res_4')] )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': [{'regressor__model__C': 0.1, 'regressor__model__epsilon': 0.01}, {'regressor__model__C': 0.1, 'regressor__model__epsilon': 0.1}, {'regressor__model__C': 0.1, 'regressor__model__epsilon': 0.5}, {'regressor__model__C': 1, 'regressor__model__epsilon': 0.01}, {'regressor__model__C': 1, 'regressor__model__epsilon': 0.1}, {'regressor__model__C': 1, 'regressor__model__epsilon': 0.5}, {'regressor__model__C': 10, 'regressor__model__epsilon': 0.01}, {'regressor__model__C': 10, 'regressor__model__epsilon': 0.1}, {'regressor__model__C': 10, 'regressor__model__epsilon': 0.5}, {'regressor__model__C': 100, 'regressor__model__epsilon': 0.01}, {'regressor__model__C': 100, 'regressor__model__epsilon': 0.1}, {'regressor__model__C': 100, 'regressor__model__epsilon': 0.5}, {'regressor__model__C': 150, 'regressor__model__epsilon': 0.01}, {'regressor__model__C': 150, 'regressor__model__epsilon': 0.1}, {'regressor__model__C': 150, 'regressor__model__epsilon': 0.5}], 'mean_fit_time': array([0.0103584 , 0.00853268, 0.00876939, 0.014104  , 0.01015562,\n",
      "       0.00934283, 0.02066688, 0.01990912, 0.01116204, 0.01999074,\n",
      "       0.01861321, 0.01709619, 0.02407331, 0.01927817, 0.01598914]), 'std_fit_time': array([0.00242845, 0.00154513, 0.00290719, 0.00464215, 0.00188686,\n",
      "       0.00396569, 0.00636748, 0.00598435, 0.0024756 , 0.00692226,\n",
      "       0.00663841, 0.00483582, 0.0060408 , 0.00491864, 0.00259413]), 'mean_score_time': array([0.00293694, 0.00206826, 0.0023242 , 0.00414661, 0.00354414,\n",
      "       0.00189816, 0.00311394, 0.00232368, 0.00198642, 0.0022345 ,\n",
      "       0.00273071, 0.00244652, 0.00489407, 0.00194718, 0.00223613]), 'std_score_time': array([1.37182120e-03, 3.36264016e-04, 6.02745068e-04, 3.74623822e-03,\n",
      "       2.35499319e-03, 8.31067360e-05, 2.00971713e-03, 2.56960351e-04,\n",
      "       7.41166854e-05, 5.10242206e-04, 1.10784609e-03, 3.99974162e-04,\n",
      "       2.98004433e-03, 5.24198196e-05, 3.87515556e-04]), 'split0_test_score': array([-0.43037746, -0.34104334, -0.35995721, -0.37572844, -0.28791382,\n",
      "       -0.41825749, -0.38975441, -0.27957754, -0.47525992, -0.49003691,\n",
      "       -0.23250852, -0.08757646,  0.13721527,  0.08314876, -0.27990477]), 'split1_test_score': array([0.78781473, 0.80703648, 0.70062504, 0.8330431 , 0.84255275,\n",
      "       0.39628963, 0.79747133, 0.83813937, 0.37517343, 0.75083557,\n",
      "       0.86926765, 0.66329739, 0.80105088, 0.55284344, 0.55264098]), 'split2_test_score': array([0.97068457, 0.94612124, 0.66848715, 0.96641556, 0.9477299 ,\n",
      "       0.74363624, 0.96491344, 0.93982227, 0.72148289, 0.86177687,\n",
      "       0.85120023, 0.65935312, 0.66947328, 0.68360817, 0.70992695]), 'split3_test_score': array([  0.85349233,  -0.31772033, -15.07735436,   0.8479633 ,\n",
      "        -0.30291763, -16.40726447,   0.08316585,  -0.81357159,\n",
      "       -16.44920197,  -1.21329869,  -4.35157293, -17.74149568,\n",
      "        -6.37822436, -21.7059502 ,  -4.4408086 ]), 'mean_test_score': array([ 0.54540354,  0.27359851, -3.51704984,  0.56792338,  0.2998628 ,\n",
      "       -3.92139902,  0.36394905,  0.17120313, -3.95695139, -0.02268079,\n",
      "       -0.71590339, -4.12660541, -1.19262123, -5.09658746, -0.86453636]), 'std_test_score': array([0.56716276, 0.60503832, 6.68796262, 0.54726264, 0.59646244,\n",
      "       7.22104262, 0.54681073, 0.74306187, 7.22553722, 0.86841604,\n",
      "       2.14594645, 7.8665043 , 3.00420827, 9.59201919, 2.09874031]), 'rank_test_score': array([ 2,  5, 11,  1,  4, 12,  3,  6, 13,  7,  8, 14, 10, 15,  9]), 'param_regressor__model__C': masked_array(data=[0.1, 0.1, 0.1, 1, 1, 1, 10, 10, 10, 100, 100, 100, 150,\n",
      "                   150, 150],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_regressor__model__epsilon': masked_array(data=[0.01, 0.1, 0.5, 0.01, 0.1, 0.5, 0.01, 0.1, 0.5, 0.01,\n",
      "                   0.1, 0.5, 0.01, 0.1, 0.5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "print(gs_results_lsvr[(1, 'h3_res_4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to store all performance measures\n",
    "    # time_gs\n",
    "    # time_training\n",
    "    # performance_training\n",
    "    # iteration\n",
    "    # model_type\n",
    "\n",
    "lsvr_measures = {}\n",
    "ksvr_measures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_linear = {\n",
    "    'regressor__model__C': [0.1, 1, 10, 100, 150],\n",
    "    #'regressor__model__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    'regressor__model__epsilon': [0.01, 0.1, 0.5],\n",
    "\n",
    "}\n",
    "\n",
    "param_grid_k = {\n",
    "    'regressor__model__C': [0.1, 1, 10, 100, 150],\n",
    "    'regressor__model__epsilon': [0.01, 0.1],\n",
    "    #'regressor__model__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    #'regressor__model__kernel': ['rbf','sigmoid', 'poly'],\n",
    "    'regressor__model__kernel': ['rbf', 'sigmoid'],\n",
    "    'regressor__model__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'regressor__model__coef0': [0, 0.1, 0.5, 1],  # Relevant for 'poly' and 'sigmoid' kernels\n",
    "    #'regressor__model__degree': [2, 3, 4]  # Only relevant for 'poly' kernel\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_sv(tss, scaler, model, params, name, measures, model_type):\n",
    "    '''\n",
    "    param tss:      TimeSeriesSplitObject to perform gridsearch on a timeseries split\n",
    "    param scaler:   preferred scaler to rescale the data\n",
    "    param model:    model used in the GS\n",
    "    param params:   param gird for the GS\n",
    "    param name:     name to store the params with joblib\n",
    "    param measures: dict to store time to find params\n",
    "    '''\n",
    "    gridsearch_results = {}\n",
    "    for period in time_periods:\n",
    "        for res in resolution: \n",
    "            \n",
    "            print(f\"time period:{period}, resolution:{res}\")\n",
    "            df = prediction_data.get(period).get(res)\n",
    "            \n",
    "            columns= get_columns(df,'cos')\n",
    "            \n",
    "            # create pipline to scale the data\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('model', model)\n",
    "            ])\n",
    "\n",
    "            # create TransformedTargetRegressor to scale target variable\n",
    "            ttr = TransformedTargetRegressor(\n",
    "                regressor = pipeline,\n",
    "                transformer = scaler\n",
    "            )\n",
    "\n",
    "            # create dask grid search object (dask to enable parallel computing)\n",
    "            Dask_GS = DaskGridSearchCV(\n",
    "                estimator=ttr,\n",
    "                param_grid=params,\n",
    "                cv=tss,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1 #use a available cores\n",
    "            )\n",
    "\n",
    "            # use Client() for enable parallel computing with dask\n",
    "            cluster = LocalCluster(n_workers=16, threads_per_worker=1)\n",
    "            with Client(cluster) as client:\n",
    "\n",
    "                num_cores = client.ncores()\n",
    "                print(f\"Number of cores: {num_cores}\")\n",
    "                print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "\n",
    "                start_time = time.time() # time computation time for the gridsearch\n",
    "                warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "                Dask_GS.fit(df[columns], df['number_of_trips']) # fit the grid search\n",
    "                end_time = time.time()\n",
    "                # gridsearch results to the dict\n",
    "                gridsearch_results[(period,res)]= Dask_GS.cv_results_\n",
    "            '''\n",
    "            try:\n",
    "                with Client() as client:\n",
    "\n",
    "                    num_cores = client.ncores()\n",
    "                    print(f\"Number of cores: {num_cores}\")\n",
    "                    print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "\n",
    "                    start_time = time.time() # time computation time for the gridsearch\n",
    "                    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "                    Dask_GS.fit(df[columns], df['number_of_trips']) # fit the grid search\n",
    "                    end_time = time.time()\n",
    "                    # gridsearch results to the dict\n",
    "                    gridsearch_results[(period,res)]= Dask_GS.cv_results_\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Exception {e} occured, swtich to running only one job\")\n",
    "\n",
    "                Dask_GS.n_jobs=1\n",
    "\n",
    "                start_time = time.time() # time computation time for the gridsearch\n",
    "                warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "                Dask_GS.fit(df[columns], df['number_of_trips']) # fit the grid search\n",
    "                end_time = time.time()\n",
    "                # gridsearch results to the dict\n",
    "                gridsearch_results[(period,res)]= Dask_GS.cv_results_\n",
    "            '''\n",
    "            print(f\"GridSearch took: {(end_time-start_time)/60} minuts\")\n",
    "            \n",
    "            joblib.dump(Dask_GS.best_params_, f'Models/{name}_{period}_{res}')\n",
    "            measures[(period, res)] = {\n",
    "                'time_gs': (end_time-start_time)/60,\n",
    "                'iteration': 1,\n",
    "                'model_type': model_type\n",
    "            }\n",
    "            #break\n",
    "        #break\n",
    "    return gridsearch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(n_splits=4)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time period:1, resolution:h3_res_4\n",
      "Number of cores: {'tcp://127.0.0.1:51082': 1, 'tcp://127.0.0.1:51083': 1, 'tcp://127.0.0.1:51086': 1, 'tcp://127.0.0.1:51089': 1, 'tcp://127.0.0.1:51092': 1, 'tcp://127.0.0.1:51097': 1, 'tcp://127.0.0.1:51098': 1, 'tcp://127.0.0.1:51101': 1, 'tcp://127.0.0.1:51106': 1, 'tcp://127.0.0.1:51107': 1, 'tcp://127.0.0.1:51112': 1, 'tcp://127.0.0.1:51115': 1, 'tcp://127.0.0.1:51118': 1, 'tcp://127.0.0.1:51119': 1, 'tcp://127.0.0.1:51124': 1, 'tcp://127.0.0.1:51127': 1}\n",
      "Dashboard link: http://127.0.0.1:8787/status\n",
      "GridSearch took: 0.107269549369812 minuts\n",
      "time period:1, resolution:h3_res_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51199 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:51259': 1, 'tcp://127.0.0.1:51266': 1, 'tcp://127.0.0.1:51273': 1, 'tcp://127.0.0.1:51276': 1, 'tcp://127.0.0.1:51277': 1, 'tcp://127.0.0.1:51278': 1, 'tcp://127.0.0.1:51281': 1, 'tcp://127.0.0.1:51286': 1, 'tcp://127.0.0.1:51287': 1, 'tcp://127.0.0.1:51290': 1, 'tcp://127.0.0.1:51295': 1, 'tcp://127.0.0.1:51296': 1, 'tcp://127.0.0.1:51299': 1, 'tcp://127.0.0.1:51304': 1, 'tcp://127.0.0.1:51309': 1, 'tcp://127.0.0.1:51312': 1}\n",
      "Dashboard link: http://127.0.0.1:51199/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 41.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 41.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took: 2.0772348284721374 minuts\n",
      "time period:1, resolution:h3_res_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51385 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (2.06s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:51433': 1, 'tcp://127.0.0.1:51456': 1, 'tcp://127.0.0.1:51457': 1, 'tcp://127.0.0.1:51462': 1, 'tcp://127.0.0.1:51463': 1, 'tcp://127.0.0.1:51464': 1, 'tcp://127.0.0.1:51469': 1, 'tcp://127.0.0.1:51470': 1, 'tcp://127.0.0.1:51473': 1, 'tcp://127.0.0.1:51474': 1, 'tcp://127.0.0.1:51479': 1, 'tcp://127.0.0.1:51486': 1, 'tcp://127.0.0.1:51487': 1, 'tcp://127.0.0.1:51492': 1, 'tcp://127.0.0.1:51495': 1, 'tcp://127.0.0.1:51513': 1}\n",
      "Dashboard link: http://127.0.0.1:51385/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 207.26 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 207.25 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took: 7.824365051587423 minuts\n",
      "time period:2, resolution:h3_res_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51598 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (3.34s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:51666': 1, 'tcp://127.0.0.1:51667': 1, 'tcp://127.0.0.1:51668': 1, 'tcp://127.0.0.1:51671': 1, 'tcp://127.0.0.1:51678': 1, 'tcp://127.0.0.1:51679': 1, 'tcp://127.0.0.1:51680': 1, 'tcp://127.0.0.1:51687': 1, 'tcp://127.0.0.1:51688': 1, 'tcp://127.0.0.1:51693': 1, 'tcp://127.0.0.1:51694': 1, 'tcp://127.0.0.1:51699': 1, 'tcp://127.0.0.1:51702': 1, 'tcp://127.0.0.1:51705': 1, 'tcp://127.0.0.1:51708': 1, 'tcp://127.0.0.1:51726': 1}\n",
      "Dashboard link: http://127.0.0.1:51598/status\n",
      "GridSearch took: 0.05746183395385742 minuts\n",
      "time period:2, resolution:h3_res_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51789 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (4.38s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:51835': 1, 'tcp://127.0.0.1:51842': 1, 'tcp://127.0.0.1:51863': 1, 'tcp://127.0.0.1:51866': 1, 'tcp://127.0.0.1:51867': 1, 'tcp://127.0.0.1:51868': 1, 'tcp://127.0.0.1:51869': 1, 'tcp://127.0.0.1:51878': 1, 'tcp://127.0.0.1:51879': 1, 'tcp://127.0.0.1:51880': 1, 'tcp://127.0.0.1:51887': 1, 'tcp://127.0.0.1:51888': 1, 'tcp://127.0.0.1:51893': 1, 'tcp://127.0.0.1:51896': 1, 'tcp://127.0.0.1:51899': 1, 'tcp://127.0.0.1:51917': 1}\n",
      "Dashboard link: http://127.0.0.1:51789/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 23.01 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 23.01 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took: 1.0644002000490824 minuts\n",
      "time period:2, resolution:h3_res_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51987 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (5.93s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:52039': 1, 'tcp://127.0.0.1:52040': 1, 'tcp://127.0.0.1:52045': 1, 'tcp://127.0.0.1:52046': 1, 'tcp://127.0.0.1:52047': 1, 'tcp://127.0.0.1:52050': 1, 'tcp://127.0.0.1:52057': 1, 'tcp://127.0.0.1:52060': 1, 'tcp://127.0.0.1:52079': 1, 'tcp://127.0.0.1:52082': 1, 'tcp://127.0.0.1:52085': 1, 'tcp://127.0.0.1:52086': 1, 'tcp://127.0.0.1:52091': 1, 'tcp://127.0.0.1:52094': 1, 'tcp://127.0.0.1:52097': 1, 'tcp://127.0.0.1:52115': 1}\n",
      "Dashboard link: http://127.0.0.1:51987/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 127.91 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 127.91 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took: 4.616246779759725 minuts\n",
      "time period:6, resolution:h3_res_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 52192 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (7.34s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:52242': 1, 'tcp://127.0.0.1:52247': 1, 'tcp://127.0.0.1:52252': 1, 'tcp://127.0.0.1:52263': 1, 'tcp://127.0.0.1:52272': 1, 'tcp://127.0.0.1:52275': 1, 'tcp://127.0.0.1:52276': 1, 'tcp://127.0.0.1:52277': 1, 'tcp://127.0.0.1:52282': 1, 'tcp://127.0.0.1:52283': 1, 'tcp://127.0.0.1:52286': 1, 'tcp://127.0.0.1:52293': 1, 'tcp://127.0.0.1:52294': 1, 'tcp://127.0.0.1:52299': 1, 'tcp://127.0.0.1:52302': 1, 'tcp://127.0.0.1:52320': 1}\n",
      "Dashboard link: http://127.0.0.1:52192/status\n",
      "GridSearch took: 0.06630708773930867 minuts\n",
      "time period:6, resolution:h3_res_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 52396 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (8.69s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:52438': 1, 'tcp://127.0.0.1:52447': 1, 'tcp://127.0.0.1:52470': 1, 'tcp://127.0.0.1:52471': 1, 'tcp://127.0.0.1:52472': 1, 'tcp://127.0.0.1:52473': 1, 'tcp://127.0.0.1:52478': 1, 'tcp://127.0.0.1:52483': 1, 'tcp://127.0.0.1:52484': 1, 'tcp://127.0.0.1:52487': 1, 'tcp://127.0.0.1:52490': 1, 'tcp://127.0.0.1:52497': 1, 'tcp://127.0.0.1:52498': 1, 'tcp://127.0.0.1:52503': 1, 'tcp://127.0.0.1:52506': 1, 'tcp://127.0.0.1:52524': 1}\n",
      "Dashboard link: http://127.0.0.1:52396/status\n",
      "GridSearch took: 0.3413338820139567 minuts\n",
      "time period:6, resolution:h3_res_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 52606 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (10.22s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:52646': 1, 'tcp://127.0.0.1:52651': 1, 'tcp://127.0.0.1:52656': 1, 'tcp://127.0.0.1:52663': 1, 'tcp://127.0.0.1:52666': 1, 'tcp://127.0.0.1:52671': 1, 'tcp://127.0.0.1:52676': 1, 'tcp://127.0.0.1:52683': 1, 'tcp://127.0.0.1:52684': 1, 'tcp://127.0.0.1:52691': 1, 'tcp://127.0.0.1:52696': 1, 'tcp://127.0.0.1:52701': 1, 'tcp://127.0.0.1:52706': 1, 'tcp://127.0.0.1:52713': 1, 'tcp://127.0.0.1:52716': 1, 'tcp://127.0.0.1:52734': 1}\n",
      "Dashboard link: http://127.0.0.1:52606/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 56.77 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 56.77 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took: 2.1509082595507305 minuts\n",
      "time period:24, resolution:h3_res_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 52816 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (11.42s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:52858': 1, 'tcp://127.0.0.1:52859': 1, 'tcp://127.0.0.1:52868': 1, 'tcp://127.0.0.1:52873': 1, 'tcp://127.0.0.1:52876': 1, 'tcp://127.0.0.1:52881': 1, 'tcp://127.0.0.1:52888': 1, 'tcp://127.0.0.1:52889': 1, 'tcp://127.0.0.1:52908': 1, 'tcp://127.0.0.1:52909': 1, 'tcp://127.0.0.1:52910': 1, 'tcp://127.0.0.1:52911': 1, 'tcp://127.0.0.1:52918': 1, 'tcp://127.0.0.1:52921': 1, 'tcp://127.0.0.1:52924': 1, 'tcp://127.0.0.1:52945': 1}\n",
      "Dashboard link: http://127.0.0.1:52816/status\n",
      "GridSearch took: 0.3986285964647929 minuts\n",
      "time period:24, resolution:h3_res_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 53029 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (11.02s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:53071': 1, 'tcp://127.0.0.1:53074': 1, 'tcp://127.0.0.1:53081': 1, 'tcp://127.0.0.1:53082': 1, 'tcp://127.0.0.1:53089': 1, 'tcp://127.0.0.1:53096': 1, 'tcp://127.0.0.1:53097': 1, 'tcp://127.0.0.1:53104': 1, 'tcp://127.0.0.1:53113': 1, 'tcp://127.0.0.1:53118': 1, 'tcp://127.0.0.1:53121': 1, 'tcp://127.0.0.1:53130': 1, 'tcp://127.0.0.1:53131': 1, 'tcp://127.0.0.1:53132': 1, 'tcp://127.0.0.1:53134': 1, 'tcp://127.0.0.1:53157': 1}\n",
      "Dashboard link: http://127.0.0.1:53029/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 14:53:44,471 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:53029', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tornado\\web.py\", line 1790, in _execute\n",
      "    result = await result\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tornado\\websocket.py\", line 273, in get\n",
      "    await self.ws_connection.accept_connection(self)\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tornado\\websocket.py\", line 863, in accept_connection\n",
      "    await self._accept_connection(handler)\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tornado\\websocket.py\", line 946, in _accept_connection\n",
      "    await self._receive_frame_loop()\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tornado\\websocket.py\", line 1105, in _receive_frame_loop\n",
      "    self.handler.on_ws_connection_close(self.close_code, self.close_reason)\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tornado\\websocket.py\", line 571, in on_ws_connection_close\n",
      "    self.on_connection_close()\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tornado\\websocket.py\", line 563, in on_connection_close\n",
      "    self.on_close()\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\bokeh\\server\\views\\ws.py\", line 308, in on_close\n",
      "    self.connection.session.notify_connection_lost()\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\bokeh\\server\\connection.py\", line 65, in session\n",
      "    assert self._session is not None\n",
      "AssertionError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took: 0.42740896145502727 minuts\n",
      "time period:24, resolution:h3_res_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 53254 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. (11.86s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: {'tcp://127.0.0.1:53294': 1, 'tcp://127.0.0.1:53301': 1, 'tcp://127.0.0.1:53304': 1, 'tcp://127.0.0.1:53311': 1, 'tcp://127.0.0.1:53312': 1, 'tcp://127.0.0.1:53319': 1, 'tcp://127.0.0.1:53324': 1, 'tcp://127.0.0.1:53333': 1, 'tcp://127.0.0.1:53336': 1, 'tcp://127.0.0.1:53339': 1, 'tcp://127.0.0.1:53344': 1, 'tcp://127.0.0.1:53355': 1, 'tcp://127.0.0.1:53356': 1, 'tcp://127.0.0.1:53359': 1, 'tcp://127.0.0.1:53360': 1, 'tcp://127.0.0.1:53382': 1}\n",
      "Dashboard link: http://127.0.0.1:53254/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 17.77 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lukas\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\distributed\\client.py:3357: UserWarning: Sending large graph of size 17.77 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took: 0.9178211331367493 minuts\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "gs_results_lsvr = grid_search_sv(tss, scaler, LinearSVR(dual= 'auto'), param_grid_linear, 'SVR/LSVR/params/lsvr', lsvr_measures, 'lsvr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parallel_coordinates(df, title):\n",
    "    para_cord =pd.DataFrame(df['params'])\n",
    "    para_cord['r2'] = df['mean_test_score']\n",
    "    print(para_cord)\n",
    "    \n",
    "    fig = px.parallel_coordinates(\n",
    "    para_cord,\n",
    "    color = 'r2',\n",
    "    dimensions = [col for col in para_cord.columns if col != 'r2'],\n",
    "    color_continuous_scale = px.colors.sequential.Viridis,\n",
    "    labels = {col : col for col in para_cord.columns}\n",
    "    )\n",
    "    fig.update_layout(title=title,\n",
    "                      title_x=0.5,\n",
    "                      title_y=1  \n",
    "                     )\n",
    "    fig.show(renderer=\"browser\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    regressor__model__C  regressor__model__epsilon        r2\n",
      "0                   0.1                       0.01  0.545404\n",
      "1                   0.1                       0.10  0.273599\n",
      "2                   0.1                       0.50 -3.517050\n",
      "3                   1.0                       0.01  0.567923\n",
      "4                   1.0                       0.10  0.299863\n",
      "5                   1.0                       0.50 -3.921399\n",
      "6                  10.0                       0.01  0.363949\n",
      "7                  10.0                       0.10  0.171203\n",
      "8                  10.0                       0.50 -3.956951\n",
      "9                 100.0                       0.01 -0.022681\n",
      "10                100.0                       0.10 -0.715903\n",
      "11                100.0                       0.50 -4.126605\n",
      "12                150.0                       0.01 -1.192621\n",
      "13                150.0                       0.10 -5.096587\n",
      "14                150.0                       0.50 -0.864536\n",
      "    regressor__model__C  regressor__model__epsilon        r2\n",
      "0                   0.1                       0.01  0.877051\n",
      "1                   0.1                       0.10  0.550765\n",
      "2                   0.1                       0.50 -2.334182\n",
      "3                   1.0                       0.01  0.874362\n",
      "4                   1.0                       0.10  0.509702\n",
      "5                   1.0                       0.50 -2.198204\n",
      "6                  10.0                       0.01  0.788338\n",
      "7                  10.0                       0.10  0.221674\n",
      "8                  10.0                       0.50 -2.332786\n",
      "9                 100.0                       0.01  0.455151\n",
      "10                100.0                       0.10 -0.381429\n",
      "11                100.0                       0.50 -2.184034\n",
      "12                150.0                       0.01  0.360598\n",
      "13                150.0                       0.10  0.205375\n",
      "14                150.0                       0.50 -4.385780\n",
      "    regressor__model__C  regressor__model__epsilon         r2\n",
      "0                   0.1                       0.01   0.563764\n",
      "1                   0.1                       0.10  -0.357655\n",
      "2                   0.1                       0.50 -11.463465\n",
      "3                   1.0                       0.01   0.500203\n",
      "4                   1.0                       0.10  -0.489227\n",
      "5                   1.0                       0.50 -11.906899\n",
      "6                  10.0                       0.01  -0.293316\n",
      "7                  10.0                       0.10  -2.394124\n",
      "8                  10.0                       0.50 -21.190660\n",
      "9                 100.0                       0.01 -22.337454\n",
      "10                100.0                       0.10  -0.987648\n",
      "11                100.0                       0.50 -15.408294\n",
      "12                150.0                       0.01  -5.959072\n",
      "13                150.0                       0.10  -3.525364\n",
      "14                150.0                       0.50 -29.434836\n",
      "    regressor__model__C  regressor__model__epsilon          r2\n",
      "0                   0.1                       0.01    0.165754\n",
      "1                   0.1                       0.10   -7.258512\n",
      "2                   0.1                       0.50 -169.393448\n",
      "3                   1.0                       0.01    0.095979\n",
      "4                   1.0                       0.10   -0.062858\n",
      "5                   1.0                       0.50   -6.154864\n",
      "6                  10.0                       0.01    0.281088\n",
      "7                  10.0                       0.10   -6.637468\n",
      "8                  10.0                       0.50   -5.736895\n",
      "9                 100.0                       0.01 -218.290607\n",
      "10                100.0                       0.10   -5.870956\n",
      "11                100.0                       0.50  -12.898837\n",
      "12                150.0                       0.01   -7.349108\n",
      "13                150.0                       0.10 -161.900387\n",
      "14                150.0                       0.50  -15.500122\n",
      "    regressor__model__C  regressor__model__epsilon         r2\n",
      "0                   0.1                       0.01   0.759140\n",
      "1                   0.1                       0.10   0.453455\n",
      "2                   0.1                       0.50  -7.118797\n",
      "3                   1.0                       0.01   0.747875\n",
      "4                   1.0                       0.10   0.452331\n",
      "5                   1.0                       0.50  -7.077778\n",
      "6                  10.0                       0.01   0.715452\n",
      "7                  10.0                       0.10   0.322110\n",
      "8                  10.0                       0.50  -7.211633\n",
      "9                 100.0                       0.01  -2.381543\n",
      "10                100.0                       0.10  -2.503760\n",
      "11                100.0                       0.50  -8.971578\n",
      "12                150.0                       0.01  -2.943801\n",
      "13                150.0                       0.10  -9.077059\n",
      "14                150.0                       0.50 -11.338877\n",
      "    regressor__model__C  regressor__model__epsilon         r2\n",
      "0                   0.1                       0.01   0.173863\n",
      "1                   0.1                       0.10  -1.557694\n",
      "2                   0.1                       0.50 -27.046844\n",
      "3                   1.0                       0.01   0.330819\n",
      "4                   1.0                       0.10  -1.386988\n",
      "5                   1.0                       0.50 -26.661183\n",
      "6                  10.0                       0.01  -1.534535\n",
      "7                  10.0                       0.10  -4.863678\n",
      "8                  10.0                       0.50 -33.815114\n",
      "9                 100.0                       0.01 -14.925107\n",
      "10                100.0                       0.10 -10.536859\n",
      "11                100.0                       0.50 -66.461389\n",
      "12                150.0                       0.01  -7.139081\n",
      "13                150.0                       0.10 -11.915258\n",
      "14                150.0                       0.50 -48.614917\n",
      "    regressor__model__C  regressor__model__epsilon            r2\n",
      "0                   0.1                       0.01     -0.901916\n",
      "1                   0.1                       0.10    -34.075577\n",
      "2                   0.1                       0.50    -12.459499\n",
      "3                   1.0                       0.01     -3.746515\n",
      "4                   1.0                       0.10    -33.070159\n",
      "5                   1.0                       0.50    -19.535114\n",
      "6                  10.0                       0.01     -7.561566\n",
      "7                  10.0                       0.10    -58.758246\n",
      "8                  10.0                       0.50    -26.918167\n",
      "9                 100.0                       0.01  -1365.471772\n",
      "10                100.0                       0.10 -15140.088442\n",
      "11                100.0                       0.50  -2679.994562\n",
      "12                150.0                       0.01  -1006.915130\n",
      "13                150.0                       0.10    -79.491730\n",
      "14                150.0                       0.50   -260.835649\n",
      "    regressor__model__C  regressor__model__epsilon         r2\n",
      "0                   0.1                       0.01  -2.077912\n",
      "1                   0.1                       0.10  -3.124406\n",
      "2                   0.1                       0.50 -11.013734\n",
      "3                   1.0                       0.01  -2.920757\n",
      "4                   1.0                       0.10  -3.990005\n",
      "5                   1.0                       0.50 -11.886442\n",
      "6                  10.0                       0.01  -2.703502\n",
      "7                  10.0                       0.10  -4.324373\n",
      "8                  10.0                       0.50 -13.030519\n",
      "9                 100.0                       0.01  -6.073968\n",
      "10                100.0                       0.10 -11.936758\n",
      "11                100.0                       0.50 -33.110028\n",
      "12                150.0                       0.01 -10.549582\n",
      "13                150.0                       0.10  -6.879864\n",
      "14                150.0                       0.50 -27.863822\n",
      "    regressor__model__C  regressor__model__epsilon          r2\n",
      "0                   0.1                       0.01   -5.693066\n",
      "1                   0.1                       0.10   -8.614831\n",
      "2                   0.1                       0.50  -38.268509\n",
      "3                   1.0                       0.01   -6.031525\n",
      "4                   1.0                       0.10   -8.276220\n",
      "5                   1.0                       0.50  -38.992681\n",
      "6                  10.0                       0.01   -9.259269\n",
      "7                  10.0                       0.10  -13.137194\n",
      "8                  10.0                       0.50  -36.101764\n",
      "9                 100.0                       0.01  -10.475208\n",
      "10                100.0                       0.10  -19.152384\n",
      "11                100.0                       0.50  -92.341882\n",
      "12                150.0                       0.01  -27.561814\n",
      "13                150.0                       0.10  -30.237486\n",
      "14                150.0                       0.50 -126.109106\n",
      "    regressor__model__C  regressor__model__epsilon         r2\n",
      "0                   0.1                       0.01  -1.206093\n",
      "1                   0.1                       0.10  -2.671158\n",
      "2                   0.1                       0.50 -32.264212\n",
      "3                   1.0                       0.01  -1.130678\n",
      "4                   1.0                       0.10  -5.470404\n",
      "5                   1.0                       0.50 -39.923060\n",
      "6                  10.0                       0.01  -1.275726\n",
      "7                  10.0                       0.10  -5.701790\n",
      "8                  10.0                       0.50 -39.902445\n",
      "9                 100.0                       0.01 -11.668217\n",
      "10                100.0                       0.10 -28.148258\n",
      "11                100.0                       0.50 -39.944053\n",
      "12                150.0                       0.01 -11.072868\n",
      "13                150.0                       0.10 -14.279316\n",
      "14                150.0                       0.50 -39.943025\n",
      "    regressor__model__C  regressor__model__epsilon          r2\n",
      "0                   0.1                       0.01  -48.189194\n",
      "1                   0.1                       0.10  -75.952443\n",
      "2                   0.1                       0.50 -299.922624\n",
      "3                   1.0                       0.01  -76.208087\n",
      "4                   1.0                       0.10 -102.348948\n",
      "5                   1.0                       0.50 -335.744534\n",
      "6                  10.0                       0.01  -74.179194\n",
      "7                  10.0                       0.10 -101.528079\n",
      "8                  10.0                       0.50 -368.594974\n",
      "9                 100.0                       0.01 -141.896150\n",
      "10                100.0                       0.10 -131.042712\n",
      "11                100.0                       0.50 -423.164910\n",
      "12                150.0                       0.01 -157.309541\n",
      "13                150.0                       0.10  -95.776591\n",
      "14                150.0                       0.50 -476.186906\n",
      "    regressor__model__C  regressor__model__epsilon          r2\n",
      "0                   0.1                       0.01  -13.680708\n",
      "1                   0.1                       0.10  -18.844437\n",
      "2                   0.1                       0.50  -63.746961\n",
      "3                   1.0                       0.01  -13.960173\n",
      "4                   1.0                       0.10  -19.081049\n",
      "5                   1.0                       0.50  -66.842127\n",
      "6                  10.0                       0.01  -14.706114\n",
      "7                  10.0                       0.10  -19.627945\n",
      "8                  10.0                       0.50  -69.565918\n",
      "9                 100.0                       0.01  -20.171077\n",
      "10                100.0                       0.10  -36.759676\n",
      "11                100.0                       0.50 -173.221289\n",
      "12                150.0                       0.01  -28.400874\n",
      "13                150.0                       0.10  -20.016423\n",
      "14                150.0                       0.50 -117.063699\n"
     ]
    }
   ],
   "source": [
    "for (time_res, res), df in gs_results_lsvr.items():\n",
    "    create_parallel_coordinates(df, f\"Time: {time_res}, Res: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_ksvr = grid_search_sv(tss, scaler, SVR(), param_grid_k, 'SVR/KSVR/params/ksvr', ksvr_measures, ksvr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parallel_coordinates(df, title):\n",
    "    para_cord =pd.DataFrame(df['params'])\n",
    "    para_cord['r2'] = df['mean_test_score']\n",
    "\n",
    "    para_cord['r2'] = para_cord['r2'].apply(lambda x: max(x, -1))\n",
    "\n",
    "    cols= ['regressor__model__kernel', 'regressor__model__gamma']\n",
    "    category_mapping = {}\n",
    "\n",
    "    for col in cols:\n",
    "        kernel_categories = para_cord[col].astype('category')\n",
    "        category_mapping[col] = dict(enumerate(kernel_categories.cat.categories))\n",
    "\n",
    "        # Convert the kernel column to categorical codes\n",
    "        para_cord[col] = kernel_categories.cat.codes\n",
    "    \n",
    "    fig = px.parallel_coordinates(\n",
    "    para_cord,\n",
    "    color = 'r2',\n",
    "    dimensions = [col for col in para_cord.columns if col != 'r2'],\n",
    "    color_continuous_scale = px.colors.sequential.Viridis,\n",
    "    labels = {col : col for col in para_cord.columns}\n",
    "    )\n",
    "    fig.update_traces(dimensions=[{'label': dim['label'],\n",
    "                                'tickvals': list(category_mapping[dim['label']].keys()),\n",
    "                                'ticktext': list(category_mapping[dim['label']].values())}\n",
    "                                if dim['label'] in cols else dim\n",
    "                                for dim in fig.data[0]['dimensions']])\n",
    "    fig.update_layout(title=title,\n",
    "                      title_x=0.5,\n",
    "                      title_y=1,\n",
    "                     )\n",
    "    fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (time_res, res), df in gs_results_ksvr.items():\n",
    "    create_parallel_coordinates(df, f\"Time: {time_res}, Res: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gs_results_ksvr, f'Models/gs_results_ksvr')\n",
    "joblib.dump(gs_results_lsvr, f'Models/gs_results_lsvr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models with different time/spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__model__C': 0.1, 'regressor__model__epsilon': 0.5}\n",
      "{'regressor__model__C': 150, 'regressor__model__coef0': 0, 'regressor__model__epsilon': 0.01, 'regressor__model__gamma': 0.001, 'regressor__model__kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "#Load the best models and params from the gridsearch and safe them in a dictionary\n",
    "params_lsvr={}\n",
    "params_ksvr={}\n",
    "for period in time_periods:\n",
    "    dict_lsvr = {}\n",
    "    dict_ksvr = {}\n",
    "    for res in resolution:\n",
    "        dict_lsvr[res] = joblib.load(f'Models/SVR/LSVR/params/lsvr_{period}_{res}')\n",
    "        dict_ksvr[res] = joblib.load(f'Models/SVR/KSVR/params/ksvr_{period}_{res}')\n",
    "    params_lsvr[period] = dict_lsvr\n",
    "    params_ksvr[period] = dict_ksvr\n",
    "\n",
    "print(params_lsvr.get(1).get('h3_res_4'))\n",
    "print(params_ksvr.get(1).get('h3_res_4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Split k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Window Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start End Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "timeperiod: 1, resolution: h3_res_4\n",
      "params: {'regressor__model__C': 0.1, 'regressor__model__epsilon': 0.5}\n",
      "R-squared 0.9671593159330416\n",
      "Mean Squared Error 0.13997997633677037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_25912\\3012117929.py:69: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_25912\\3012117929.py:70: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(1, 'h3_res_4')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# save time in dict\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mlsvr_measures\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_training\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (end_time\u001b[38;5;241m-\u001b[39mstart_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\n\u001b[0;32m     18\u001b[0m lsvr_measures[(period, res)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m r2\n\u001b[0;32m     19\u001b[0m lsvr_measures[(period, res)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m MAE\n",
      "\u001b[1;31mKeyError\u001b[0m: (1, 'h3_res_4')"
     ]
    }
   ],
   "source": [
    "for period in time_periods:\n",
    "    for res in resolution:\n",
    "        print(f'\\ntimeperiod: {period}, resolution: {res}')\n",
    "        \n",
    "        df = prediction_data.get(period).get(res)\n",
    "        columns = get_columns(df, \"cos\")\n",
    "        model = LinearSVR()\n",
    "        params= params_lsvr.get(period).get(res)\n",
    "        print(f\"params: {params}\")\n",
    "\n",
    "        # measure time for training\n",
    "        start_time = time.time()\n",
    "        # train the model\n",
    "        r2, MAE = sorted_train_test_split(df[columns], df[['number_of_trips']], 0.15, model, train_lsvr, params=params)\n",
    "        end_time = time.time()\n",
    "        # save time in dict\n",
    "        lsvr_measures[(period, res)]['time_training'] = (end_time-start_time)/60\n",
    "        lsvr_measures[(period, res)]['r2'] = r2\n",
    "        lsvr_measures[(period, res)]['MAE'] = MAE\n",
    "        # save performance in dict\n",
    "        \n",
    "        \n",
    "        joblib.dump(model, f'Models/SVR/LSVR/models/lsvr_{period}_{res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for period in time_periods:\n",
    "    for res in resolution:\n",
    "        print(f'\\ntimeperiod: {period}, resolution: {res}')\n",
    "        \n",
    "        df = prediction_data.get(period).get(res)\n",
    "        columns = get_columns(df, \"cos\")\n",
    "        model = SVR()\n",
    "        params= params_ksvr.get(period).get(res)\n",
    "        print(f\"params: {params}\")\n",
    "\n",
    "        # measure time for training\n",
    "        start_time = time.time()\n",
    "        # train the model\n",
    "        r2, MAE = sorted_train_test_split(df[columns], df[['number_of_trips']], 0.15, model, train_ksvr, params=params)\n",
    "        end_time = time.time()\n",
    "        # save time in dict\n",
    "        ksvr_measures[(period, res)]['time_training'] = (end_time-start_time)/60\n",
    "        ksvr_measures[(period, res)]['r2'] = r2\n",
    "        ksvr_measures[(period, res)]['MAE'] = MAE\n",
    "        # save performance in dict\n",
    "        \n",
    "        joblib.dump(model, f'Models/SVR/LSVR/models/ksvr_{period}_{res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare the results of the different time/spaitla bin combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lsvr_measures, f'Models/SVR/ksvr_measures')\n",
    "joblib.dump(ksvr_measures, f'Models/SVR/lsvr_measures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvr_measures = joblib.load(f'Models/SVR/ksvr_measures')\n",
    "ksvr_measures = joblib.load(f'Models/SVR/lsvr_measures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_from_measures(measure, score, title, ax=None, cmap = \"RdYlGn\"):\n",
    "\n",
    "    x_values = sorted(set(keys[0] for keys in measure))\n",
    "    y_values = sorted(set(keys[1] for keys in measure))\n",
    "\n",
    "    matrix = np.zeros((len(y_values), len(x_values)))\n",
    "\n",
    "    for (x,y), element in measure.items():\n",
    "        x_index = x_values.index(x)\n",
    "        y_index = y_values.index(y)\n",
    "        matrix[y_index, x_index] = element[score]\n",
    "\n",
    "    sns.heatmap(matrix, \n",
    "                xticklabels= x_values, \n",
    "                yticklabels=y_values, \n",
    "                annot=True, cmap=cmap,  # cmap=\"YlGnBu\" \"RdYlGn\"\n",
    "                cbar_kws={'label': f\"{score} Value\"}, \n",
    "                ax=ax)\n",
    "\n",
    "    ax.set_xlabel('Temporal resolution')\n",
    "    ax.set_ylabel('Spatial resolution')\n",
    "    ax.set_title(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,1 , figsize=(10,32))\n",
    "\n",
    "plot_heatmap_from_measures(lsvr_measures, 'r2', 'R2 Values for LSVR', ax=ax[0])\n",
    "plot_heatmap_from_measures(ksvr_measures, 'r2', 'R2 Values for KSVR', ax=ax[1])\n",
    "plot_heatmap_from_measures(lsvr_measures, 'time_training', 'Training Time for LSVR in Min', ax=ax[2], cmap=\"YlGnBu\")\n",
    "plot_heatmap_from_measures(ksvr_measures, 'time_training', 'Training Time for KSVR in Min', ax=ax[3], cmap = \"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyize the models with XAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit the number of plots only the model with the best performance and the model with the worst performance are analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_measures= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_train_test_split(X, Y, test_size):\n",
    "    '''\n",
    "    '''\n",
    "    # sort the entries in ascending order\n",
    "    X.sort_index(inplace=True)\n",
    "    Y.sort_index(inplace=True)\n",
    "    # get split index\n",
    "    test_index = int(len(X)*(1-test_size))-1\n",
    "    # normalize data\n",
    "    X_train, Y_train, X_val, Y_val = normalize_data(\n",
    "        X.iloc[:test_index],\n",
    "        Y.iloc[:test_index].values.reshape(-1,1), \n",
    "        X.iloc[test_index:], \n",
    "        Y.iloc[test_index:].values.reshape(-1,1)\n",
    "    )\n",
    "    # train & validate the model\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(\n",
    "    input_size, \n",
    "    act_function, \n",
    "    decrease_func, \n",
    "    #factor, \n",
    "    first_layer_factor,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    metrics,\n",
    "    dropout_rate,\n",
    "    learning_rate\n",
    "):\n",
    "    nn = Sequential()\n",
    "    neurons = int(input_size * first_layer_factor)\n",
    "    nn.add(Dense(neurons, input_shape=(input_size,), activation=act_function))\n",
    "    counter = 1\n",
    "    while True:\n",
    "        if decrease_func == \"exp\":\n",
    "            neurons = exp_dec_func(neurons, counter)\n",
    "        elif decrease_func == \"linear\":\n",
    "            neurons = linear_dec_func(neurons, counter)\n",
    "        if neurons <= 1:\n",
    "            break\n",
    "        else:\n",
    "            nn.add(Dense(neurons, activation=act_function))\n",
    "            nn.add(Dropout(dropout_rate))\n",
    "            counter += 1\n",
    "    nn.add(Dense(1))\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer_instance = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer_instance = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'RMSPROP':\n",
    "        optimizer_instance = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        print(\"invalid optimizer\")\n",
    "        \n",
    "    nn.compile(\n",
    "        optimizer=optimizer_instance,\n",
    "        loss=loss,\n",
    "        metrics=[metrics],\n",
    "    )\n",
    "    return nn\n",
    "\n",
    "def linear_dec_func(neurons, counter, factor=0.5):\n",
    "    return int((1 - factor * counter) * neurons)\n",
    "\n",
    "def exp_dec_func(neurons, counter, factor=1):\n",
    "    return int(math.exp(-counter * factor) * neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_train, Y_train, X_val, Y_val, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a neural network with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   NN to be trained\n",
    "    param params:  Parameters to compile and fit the NN\n",
    "    returns:       Nothing     \n",
    "    '''\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=Y_train,\n",
    "        batch_size=params.get(\"batch_size\"),\n",
    "        epochs=params.get(\"epochs\"),\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, Y_val)\n",
    "    )\n",
    "    Y_pred=model.predict(X_val)\n",
    "    r2 = r2_score(Y_val, Y_pred)\n",
    "    MAE = mean_absolute_error(Y_val, Y_pred)\n",
    "    #print(f\"R-squared {r2}\")\n",
    "    #print(f\"Mean Absolut Error {MAE}\\n\")\n",
    "    \n",
    "    return r2, MAE, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_nn(param_grid, test_size):\n",
    "\n",
    "    grid_search_results_nn= {}\n",
    "    \n",
    "    for period in time_periods:\n",
    "        for res in resolution: \n",
    "\n",
    "            print(f\"time period:{period}, resolution:{res}\")\n",
    "            df = prediction_data.get(period).get(res)\n",
    "\n",
    "            columns= get_columns(df,'cos')\n",
    "            \n",
    "            X_train, Y_train, X_val, Y_val = sorted_train_test_split(df[columns], df['number_of_trips'], test_size)\n",
    "            \n",
    "            grid_object = ParameterGrid(param_grid)\n",
    "            print(f\"Number of parameter combinations: {len(grid_object)}\")\n",
    "            \n",
    "            results = []\n",
    "            delayed_results = []\n",
    "            \n",
    "            start_time= time.time()\n",
    "            \n",
    "            for params in grid_object:\n",
    "                delayed_result = delayed(create_train)(params, len(columns), X_train, Y_train, X_val, Y_val)\n",
    "                #delayed_result = create_train(params, len(columns), X_train, Y_train, X_val, Y_val)\n",
    "                delayed_results.append(delayed_result)\n",
    "                results.append(params)\n",
    "            \n",
    "            computed_r2 = dask.compute(*delayed_results)\n",
    "\n",
    "            results_df = pd.DataFrame(results)\n",
    "\n",
    "            results_df['r2'] = computed_r2\n",
    "            #results_df['r2'] = delayed_results\n",
    "\n",
    "\n",
    "            best_params_index = results_df['r2'].idxmax()\n",
    "            best_params = results_df.iloc[best_params_index]\n",
    "            end_time = time.time()\n",
    "            print(f\"Grid Search took {(end_time-start_time)/60} Minuts\")\n",
    "            joblib.dump(best_params.to_dict(), f'Models/FNN/params/nn_{period}_{res}')\n",
    "            \n",
    "            grid_search_results_nn[(period,res)] = results_df\n",
    "\n",
    "            fnn_measures[(period, res)] = {\n",
    "                'time_gs': (end_time-start_time)/60,\n",
    "                'iteration': 1,\n",
    "                'model_type': 'FNN'\n",
    "            }\n",
    "            \n",
    "    return grid_search_results_nn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def create_train(params, columns, X_train, Y_train, X_val, Y_val):\n",
    "    nn = create_nn(\n",
    "        input_size= columns, \n",
    "        act_function = params['act_function'], \n",
    "        decrease_func = params['decrease_func'], \n",
    "        #factor = params['factor'], \n",
    "        first_layer_factor = params['first_layer_factor'],\n",
    "        optimizer = params['optimizer'],\n",
    "        loss = params['loss'],\n",
    "        metrics = params['metrics'],\n",
    "        dropout_rate = params['dropout_rate'],\n",
    "        learning_rate = params['learning_rate']\n",
    "            )\n",
    "    r2, MAE, history = train_nn(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=nn, params=params)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nn = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [75],\n",
    "    #'optimizer': ['SGD', 'Adam', 'RMSPROP'],\n",
    "    'optimizer': ['SGD'],\n",
    "    #'regressor__model__act_function': ['relu', 'tanh', 'elu'],\n",
    "    'act_function': ['relu'],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "    'loss': ['mse'],\n",
    "    'metrics': ['mae'],\n",
    "    #'regressor__model__first_layer_factor': [0.8, 1, 1.2],\n",
    "    'first_layer_factor': [1],\n",
    "    #'factor': [1],\n",
    "    'decrease_func': [\"exp\", \"linear\"],\n",
    "    'epochs': [200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time period:1, resolution:h3_res_4\n",
      "Number of parameter combinations: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_8828\\3136552141.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_index(inplace=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tf\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39msetLevel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m grid_search_results_nn \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 29\u001b[0m, in \u001b[0;36mgrid_search_nn\u001b[1;34m(param_grid, test_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m     delayed_results\u001b[38;5;241m.\u001b[39mappend(delayed_result)\n\u001b[0;32m     27\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(params)\n\u001b[1;32m---> 29\u001b[0m computed_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdelayed_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[0;32m     33\u001b[0m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m computed_r2\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\dask\\base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# to suppress retracing warning: However, retracing cannot be avoided since the gridsearch require\n",
    "# recreation of the models\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "grid_search_results_nn = grid_search_nn(param_grid_nn, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parallel_coordinates(df, title):\n",
    "\n",
    "    cord_df = df.copy()\n",
    "\n",
    "    cord_df['r2'] = cord_df['r2'].apply(lambda x: max(x, 0.8))\n",
    "\n",
    "    cord_df.drop(columns=['loss', 'metrics', 'epochs','first_layer_factor'], inplace=True)\n",
    "    cols= ['optimizer', 'act_function', 'decrease_func']\n",
    "    category_mapping = {}\n",
    "\n",
    "    for col in cols:\n",
    "        kernel_categories = cord_df[col].astype('category')\n",
    "        category_mapping[col] = dict(enumerate(kernel_categories.cat.categories))\n",
    "\n",
    "        # Convert the kernel column to categorical codes\n",
    "        cord_df[col] = kernel_categories.cat.codes\n",
    "    \n",
    "    fig = px.parallel_coordinates(\n",
    "    cord_df,\n",
    "    color = 'r2',\n",
    "    dimensions = [col for col in cord_df.columns if col != 'r2'],\n",
    "    color_continuous_scale = px.colors.sequential.Viridis,\n",
    "    labels = {col : col for col in cord_df.columns}\n",
    "    )\n",
    "    fig.update_traces(dimensions=[{'label': dim['label'],\n",
    "                                'tickvals': list(category_mapping[dim['label']].keys()),\n",
    "                                'ticktext': list(category_mapping[dim['label']].values())}\n",
    "                                if dim['label'] in cols else dim\n",
    "                                for dim in fig.data[0]['dimensions']])\n",
    "    fig.update_layout(title=title,\n",
    "                      title_x=0.5,\n",
    "                      title_y=1,\n",
    "                     )\n",
    "    fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (period, res), dataframe in grid_search_results_nn.items():\n",
    "    create_parallel_coordinates(dataframe, f\"Time: {time_res}, Res: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models with different time/spatial bins and different test/train spilt techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'act_function': 'relu', 'batch_size': 50, 'decrease_func': 'exp', 'dropout_rate': 0.2, 'epochs': 200, 'first_layer_factor': 1, 'learning_rate': 0.005, 'loss': 'mse', 'metrics': 'mae', 'optimizer': 'RMSPROP', 'r2': 0.9486450738438839}\n"
     ]
    }
   ],
   "source": [
    "#Load the best models and params from the gridsearch and safe them in a dictionary\n",
    "params_fnn={}\n",
    "for period in time_periods:\n",
    "    dict_fnn = {}\n",
    "    for res in resolution:\n",
    "        dict_fnn[res] = joblib.load(f'Models/FNN/params/nn_{period}_{res}')\n",
    "        #break\n",
    "    params_fnn[period] = dict_fnn\n",
    "   #break\n",
    "\n",
    "\n",
    "print(params_fnn.get(1).get('h3_res_4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Split k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start End Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "timeperiod: 1, resolution: h3_res_4\n",
      "22815\n",
      "{'act_function': 'relu', 'batch_size': 50, 'decrease_func': 'exp', 'dropout_rate': 0.2, 'epochs': 10, 'first_layer_factor': 1, 'learning_rate': 0.005, 'loss': 'mse', 'metrics': 'mae', 'optimizer': 'RMSPROP', 'r2': 0.9486450738438839}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_8828\\3136552141.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343/343 [==============================] - 2s 4ms/step - loss: 0.1789 - mae: 0.2913 - val_loss: 0.0319 - val_mae: 0.1266\n",
      "Epoch 2/10\n",
      "343/343 [==============================] - 1s 4ms/step - loss: 0.1158 - mae: 0.2324 - val_loss: 0.0292 - val_mae: 0.1325\n",
      "Epoch 3/10\n",
      "343/343 [==============================] - 1s 3ms/step - loss: 0.1096 - mae: 0.2294 - val_loss: 0.0348 - val_mae: 0.1517\n",
      "Epoch 4/10\n",
      "343/343 [==============================] - 1s 3ms/step - loss: 0.1051 - mae: 0.2238 - val_loss: 0.0443 - val_mae: 0.1382\n",
      "Epoch 5/10\n",
      "343/343 [==============================] - 1s 3ms/step - loss: 0.1058 - mae: 0.2230 - val_loss: 0.0281 - val_mae: 0.1310\n",
      "Epoch 6/10\n",
      "343/343 [==============================] - 1s 3ms/step - loss: 0.1076 - mae: 0.2258 - val_loss: 0.0518 - val_mae: 0.1791\n",
      "Epoch 7/10\n",
      "343/343 [==============================] - 1s 3ms/step - loss: 0.1068 - mae: 0.2240 - val_loss: 0.0279 - val_mae: 0.1352\n",
      "Epoch 8/10\n",
      "343/343 [==============================] - 1s 3ms/step - loss: 0.1007 - mae: 0.2191 - val_loss: 0.0229 - val_mae: 0.1153\n",
      "Epoch 9/10\n",
      "343/343 [==============================] - 1s 3ms/step - loss: 0.1086 - mae: 0.2255 - val_loss: 0.0211 - val_mae: 0.0946\n",
      "Epoch 10/10\n",
      "343/343 [==============================] - 1s 3ms/step - loss: 0.1037 - mae: 0.2222 - val_loss: 0.0252 - val_mae: 0.1274\n",
      "179/179 [==============================] - 0s 526us/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(1, 'h3_res_4')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m r2, MAE, history \u001b[38;5;241m=\u001b[39m train_nn(X_train\u001b[38;5;241m=\u001b[39mX_train, Y_train\u001b[38;5;241m=\u001b[39m Y_train, X_val\u001b[38;5;241m=\u001b[39mX_val, Y_val\u001b[38;5;241m=\u001b[39mY_val,  model\u001b[38;5;241m=\u001b[39mmodel, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 30\u001b[0m \u001b[43mfnn_measures\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_training\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (end_time\u001b[38;5;241m-\u001b[39mstart_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\n\u001b[0;32m     31\u001b[0m fnn_measures[(period, res)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m r2\n\u001b[0;32m     32\u001b[0m fnn_measures[(period, res)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m MAE\n",
      "\u001b[1;31mKeyError\u001b[0m: (1, 'h3_res_4')"
     ]
    }
   ],
   "source": [
    "for period in time_periods:\n",
    "    for res in resolution:\n",
    "        print(f'\\ntimeperiod: {period}, resolution: {res}')\n",
    "        df = prediction_data.get(period).get(res)\n",
    "        columns = get_columns(df, \"cos\")\n",
    "        print(len(df))\n",
    "\n",
    "        params= params_fnn.get(period).get(res)\n",
    "        params['epochs'] = 10\n",
    "        print(params)\n",
    "        model = create_nn(\n",
    "                input_size= len(columns), \n",
    "                act_function = params['act_function'], \n",
    "                decrease_func = params['decrease_func'], \n",
    "                #factor = params['factor'], \n",
    "                first_layer_factor = params['first_layer_factor'],\n",
    "                optimizer = params['optimizer'],\n",
    "                loss = params['loss'],\n",
    "                metrics = params['metrics'],\n",
    "                dropout_rate = params['dropout_rate'],\n",
    "                #kernel_regularizer,\n",
    "                learning_rate = params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        X_train, Y_train, X_val, Y_val = sorted_train_test_split(df[columns], df['number_of_trips'], 0.25)\n",
    "        start_time = time.time()\n",
    "        r2, MAE, history = train_nn(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=model, params=params)\n",
    "        end_time = time.time()\n",
    "\n",
    "        fnn_measures[(period, res)]['time_training'] = (end_time-start_time)/60\n",
    "        fnn_measures[(period, res)]['r2'] = r2\n",
    "        fnn_measures[(period, res)]['MAE'] = MAE\n",
    "        \n",
    "        print(f\"R-squared {r2}\")\n",
    "        print(f\"Mean Absolut Error {MAE}\\n\")\n",
    "        break\n",
    "        model.save(f'Models/FNN/models/fnn_{period}_{res}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of  the different time/spaitla bin combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.166526660323143, 0.13532264530658722, 0.13037121295928955, 0.12831810116767883, 0.12980112433433533, 0.1292315274477005, 0.12695200741291046, 0.12672168016433716, 0.12852521240711212, 0.1282258927822113, 0.12817752361297607, 0.13044136762619019, 0.13032127916812897, 0.12617285549640656, 0.1280498206615448, 0.12975043058395386, 0.12896715104579926, 0.1245030090212822, 0.1282295286655426, 0.1279408186674118], 'mae': [0.2795669436454773, 0.25397711992263794, 0.24925729632377625, 0.24536257982254028, 0.2465682029724121, 0.2467007040977478, 0.2450987845659256, 0.24329087138175964, 0.2461508810520172, 0.2441408485174179, 0.24596379697322845, 0.24716827273368835, 0.24763251841068268, 0.24378736317157745, 0.24398738145828247, 0.24750405550003052, 0.2476472109556198, 0.2419748157262802, 0.24743402004241943, 0.24501581490039825], 'val_loss': [0.048149626702070236, 0.04051646962761879, 0.02821880578994751, 0.03243345022201538, 0.048505257815122604, 0.03195994719862938, 0.02991948090493679, 0.041995570063591, 0.03444195166230202, 0.026059765368700027, 0.03842019662261009, 0.03339068964123726, 0.024434350430965424, 0.04204687103629112, 0.03262811526656151, 0.03364412114024162, 0.02828659489750862, 0.0241321362555027, 0.03710436075925827, 0.03210132196545601], 'val_mae': [0.17520053684711456, 0.14303308725357056, 0.12472131848335266, 0.1341983526945114, 0.16126613318920135, 0.14182338118553162, 0.12237302958965302, 0.1659330427646637, 0.14346575736999512, 0.12039639055728912, 0.14621905982494354, 0.14324627816677094, 0.11869832873344421, 0.16937419772148132, 0.1392320692539215, 0.15262086689472198, 0.1151006892323494, 0.11595162749290466, 0.14827968180179596, 0.12973052263259888]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(history.history['mae']) + 1)\n",
    "# Plotting the traning and validation accuracy\n",
    "plt.clf()\n",
    "acc = history.history['mae']\n",
    "val_acc = history.history['val_mae']\n",
    "plt.plot(epochs, acc, 'b', label='mae')\n",
    "plt.plot(epochs, val_acc, 'r', label='val_mae')\n",
    "plt.plot(epochs, np.ones(len(history.history['mae']))*0.09, 'g', label= 'line')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yscale('log')\n",
    "#plt.grid('true')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "timeperiod: 1, resolution: h3_res_4\n",
      "22815\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_8828\\3136552141.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 1s 5ms/step - loss: 0.2701 - mae: 0.3807 - val_loss: 0.1041 - val_mae: 0.2435\n",
      "Epoch 2/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0640 - mae: 0.1873 - val_loss: 0.0497 - val_mae: 0.1597\n",
      "Epoch 3/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0379 - mae: 0.1377 - val_loss: 0.0364 - val_mae: 0.1329\n",
      "Epoch 4/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0302 - mae: 0.1192 - val_loss: 0.0306 - val_mae: 0.1209\n",
      "Epoch 5/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0267 - mae: 0.1100 - val_loss: 0.0278 - val_mae: 0.1148\n",
      "Epoch 6/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0247 - mae: 0.1043 - val_loss: 0.0260 - val_mae: 0.1100\n",
      "Epoch 7/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0232 - mae: 0.0999 - val_loss: 0.0259 - val_mae: 0.1093\n",
      "Epoch 8/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0220 - mae: 0.0966 - val_loss: 0.0238 - val_mae: 0.1039\n",
      "Epoch 9/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0211 - mae: 0.0940 - val_loss: 0.0240 - val_mae: 0.1035\n",
      "Epoch 10/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0205 - mae: 0.0918 - val_loss: 0.0227 - val_mae: 0.1006\n",
      "Epoch 11/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0199 - mae: 0.0902 - val_loss: 0.0222 - val_mae: 0.0990\n",
      "Epoch 12/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0194 - mae: 0.0888 - val_loss: 0.0220 - val_mae: 0.0977\n",
      "Epoch 13/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0191 - mae: 0.0876 - val_loss: 0.0212 - val_mae: 0.0959\n",
      "Epoch 14/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0188 - mae: 0.0867 - val_loss: 0.0209 - val_mae: 0.0947\n",
      "Epoch 15/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0185 - mae: 0.0857 - val_loss: 0.0207 - val_mae: 0.0938\n",
      "Epoch 16/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0183 - mae: 0.0852 - val_loss: 0.0212 - val_mae: 0.0944\n",
      "Epoch 17/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0181 - mae: 0.0845 - val_loss: 0.0208 - val_mae: 0.0941\n",
      "Epoch 18/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0180 - mae: 0.0841 - val_loss: 0.0208 - val_mae: 0.0938\n",
      "Epoch 19/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0178 - mae: 0.0834 - val_loss: 0.0202 - val_mae: 0.0920\n",
      "Epoch 20/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0177 - mae: 0.0829 - val_loss: 0.0202 - val_mae: 0.0917\n",
      "Epoch 21/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0176 - mae: 0.0826 - val_loss: 0.0208 - val_mae: 0.0927\n",
      "Epoch 22/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0175 - mae: 0.0821 - val_loss: 0.0201 - val_mae: 0.0917\n",
      "Epoch 23/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0174 - mae: 0.0818 - val_loss: 0.0198 - val_mae: 0.0905\n",
      "Epoch 24/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0173 - mae: 0.0815 - val_loss: 0.0199 - val_mae: 0.0904\n",
      "Epoch 25/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0172 - mae: 0.0812 - val_loss: 0.0197 - val_mae: 0.0898\n",
      "Epoch 26/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0171 - mae: 0.0812 - val_loss: 0.0197 - val_mae: 0.0898\n",
      "Epoch 27/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0808 - val_loss: 0.0198 - val_mae: 0.0902\n",
      "Epoch 28/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0169 - mae: 0.0805 - val_loss: 0.0200 - val_mae: 0.0902\n",
      "Epoch 29/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0802 - val_loss: 0.0202 - val_mae: 0.0907\n",
      "Epoch 30/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0801 - val_loss: 0.0198 - val_mae: 0.0908\n",
      "Epoch 31/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0799 - val_loss: 0.0198 - val_mae: 0.0900\n",
      "Epoch 32/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0797 - val_loss: 0.0199 - val_mae: 0.0900\n",
      "Epoch 33/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0795 - val_loss: 0.0198 - val_mae: 0.0892\n",
      "Epoch 34/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0166 - mae: 0.0792 - val_loss: 0.0198 - val_mae: 0.0893\n",
      "Epoch 35/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0791 - val_loss: 0.0195 - val_mae: 0.0893\n",
      "Epoch 36/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0789 - val_loss: 0.0197 - val_mae: 0.0894\n",
      "Epoch 37/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0788 - val_loss: 0.0199 - val_mae: 0.0896\n",
      "Epoch 38/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0164 - mae: 0.0787 - val_loss: 0.0207 - val_mae: 0.0909\n",
      "Epoch 39/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0785 - val_loss: 0.0195 - val_mae: 0.0887\n",
      "Epoch 40/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0783 - val_loss: 0.0201 - val_mae: 0.0910\n",
      "Epoch 41/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0163 - mae: 0.0784 - val_loss: 0.0197 - val_mae: 0.0885\n",
      "Epoch 42/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0781 - val_loss: 0.0195 - val_mae: 0.0888\n",
      "Epoch 43/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0780 - val_loss: 0.0195 - val_mae: 0.0894\n",
      "Epoch 44/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0161 - mae: 0.0780 - val_loss: 0.0200 - val_mae: 0.0899\n",
      "Epoch 45/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0778 - val_loss: 0.0196 - val_mae: 0.0881\n",
      "Epoch 46/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0775 - val_loss: 0.0193 - val_mae: 0.0887\n",
      "Epoch 47/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0775 - val_loss: 0.0192 - val_mae: 0.0883\n",
      "Epoch 48/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0776 - val_loss: 0.0196 - val_mae: 0.0889\n",
      "Epoch 49/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0773 - val_loss: 0.0202 - val_mae: 0.0894\n",
      "Epoch 50/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0773 - val_loss: 0.0195 - val_mae: 0.0890\n",
      "Epoch 51/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0771 - val_loss: 0.0199 - val_mae: 0.0889\n",
      "Epoch 52/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0158 - mae: 0.0772 - val_loss: 0.0192 - val_mae: 0.0882\n",
      "Epoch 53/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0157 - mae: 0.0770 - val_loss: 0.0193 - val_mae: 0.0886\n",
      "Epoch 54/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0157 - mae: 0.0769 - val_loss: 0.0190 - val_mae: 0.0878\n",
      "Epoch 55/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0156 - mae: 0.0769 - val_loss: 0.0194 - val_mae: 0.0887\n",
      "Epoch 56/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0156 - mae: 0.0766 - val_loss: 0.0194 - val_mae: 0.0887\n",
      "Epoch 57/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0156 - mae: 0.0766 - val_loss: 0.0194 - val_mae: 0.0884\n",
      "Epoch 58/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0156 - mae: 0.0765 - val_loss: 0.0190 - val_mae: 0.0882\n",
      "Epoch 59/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0155 - mae: 0.0764 - val_loss: 0.0192 - val_mae: 0.0882\n",
      "Epoch 60/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0155 - mae: 0.0764 - val_loss: 0.0194 - val_mae: 0.0881\n",
      "Epoch 61/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0153 - mae: 0.0762 - val_loss: 0.0195 - val_mae: 0.0888\n",
      "Epoch 62/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0154 - mae: 0.0762 - val_loss: 0.0191 - val_mae: 0.0880\n",
      "Epoch 63/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0153 - mae: 0.0761 - val_loss: 0.0187 - val_mae: 0.0866\n",
      "Epoch 64/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0152 - mae: 0.0759 - val_loss: 0.0193 - val_mae: 0.0884\n",
      "Epoch 65/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0152 - mae: 0.0757 - val_loss: 0.0196 - val_mae: 0.0896\n",
      "Epoch 66/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0152 - mae: 0.0757 - val_loss: 0.0190 - val_mae: 0.0880\n",
      "Epoch 67/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0151 - mae: 0.0756 - val_loss: 0.0189 - val_mae: 0.0880\n",
      "Epoch 68/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0151 - mae: 0.0755 - val_loss: 0.0190 - val_mae: 0.0873\n",
      "Epoch 69/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0151 - mae: 0.0754 - val_loss: 0.0192 - val_mae: 0.0880\n",
      "Epoch 70/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0150 - mae: 0.0756 - val_loss: 0.0186 - val_mae: 0.0865\n",
      "Epoch 71/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0149 - mae: 0.0752 - val_loss: 0.0184 - val_mae: 0.0859\n",
      "Epoch 72/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0149 - mae: 0.0752 - val_loss: 0.0186 - val_mae: 0.0865\n",
      "Epoch 73/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0148 - mae: 0.0749 - val_loss: 0.0200 - val_mae: 0.0894\n",
      "Epoch 74/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0148 - mae: 0.0748 - val_loss: 0.0197 - val_mae: 0.0885\n",
      "Epoch 75/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0148 - mae: 0.0749 - val_loss: 0.0185 - val_mae: 0.0868\n",
      "Epoch 76/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0148 - mae: 0.0748 - val_loss: 0.0186 - val_mae: 0.0865\n",
      "Epoch 77/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0147 - mae: 0.0746 - val_loss: 0.0190 - val_mae: 0.0875\n",
      "Epoch 78/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0147 - mae: 0.0746 - val_loss: 0.0193 - val_mae: 0.0873\n",
      "Epoch 79/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0146 - mae: 0.0743 - val_loss: 0.0186 - val_mae: 0.0862\n",
      "Epoch 80/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0145 - mae: 0.0743 - val_loss: 0.0190 - val_mae: 0.0871\n",
      "Epoch 81/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0146 - mae: 0.0743 - val_loss: 0.0183 - val_mae: 0.0858\n",
      "Epoch 82/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0145 - mae: 0.0741 - val_loss: 0.0186 - val_mae: 0.0866\n",
      "Epoch 83/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0145 - mae: 0.0742 - val_loss: 0.0184 - val_mae: 0.0870\n",
      "Epoch 84/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0144 - mae: 0.0741 - val_loss: 0.0183 - val_mae: 0.0856\n",
      "Epoch 85/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0144 - mae: 0.0739 - val_loss: 0.0182 - val_mae: 0.0854\n",
      "Epoch 86/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0144 - mae: 0.0737 - val_loss: 0.0182 - val_mae: 0.0860\n",
      "Epoch 87/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0143 - mae: 0.0738 - val_loss: 0.0182 - val_mae: 0.0850\n",
      "Epoch 88/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0143 - mae: 0.0735 - val_loss: 0.0181 - val_mae: 0.0846\n",
      "Epoch 89/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0143 - mae: 0.0735 - val_loss: 0.0183 - val_mae: 0.0854\n",
      "Epoch 90/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0142 - mae: 0.0733 - val_loss: 0.0184 - val_mae: 0.0866\n",
      "Epoch 91/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0142 - mae: 0.0733 - val_loss: 0.0177 - val_mae: 0.0843\n",
      "Epoch 92/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0141 - mae: 0.0733 - val_loss: 0.0183 - val_mae: 0.0853\n",
      "Epoch 93/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0142 - mae: 0.0732 - val_loss: 0.0181 - val_mae: 0.0852\n",
      "Epoch 94/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0731 - val_loss: 0.0180 - val_mae: 0.0844\n",
      "Epoch 95/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0141 - mae: 0.0730 - val_loss: 0.0184 - val_mae: 0.0863\n",
      "Epoch 96/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0140 - mae: 0.0729 - val_loss: 0.0181 - val_mae: 0.0852\n",
      "Epoch 97/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0140 - mae: 0.0728 - val_loss: 0.0180 - val_mae: 0.0848\n",
      "Epoch 98/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0140 - mae: 0.0728 - val_loss: 0.0180 - val_mae: 0.0848\n",
      "Epoch 99/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0728 - val_loss: 0.0181 - val_mae: 0.0847\n",
      "Epoch 100/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0726 - val_loss: 0.0180 - val_mae: 0.0845\n",
      "Epoch 101/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0726 - val_loss: 0.0179 - val_mae: 0.0839\n",
      "Epoch 102/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0139 - mae: 0.0724 - val_loss: 0.0175 - val_mae: 0.0837\n",
      "Epoch 103/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0139 - mae: 0.0724 - val_loss: 0.0175 - val_mae: 0.0835\n",
      "Epoch 104/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0139 - mae: 0.0724 - val_loss: 0.0177 - val_mae: 0.0844\n",
      "Epoch 105/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0723 - val_loss: 0.0177 - val_mae: 0.0842\n",
      "Epoch 106/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0723 - val_loss: 0.0176 - val_mae: 0.0837\n",
      "Epoch 107/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0722 - val_loss: 0.0177 - val_mae: 0.0845\n",
      "Epoch 108/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0721 - val_loss: 0.0178 - val_mae: 0.0845\n",
      "Epoch 109/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0722 - val_loss: 0.0180 - val_mae: 0.0846\n",
      "Epoch 110/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0720 - val_loss: 0.0174 - val_mae: 0.0830\n",
      "Epoch 111/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0721 - val_loss: 0.0174 - val_mae: 0.0833\n",
      "Epoch 112/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0720 - val_loss: 0.0177 - val_mae: 0.0851\n",
      "Epoch 113/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0136 - mae: 0.0719 - val_loss: 0.0175 - val_mae: 0.0834\n",
      "Epoch 114/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0137 - mae: 0.0718 - val_loss: 0.0178 - val_mae: 0.0848\n",
      "Epoch 115/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0136 - mae: 0.0717 - val_loss: 0.0173 - val_mae: 0.0834\n",
      "Epoch 116/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0136 - mae: 0.0719 - val_loss: 0.0175 - val_mae: 0.0839\n",
      "Epoch 117/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0136 - mae: 0.0716 - val_loss: 0.0174 - val_mae: 0.0836\n",
      "Epoch 118/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0136 - mae: 0.0718 - val_loss: 0.0175 - val_mae: 0.0833\n",
      "Epoch 119/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0135 - mae: 0.0715 - val_loss: 0.0175 - val_mae: 0.0838\n",
      "Epoch 120/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0135 - mae: 0.0715 - val_loss: 0.0173 - val_mae: 0.0826\n",
      "Epoch 121/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0135 - mae: 0.0715 - val_loss: 0.0172 - val_mae: 0.0834\n",
      "Epoch 122/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0135 - mae: 0.0715 - val_loss: 0.0176 - val_mae: 0.0845\n",
      "Epoch 123/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0135 - mae: 0.0715 - val_loss: 0.0171 - val_mae: 0.0821\n",
      "Epoch 124/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0134 - mae: 0.0713 - val_loss: 0.0171 - val_mae: 0.0825\n",
      "Epoch 125/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0135 - mae: 0.0716 - val_loss: 0.0175 - val_mae: 0.0833\n",
      "Epoch 126/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0135 - mae: 0.0712 - val_loss: 0.0173 - val_mae: 0.0828\n",
      "Epoch 127/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0134 - mae: 0.0712 - val_loss: 0.0174 - val_mae: 0.0835\n",
      "Epoch 128/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0134 - mae: 0.0712 - val_loss: 0.0176 - val_mae: 0.0836\n",
      "Epoch 129/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0134 - mae: 0.0713 - val_loss: 0.0178 - val_mae: 0.0837\n",
      "Epoch 130/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0134 - mae: 0.0712 - val_loss: 0.0171 - val_mae: 0.0820\n",
      "Epoch 131/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0134 - mae: 0.0711 - val_loss: 0.0173 - val_mae: 0.0829\n",
      "Epoch 132/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0134 - mae: 0.0711 - val_loss: 0.0171 - val_mae: 0.0825\n",
      "Epoch 133/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0133 - mae: 0.0711 - val_loss: 0.0173 - val_mae: 0.0833\n",
      "Epoch 134/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0133 - mae: 0.0710 - val_loss: 0.0178 - val_mae: 0.0844\n",
      "Epoch 135/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0133 - mae: 0.0709 - val_loss: 0.0171 - val_mae: 0.0824\n",
      "Epoch 136/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0133 - mae: 0.0708 - val_loss: 0.0173 - val_mae: 0.0829\n",
      "Epoch 137/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0133 - mae: 0.0709 - val_loss: 0.0173 - val_mae: 0.0829\n",
      "Epoch 138/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0708 - val_loss: 0.0171 - val_mae: 0.0823\n",
      "Epoch 139/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0133 - mae: 0.0708 - val_loss: 0.0171 - val_mae: 0.0827\n",
      "Epoch 140/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0707 - val_loss: 0.0172 - val_mae: 0.0826\n",
      "Epoch 141/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0709 - val_loss: 0.0171 - val_mae: 0.0824\n",
      "Epoch 142/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0708 - val_loss: 0.0174 - val_mae: 0.0835\n",
      "Epoch 143/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0706 - val_loss: 0.0170 - val_mae: 0.0821\n",
      "Epoch 144/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0706 - val_loss: 0.0171 - val_mae: 0.0822\n",
      "Epoch 145/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0706 - val_loss: 0.0170 - val_mae: 0.0822\n",
      "Epoch 146/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0707 - val_loss: 0.0178 - val_mae: 0.0847\n",
      "Epoch 147/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0707 - val_loss: 0.0170 - val_mae: 0.0816\n",
      "Epoch 148/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0132 - mae: 0.0705 - val_loss: 0.0168 - val_mae: 0.0813\n",
      "Epoch 149/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0131 - mae: 0.0706 - val_loss: 0.0173 - val_mae: 0.0829\n",
      "Epoch 150/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0131 - mae: 0.0705 - val_loss: 0.0170 - val_mae: 0.0820\n",
      "Epoch 151/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0131 - mae: 0.0705 - val_loss: 0.0168 - val_mae: 0.0820\n",
      "Epoch 152/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0131 - mae: 0.0705 - val_loss: 0.0177 - val_mae: 0.0834\n",
      "Epoch 153/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0704 - val_loss: 0.0170 - val_mae: 0.0824\n",
      "Epoch 154/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0705 - val_loss: 0.0171 - val_mae: 0.0821\n",
      "Epoch 155/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0705 - val_loss: 0.0168 - val_mae: 0.0814\n",
      "Epoch 156/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0703 - val_loss: 0.0169 - val_mae: 0.0821\n",
      "Epoch 157/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0704 - val_loss: 0.0170 - val_mae: 0.0820\n",
      "Epoch 158/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0130 - mae: 0.0702 - val_loss: 0.0174 - val_mae: 0.0834\n",
      "Epoch 159/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0702 - val_loss: 0.0170 - val_mae: 0.0818\n",
      "Epoch 160/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0131 - mae: 0.0704 - val_loss: 0.0169 - val_mae: 0.0815\n",
      "Epoch 161/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0702 - val_loss: 0.0168 - val_mae: 0.0824\n",
      "Epoch 162/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0701 - val_loss: 0.0171 - val_mae: 0.0827\n",
      "Epoch 163/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0702 - val_loss: 0.0168 - val_mae: 0.0812\n",
      "Epoch 164/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0702 - val_loss: 0.0169 - val_mae: 0.0815\n",
      "Epoch 165/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0701 - val_loss: 0.0172 - val_mae: 0.0824\n",
      "Epoch 166/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0699 - val_loss: 0.0169 - val_mae: 0.0816\n",
      "Epoch 167/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0701 - val_loss: 0.0169 - val_mae: 0.0815\n",
      "Epoch 168/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0699 - val_loss: 0.0172 - val_mae: 0.0823\n",
      "Epoch 169/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0701 - val_loss: 0.0168 - val_mae: 0.0810\n",
      "Epoch 170/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0700 - val_loss: 0.0178 - val_mae: 0.0846\n",
      "Epoch 171/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0701 - val_loss: 0.0168 - val_mae: 0.0810\n",
      "Epoch 172/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0129 - mae: 0.0698 - val_loss: 0.0170 - val_mae: 0.0821\n",
      "Epoch 173/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0129 - mae: 0.0699 - val_loss: 0.0166 - val_mae: 0.0809\n",
      "Epoch 174/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0130 - mae: 0.0700 - val_loss: 0.0170 - val_mae: 0.0820\n",
      "Epoch 175/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0129 - mae: 0.0698 - val_loss: 0.0171 - val_mae: 0.0823\n",
      "Epoch 176/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0129 - mae: 0.0699 - val_loss: 0.0169 - val_mae: 0.0814\n",
      "Epoch 177/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0129 - mae: 0.0698 - val_loss: 0.0169 - val_mae: 0.0813\n",
      "Epoch 178/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0129 - mae: 0.0698 - val_loss: 0.0167 - val_mae: 0.0811\n",
      "Epoch 179/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0698 - val_loss: 0.0167 - val_mae: 0.0809\n",
      "Epoch 180/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0696 - val_loss: 0.0167 - val_mae: 0.0817\n",
      "Epoch 181/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0129 - mae: 0.0698 - val_loss: 0.0175 - val_mae: 0.0833\n",
      "Epoch 182/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0129 - mae: 0.0698 - val_loss: 0.0165 - val_mae: 0.0805\n",
      "Epoch 183/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0695 - val_loss: 0.0166 - val_mae: 0.0809\n",
      "Epoch 184/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0697 - val_loss: 0.0166 - val_mae: 0.0808\n",
      "Epoch 185/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0697 - val_loss: 0.0174 - val_mae: 0.0833\n",
      "Epoch 186/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0696 - val_loss: 0.0169 - val_mae: 0.0816\n",
      "Epoch 187/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0695 - val_loss: 0.0173 - val_mae: 0.0824\n",
      "Epoch 188/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0694 - val_loss: 0.0171 - val_mae: 0.0825\n",
      "Epoch 189/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0696 - val_loss: 0.0171 - val_mae: 0.0820\n",
      "Epoch 190/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0695 - val_loss: 0.0167 - val_mae: 0.0809\n",
      "Epoch 191/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0694 - val_loss: 0.0169 - val_mae: 0.0816\n",
      "Epoch 192/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0694 - val_loss: 0.0167 - val_mae: 0.0810\n",
      "Epoch 193/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0695 - val_loss: 0.0168 - val_mae: 0.0813\n",
      "Epoch 194/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0693 - val_loss: 0.0171 - val_mae: 0.0822\n",
      "Epoch 195/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0695 - val_loss: 0.0171 - val_mae: 0.0819\n",
      "Epoch 196/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0693 - val_loss: 0.0172 - val_mae: 0.0825\n",
      "Epoch 197/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0693 - val_loss: 0.0168 - val_mae: 0.0813\n",
      "Epoch 198/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0694 - val_loss: 0.0167 - val_mae: 0.0810\n",
      "Epoch 199/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0692 - val_loss: 0.0167 - val_mae: 0.0806\n",
      "Epoch 200/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0692 - val_loss: 0.0171 - val_mae: 0.0816\n",
      "Epoch 201/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0692 - val_loss: 0.0166 - val_mae: 0.0806\n",
      "Epoch 202/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0693 - val_loss: 0.0168 - val_mae: 0.0812\n",
      "Epoch 203/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0693 - val_loss: 0.0168 - val_mae: 0.0814\n",
      "Epoch 204/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0693 - val_loss: 0.0168 - val_mae: 0.0810\n",
      "Epoch 205/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0691 - val_loss: 0.0166 - val_mae: 0.0812\n",
      "Epoch 206/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0690 - val_loss: 0.0166 - val_mae: 0.0809\n",
      "Epoch 207/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0693 - val_loss: 0.0167 - val_mae: 0.0813\n",
      "Epoch 208/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0690 - val_loss: 0.0169 - val_mae: 0.0815\n",
      "Epoch 209/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0690 - val_loss: 0.0169 - val_mae: 0.0815\n",
      "Epoch 210/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0689 - val_loss: 0.0167 - val_mae: 0.0807\n",
      "Epoch 211/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0690 - val_loss: 0.0170 - val_mae: 0.0829\n",
      "Epoch 212/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0691 - val_loss: 0.0165 - val_mae: 0.0807\n",
      "Epoch 213/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0690 - val_loss: 0.0164 - val_mae: 0.0802\n",
      "Epoch 214/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0688 - val_loss: 0.0167 - val_mae: 0.0816\n",
      "Epoch 215/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0690 - val_loss: 0.0167 - val_mae: 0.0811\n",
      "Epoch 216/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0688 - val_loss: 0.0167 - val_mae: 0.0810\n",
      "Epoch 217/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0688 - val_loss: 0.0167 - val_mae: 0.0809\n",
      "Epoch 218/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0688 - val_loss: 0.0168 - val_mae: 0.0822\n",
      "Epoch 219/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0690 - val_loss: 0.0163 - val_mae: 0.0800\n",
      "Epoch 220/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0126 - mae: 0.0688 - val_loss: 0.0168 - val_mae: 0.0811\n",
      "Epoch 221/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0125 - mae: 0.0688 - val_loss: 0.0171 - val_mae: 0.0820\n",
      "Epoch 222/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0125 - mae: 0.0688 - val_loss: 0.0168 - val_mae: 0.0810\n",
      "Epoch 223/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0125 - mae: 0.0686 - val_loss: 0.0170 - val_mae: 0.0818\n",
      "Epoch 224/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0125 - mae: 0.0687 - val_loss: 0.0167 - val_mae: 0.0809\n",
      "Epoch 225/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0125 - mae: 0.0687 - val_loss: 0.0166 - val_mae: 0.0803\n",
      "Epoch 226/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0125 - mae: 0.0687 - val_loss: 0.0167 - val_mae: 0.0806\n",
      "Epoch 227/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0686 - val_loss: 0.0166 - val_mae: 0.0808\n",
      "Epoch 228/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0687 - val_loss: 0.0166 - val_mae: 0.0807\n",
      "Epoch 229/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0685 - val_loss: 0.0176 - val_mae: 0.0823\n",
      "Epoch 230/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0686 - val_loss: 0.0165 - val_mae: 0.0806\n",
      "Epoch 231/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0684 - val_loss: 0.0166 - val_mae: 0.0804\n",
      "Epoch 232/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0125 - mae: 0.0686 - val_loss: 0.0164 - val_mae: 0.0801\n",
      "Epoch 233/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0685 - val_loss: 0.0163 - val_mae: 0.0798\n",
      "Epoch 234/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0685 - val_loss: 0.0165 - val_mae: 0.0804\n",
      "Epoch 235/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0685 - val_loss: 0.0167 - val_mae: 0.0814\n",
      "Epoch 236/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0684 - val_loss: 0.0174 - val_mae: 0.0827\n",
      "Epoch 237/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0684 - val_loss: 0.0168 - val_mae: 0.0812\n",
      "Epoch 238/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0684 - val_loss: 0.0163 - val_mae: 0.0798\n",
      "Epoch 239/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0683 - val_loss: 0.0166 - val_mae: 0.0807\n",
      "Epoch 240/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0124 - mae: 0.0683 - val_loss: 0.0166 - val_mae: 0.0809\n",
      "Epoch 241/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0124 - mae: 0.0684 - val_loss: 0.0168 - val_mae: 0.0815\n",
      "Epoch 242/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0124 - mae: 0.0682 - val_loss: 0.0166 - val_mae: 0.0809\n",
      "Epoch 243/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0684 - val_loss: 0.0165 - val_mae: 0.0803\n",
      "Epoch 244/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0684 - val_loss: 0.0169 - val_mae: 0.0816\n",
      "Epoch 245/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0682 - val_loss: 0.0167 - val_mae: 0.0804\n",
      "Epoch 246/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0681 - val_loss: 0.0165 - val_mae: 0.0801\n",
      "Epoch 247/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0683 - val_loss: 0.0166 - val_mae: 0.0811\n",
      "Epoch 248/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0682 - val_loss: 0.0167 - val_mae: 0.0812\n",
      "Epoch 249/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0124 - mae: 0.0681 - val_loss: 0.0168 - val_mae: 0.0812\n",
      "Epoch 250/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0680 - val_loss: 0.0163 - val_mae: 0.0799\n",
      "Epoch 251/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0683 - val_loss: 0.0163 - val_mae: 0.0806\n",
      "Epoch 252/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0681 - val_loss: 0.0169 - val_mae: 0.0812\n",
      "Epoch 253/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0680 - val_loss: 0.0168 - val_mae: 0.0814\n",
      "Epoch 254/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0681 - val_loss: 0.0165 - val_mae: 0.0804\n",
      "Epoch 255/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0680 - val_loss: 0.0164 - val_mae: 0.0798\n",
      "Epoch 256/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0679 - val_loss: 0.0164 - val_mae: 0.0801\n",
      "Epoch 257/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0681 - val_loss: 0.0164 - val_mae: 0.0797\n",
      "Epoch 258/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0679 - val_loss: 0.0170 - val_mae: 0.0818\n",
      "Epoch 259/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0122 - mae: 0.0678 - val_loss: 0.0168 - val_mae: 0.0808\n",
      "Epoch 260/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0679 - val_loss: 0.0166 - val_mae: 0.0804\n",
      "Epoch 261/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0679 - val_loss: 0.0164 - val_mae: 0.0799\n",
      "Epoch 262/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0680 - val_loss: 0.0163 - val_mae: 0.0795\n",
      "Epoch 263/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0680 - val_loss: 0.0163 - val_mae: 0.0797\n",
      "Epoch 264/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0122 - mae: 0.0678 - val_loss: 0.0168 - val_mae: 0.0813\n",
      "Epoch 265/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0679 - val_loss: 0.0168 - val_mae: 0.0812\n",
      "Epoch 266/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0122 - mae: 0.0679 - val_loss: 0.0166 - val_mae: 0.0805\n",
      "Epoch 267/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0122 - mae: 0.0678 - val_loss: 0.0167 - val_mae: 0.0812\n",
      "Epoch 268/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0122 - mae: 0.0679 - val_loss: 0.0165 - val_mae: 0.0806\n",
      "Epoch 269/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0122 - mae: 0.0677 - val_loss: 0.0164 - val_mae: 0.0797\n",
      "Epoch 270/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0122 - mae: 0.0678 - val_loss: 0.0163 - val_mae: 0.0797\n",
      "Epoch 271/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0122 - mae: 0.0680 - val_loss: 0.0163 - val_mae: 0.0794\n",
      "Epoch 272/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0677 - val_loss: 0.0162 - val_mae: 0.0795\n",
      "Epoch 273/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0677 - val_loss: 0.0165 - val_mae: 0.0804\n",
      "Epoch 274/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0676 - val_loss: 0.0164 - val_mae: 0.0797\n",
      "Epoch 275/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0122 - mae: 0.0676 - val_loss: 0.0167 - val_mae: 0.0807\n",
      "Epoch 276/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0121 - mae: 0.0675 - val_loss: 0.0164 - val_mae: 0.0795\n",
      "Epoch 277/300\n",
      "134/134 [==============================] - 1s 4ms/step - loss: 0.0122 - mae: 0.0676 - val_loss: 0.0165 - val_mae: 0.0802\n",
      "Epoch 278/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0121 - mae: 0.0676 - val_loss: 0.0167 - val_mae: 0.0804\n",
      "Epoch 279/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0674 - val_loss: 0.0166 - val_mae: 0.0804\n",
      "Epoch 280/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0675 - val_loss: 0.0168 - val_mae: 0.0809\n",
      "Epoch 281/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0676 - val_loss: 0.0165 - val_mae: 0.0800\n",
      "Epoch 282/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0675 - val_loss: 0.0163 - val_mae: 0.0793\n",
      "Epoch 283/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0676 - val_loss: 0.0162 - val_mae: 0.0794\n",
      "Epoch 284/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0673 - val_loss: 0.0166 - val_mae: 0.0802\n",
      "Epoch 285/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0673 - val_loss: 0.0169 - val_mae: 0.0811\n",
      "Epoch 286/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0121 - mae: 0.0674 - val_loss: 0.0162 - val_mae: 0.0791\n",
      "Epoch 287/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0673 - val_loss: 0.0165 - val_mae: 0.0801\n",
      "Epoch 288/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0121 - mae: 0.0673 - val_loss: 0.0165 - val_mae: 0.0800\n",
      "Epoch 289/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0673 - val_loss: 0.0166 - val_mae: 0.0804\n",
      "Epoch 290/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0672 - val_loss: 0.0164 - val_mae: 0.0797\n",
      "Epoch 291/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0673 - val_loss: 0.0169 - val_mae: 0.0816\n",
      "Epoch 292/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0673 - val_loss: 0.0168 - val_mae: 0.0815\n",
      "Epoch 293/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0671 - val_loss: 0.0167 - val_mae: 0.0807\n",
      "Epoch 294/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0672 - val_loss: 0.0163 - val_mae: 0.0793\n",
      "Epoch 295/300\n",
      "134/134 [==============================] - 0s 4ms/step - loss: 0.0120 - mae: 0.0673 - val_loss: 0.0164 - val_mae: 0.0795\n",
      "Epoch 296/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0671 - val_loss: 0.0167 - val_mae: 0.0812\n",
      "Epoch 297/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0673 - val_loss: 0.0167 - val_mae: 0.0806\n",
      "Epoch 298/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0671 - val_loss: 0.0165 - val_mae: 0.0799\n",
      "Epoch 299/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0672 - val_loss: 0.0162 - val_mae: 0.0795\n",
      "Epoch 300/300\n",
      "134/134 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0671 - val_loss: 0.0162 - val_mae: 0.0792\n",
      "179/179 [==============================] - 0s 574us/step\n"
     ]
    }
   ],
   "source": [
    "for period in time_periods:\n",
    "    for res in resolution:\n",
    "        print(f'\\ntimeperiod: {period}, resolution: {res}')\n",
    "        df = prediction_data.get(period).get(res)\n",
    "        columns = get_columns(df, \"cos\")\n",
    "        print(len(df))\n",
    "\n",
    "        nn_model = Sequential()\n",
    "        nn_model.add(Dense(16, input_shape=(16,), activation='tanh'))\n",
    "        model.add(BatchNormalization())\n",
    "        nn_model.add(Dense(8, activation='tanh'))\n",
    "        model.add(BatchNormalization())\n",
    "        nn_model.add(Dense(4, activation='tanh'))\n",
    "        model.add(BatchNormalization())\n",
    "        nn_model.add(Dense(1))\n",
    "        nn_model.save_weights('initial_weights')\n",
    "\n",
    "        params={\n",
    "            \"learning_rate\": 0.0005,\n",
    "            \"optimizer\":\"RMSPROP\",\n",
    "            \"loss\":\"mse\",\n",
    "            \"loss_weights\":None,\n",
    "            \"metrics\":['mae'],\n",
    "            \"batch_size\":128,\n",
    "            \"epochs\":300,\n",
    "            \"verbose\":1,\n",
    "            'dropout_rate': 0.2\n",
    "        }\n",
    "        if params['optimizer'] == 'Adam':\n",
    "            optimizer_instance = Adam(learning_rate=params['learning_rate'])\n",
    "        elif params['optimizer'] == 'SGD':\n",
    "            optimizer_instance = SGD(learning_rate=params['learning_rate'])\n",
    "        elif params['optimizer'] == 'RMSPROP':\n",
    "            optimizer_instance = RMSprop(learning_rate=params['learning_rate'])\n",
    "        else:\n",
    "            print(\"invalid optimizer\")\n",
    "        nn_model.compile(\n",
    "            optimizer=optimizer_instance,\n",
    "            loss=params['loss'],\n",
    "            metrics=params['metrics']\n",
    "        )\n",
    "\n",
    "        X_train, Y_train, X_val, Y_val = sorted_train_test_split(df[columns], df['number_of_trips'], 0.25)\n",
    "        start_time = time.time()\n",
    "        r2, MAE, history = train_nn(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=nn_model, params=params)\n",
    "        end_time = time.time()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837978539218\n"
     ]
    }
   ],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_from_measures(measure, score, title, ax=None, cmap = \"RdYlGn\"):\n",
    "\n",
    "    x_values = sorted(set(keys[0] for keys in measure))\n",
    "    y_values = sorted(set(keys[1] for keys in measure))\n",
    "\n",
    "    matrix = np.zeros((len(y_values), len(x_values)))\n",
    "\n",
    "    for (x,y), element in measure.items():\n",
    "        x_index = x_values.index(x)\n",
    "        y_index = y_values.index(y)\n",
    "        matrix[y_index, x_index] = element[score]\n",
    "\n",
    "    sns.heatmap(matrix, \n",
    "                xticklabels= x_values, \n",
    "                yticklabels=y_values, \n",
    "                annot=True, cmap=cmap,  # cmap=\"YlGnBu\" \"RdYlGn\"\n",
    "                cbar_kws={'label': f\"{score} Value\"}, \n",
    "                ax=ax)\n",
    "\n",
    "    ax.set_xlabel('Temporal resolution')\n",
    "    ax.set_ylabel('Spatial resolution')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation fmin which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m , figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m16\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplot_heatmap_from_measures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfnn_measures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mR2 Values for LSVR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plot_heatmap_from_measures(fnn_measures, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_training\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Time for LSVR in Min\u001b[39m\u001b[38;5;124m'\u001b[39m, ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m1\u001b[39m], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYlGnBu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 13\u001b[0m, in \u001b[0;36mplot_heatmap_from_measures\u001b[1;34m(measure, score, title, ax, cmap)\u001b[0m\n\u001b[0;32m     10\u001b[0m     y_index \u001b[38;5;241m=\u001b[39m y_values\u001b[38;5;241m.\u001b[39mindex(y)\n\u001b[0;32m     11\u001b[0m     matrix[y_index, x_index] \u001b[38;5;241m=\u001b[39m element[score]\n\u001b[1;32m---> 13\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mannot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# cmap=\"YlGnBu\" \"RdYlGn\"\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscore\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m Value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemporal resolution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpatial resolution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\seaborn\\matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[0;32m    451\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinewidths\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linewidths\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\seaborn\\matrix.py:163\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[1;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mylabel \u001b[38;5;241m=\u001b[39m ylabel \u001b[38;5;28;01mif\u001b[39;00m ylabel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Determine good default values for the colormapping\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_determine_cmap_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Sort out the annotations\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m annot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m annot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\seaborn\\matrix.py:202\u001b[0m, in \u001b[0;36m_HeatMapper._determine_cmap_params\u001b[1;34m(self, plot_data, vmin, vmax, cmap, center, robust)\u001b[0m\n\u001b[0;32m    200\u001b[0m         vmin \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanpercentile(calc_data, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m         vmin \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalc_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vmax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m robust:\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:343\u001b[0m, in \u001b[0;36mnanmin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m    338\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m where\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(a) \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;129;01mand\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;66;03m# Fast, but not safe for subclasses of ndarray, or object arrays,\u001b[39;00m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;66;03m# which do not implement isnan (gh-9009), or fmin correctly (gh-8975)\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     res \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfmin\u001b[38;5;241m.\u001b[39mreduce(a, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(res)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    345\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll-NaN slice encountered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[0;32m    346\u001b[0m                       stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation fmin which has no identity"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2,1 , figsize=(10,16))\n",
    "\n",
    "plot_heatmap_from_measures(fnn_measures, 'r2', 'R2 Values for LSVR', ax=ax[0])\n",
    "plot_heatmap_from_measures(fnn_measures, 'time_training', 'Training Time for LSVR in Min', ax=ax[1], cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aanalyize the models with XAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply different NN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional NN (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent NN (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Networks (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernalized NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare SVM and NN models in terms of predictive performance and computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,2 , figsize=(16, 24))\n",
    "i=0\n",
    "for period in time_periods:\n",
    "    list_measures = []\n",
    "    df_barplot = pd.DataFrame()\n",
    "    for res in resolution:\n",
    "        list_measures.append(fnn_measures[(period, res)])\n",
    "        list_measures.append(lsvr_measures[(period, res)])\n",
    "        list_measures.append(ksvr_measures[(period, res)])\n",
    "        df = pd.DataFrame(list_measures)\n",
    "        df['res'] = res\n",
    "        df_barplot = pd.concat([df_barplot, df], ignore_index=True)\n",
    "    sns.barplot(df_barplot, x='res', y='r2', hue= 'model_type', ax = ax[i,0], gap=0.1, saturation = 0.95, errorbar= None)\n",
    "\n",
    "    handles, labels = ax[i, 0].get_legend_handles_labels()\n",
    "    ax[i, 0].legend(handles, labels, loc='lower right', framealpha = 1)\n",
    "    ax[i, 0].set_axisbelow(True)\n",
    "    ax[i, 0].grid(True, which='both', linestyle='-', linewidth=0.7, alpha = 0.5)\n",
    "    ax[i, 0].set_yticks(np.arange(11)/ 10)\n",
    "\n",
    "    sns.barplot(df_barplot, x='res', y='time_training', hue= 'model_type', ax = ax[i,1], gap=0.1, saturation = 0.95, errorbar= None)\n",
    "    ax[i, 1].set_axisbelow(True)\n",
    "    ax[i, 1].grid(True, which='both', linestyle='-', linewidth=0.7, alpha = 0.5)\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
