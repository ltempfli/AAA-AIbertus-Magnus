{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Additionial-preprocessing\" data-toc-modified-id=\"Additionial-preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Additionial preprocessing</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Transformations\" data-toc-modified-id=\"Data-Transformations-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Data Transformations</a></span></li><li><span><a href=\"#Test/Train-split-Techniques\" data-toc-modified-id=\"Test/Train-split-Techniques-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Test/Train split Techniques</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Support Vector Machines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Perform-Grid-search\" data-toc-modified-id=\"Perform-Grid-search-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Perform Grid search</a></span></li><li><span><a href=\"#Train-the-models-with-different-time/spatial\" data-toc-modified-id=\"Train-the-models-with-different-time/spatial-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Train the models with different time/spatial</a></span></li><li><span><a href=\"#Compare-the-results-of-the-different-time/spaitla-bin-combinations\" data-toc-modified-id=\"Compare-the-results-of-the-different-time/spaitla-bin-combinations-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Compare the results of the different time/spaitla bin combinations</a></span></li><li><span><a href=\"#Analyize-the-models-with-XAI-methods\" data-toc-modified-id=\"Analyize-the-models-with-XAI-methods-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Analyize the models with XAI methods</a></span></li></ul></li><li><span><a href=\"#Neural-Networks\" data-toc-modified-id=\"Neural-Networks-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Grid Search</a></span></li><li><span><a href=\"#Train-the-models-with-different-time/spatial-bins-and-different-test/train-spilt-techniques\" data-toc-modified-id=\"Train-the-models-with-different-time/spatial-bins-and-different-test/train-spilt-techniques-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Train the models with different time/spatial bins and different test/train spilt techniques</a></span></li><li><span><a href=\"#Compare-the-results-of--the-different-time/spaitla-bin-combinations\" data-toc-modified-id=\"Compare-the-results-of--the-different-time/spaitla-bin-combinations-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Compare the results of  the different time/spaitla bin combinations</a></span></li><li><span><a href=\"#Compare-the-different-test/train-split-techniques\" data-toc-modified-id=\"Compare-the-different-test/train-split-techniques-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Compare the different test/train split techniques</a></span></li><li><span><a href=\"#Aanalyize-the-models-with-XAI-methods\" data-toc-modified-id=\"Aanalyize-the-models-with-XAI-methods-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Aanalyize the models with XAI methods</a></span></li><li><span><a href=\"#Apply-different-NN-architectures\" data-toc-modified-id=\"Apply-different-NN-architectures-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Apply different NN architectures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recurrent-NN-(RNN)\" data-toc-modified-id=\"Recurrent-NN-(RNN)-6.6.1\"><span class=\"toc-item-num\">6.6.1&nbsp;&nbsp;</span>Recurrent NN (RNN)</a></span></li><li><span><a href=\"#Long-Short-Term-Memory-Networks-(LSTM)\" data-toc-modified-id=\"Long-Short-Term-Memory-Networks-(LSTM)-6.6.2\"><span class=\"toc-item-num\">6.6.2&nbsp;&nbsp;</span>Long Short-Term Memory Networks (LSTM)</a></span></li><li><span><a href=\"#Kernalized-NN\" data-toc-modified-id=\"Kernalized-NN-6.6.3\"><span class=\"toc-item-num\">6.6.3&nbsp;&nbsp;</span>Kernalized NN</a></span></li><li><span><a href=\"#Comparison-of-the-results\" data-toc-modified-id=\"Comparison-of-the-results-6.6.4\"><span class=\"toc-item-num\">6.6.4&nbsp;&nbsp;</span>Comparison of the results</a></span></li></ul></li></ul></li><li><span><a href=\"#Compare-SVM-and-NN-models-in-terms-of-predictive-performance-and-computation-time\" data-toc-modified-id=\"Compare-SVM-and-NN-models-in-terms-of-predictive-performance-and-computation-time-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Compare SVM and NN models in terms of predictive performance and computation time</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "#import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods = [1, 2, 6, 24] # time bins we want to predict the demand for\n",
    "resolution = ['h3_res_4', 'h3_res_6', 'h3_res_8'] # spatial resolution we want to predict the demand for\n",
    "\n",
    "prediction_data={}\n",
    "for periods in time_periods:\n",
    "    res_data={}\n",
    "    for res in resolution:\n",
    "        res_data[res]=pd.read_csv(f'../data/{periods}hours_{res}.csv', index_col=\"trip_start_timestamp\")\n",
    "    prediction_data[periods]=res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3_res_4</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>number_of_trips</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>lagged_1h</th>\n",
       "      <th>lagged_1day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>8426645ffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2321</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>8427593ffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>8426645ffffffff</td>\n",
       "      <td>-18.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             h3_res_4  temperature  precipitation  \\\n",
       "trip_start_timestamp                                                \n",
       "2018-01-01 00:00:00   8426645ffffffff   -20.555556            0.0   \n",
       "2018-01-01 00:00:00   842664dffffffff   -20.555556            0.0   \n",
       "2018-01-01 00:00:00   8427593ffffffff   -20.555556            0.0   \n",
       "2018-01-01 01:00:00   8426645ffffffff   -18.333333            0.0   \n",
       "\n",
       "                      number_of_trips  weekday  month  hour  hour_sin  \\\n",
       "trip_start_timestamp                                                    \n",
       "2018-01-01 00:00:00                 2      1.0    1.0   0.0  0.000000   \n",
       "2018-01-01 00:00:00              2321      1.0    1.0   0.0  0.000000   \n",
       "2018-01-01 00:00:00                35      1.0    1.0   0.0  0.000000   \n",
       "2018-01-01 01:00:00                 2      1.0    1.0   1.0  0.269797   \n",
       "\n",
       "                      hour_cos   weekday_sin  weekday_cos  lagged_1h  \\\n",
       "trip_start_timestamp                                                   \n",
       "2018-01-01 00:00:00   1.000000 -2.449294e-16          1.0        NaN   \n",
       "2018-01-01 00:00:00   1.000000 -2.449294e-16          1.0        NaN   \n",
       "2018-01-01 00:00:00   1.000000 -2.449294e-16          1.0        NaN   \n",
       "2018-01-01 01:00:00   0.962917 -2.449294e-16          1.0        2.0   \n",
       "\n",
       "                      lagged_1day  \n",
       "trip_start_timestamp               \n",
       "2018-01-01 00:00:00           NaN  \n",
       "2018-01-01 00:00:00           NaN  \n",
       "2018-01-01 00:00:00           NaN  \n",
       "2018-01-01 01:00:00           NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prediction_data.get(1).get('h3_res_4')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additionial preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "encoded_data = encoder.fit_transform(df[['h3_res_4']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['h3_res_4']))\n",
    "\n",
    "df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "df = pd.concat([df, encoded_df], axis=1).drop('h3_res_4', axis=1)\n",
    "\n",
    "df.set_index(\"trip_start_timestamp\", inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>number_of_trips</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>lagged_1h</th>\n",
       "      <th>lagged_1day</th>\n",
       "      <th>h3_res_4_8426641ffffffff</th>\n",
       "      <th>h3_res_4_8426645ffffffff</th>\n",
       "      <th>h3_res_4_842664dffffffff</th>\n",
       "      <th>h3_res_4_8427593ffffffff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2321</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>-18.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>-19.374470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2321.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      temperature  precipitation  number_of_trips  weekday  \\\n",
       "trip_start_timestamp                                                         \n",
       "2018-01-01 00:00:00    -20.555556            0.0                2      1.0   \n",
       "2018-01-01 00:00:00    -20.555556            0.0             2321      1.0   \n",
       "2018-01-01 00:00:00    -20.555556            0.0               35      1.0   \n",
       "2018-01-01 01:00:00    -18.333333            0.0                2      1.0   \n",
       "2018-01-01 01:00:00    -19.374470            0.0             4192      1.0   \n",
       "\n",
       "                      month  hour  hour_sin  hour_cos   weekday_sin  \\\n",
       "trip_start_timestamp                                                  \n",
       "2018-01-01 00:00:00     1.0   0.0  0.000000  1.000000 -2.449294e-16   \n",
       "2018-01-01 00:00:00     1.0   0.0  0.000000  1.000000 -2.449294e-16   \n",
       "2018-01-01 00:00:00     1.0   0.0  0.000000  1.000000 -2.449294e-16   \n",
       "2018-01-01 01:00:00     1.0   1.0  0.269797  0.962917 -2.449294e-16   \n",
       "2018-01-01 01:00:00     1.0   1.0  0.269797  0.962917 -2.449294e-16   \n",
       "\n",
       "                      weekday_cos  lagged_1h  lagged_1day  \\\n",
       "trip_start_timestamp                                        \n",
       "2018-01-01 00:00:00           1.0        NaN          NaN   \n",
       "2018-01-01 00:00:00           1.0        NaN          NaN   \n",
       "2018-01-01 00:00:00           1.0        NaN          NaN   \n",
       "2018-01-01 01:00:00           1.0        2.0          NaN   \n",
       "2018-01-01 01:00:00           1.0     2321.0          NaN   \n",
       "\n",
       "                      h3_res_4_8426641ffffffff  h3_res_4_8426645ffffffff  \\\n",
       "trip_start_timestamp                                                       \n",
       "2018-01-01 00:00:00                        0.0                       1.0   \n",
       "2018-01-01 00:00:00                        0.0                       0.0   \n",
       "2018-01-01 00:00:00                        0.0                       0.0   \n",
       "2018-01-01 01:00:00                        0.0                       1.0   \n",
       "2018-01-01 01:00:00                        0.0                       0.0   \n",
       "\n",
       "                      h3_res_4_842664dffffffff  h3_res_4_8427593ffffffff  \n",
       "trip_start_timestamp                                                      \n",
       "2018-01-01 00:00:00                        0.0                       0.0  \n",
       "2018-01-01 00:00:00                        1.0                       0.0  \n",
       "2018-01-01 00:00:00                        0.0                       1.0  \n",
       "2018-01-01 01:00:00                        0.0                       0.0  \n",
       "2018-01-01 01:00:00                        1.0                       0.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lagged_1day'].fillna(df['number_of_trips'].mean(), inplace= True)\n",
    "df['lagged_1h'].fillna(df['number_of_trips'].mean(), inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in 'Column1': 0\n",
      "Null values in 'Column1': 0\n"
     ]
    }
   ],
   "source": [
    "null_count_column1 = df['lagged_1h'].isnull().sum()\n",
    "print(f\"Null values in 'Column1': {null_count_column1}\")\n",
    "null_count_column1 = df['lagged_1day'].isnull().sum()\n",
    "print(f\"Null values in 'Column1': {null_count_column1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['temperature', 'precipitation', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
    "       'lagged_1h', 'lagged_1day', 'h3_res_4_8426641ffffffff',\n",
    "       'h3_res_4_8426645ffffffff', 'h3_res_4_842664dffffffff',\n",
    "       'h3_res_4_8427593ffffffff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X,Y,train_index, val_index):\n",
    "    '''\n",
    "    Method that prepares the data for training (split the data/)\n",
    "    param X: feature data to be prepared\n",
    "    param Y: target data to be prepared\n",
    "    param train_index: index that defines the split for trainig data\n",
    "    param val_index: index that defines the split for target data\n",
    "    returns X_train, Y_train, X_val, Y_val: prepared training & validation data\n",
    "    '''\n",
    "    Scaler=StandardScaler()\n",
    "    \n",
    "    X_train = Scaler.fit_transform(X.iloc[train_index])\n",
    "    Y_train = Scaler.fit_transform(Y[train_index].values.reshape(-1,1))\n",
    "    \n",
    "    X_val = Scaler.fit_transform(X.iloc[val_index])\n",
    "    Y_val = Scaler.fit_transform(Y[val_index].values.reshape(-1,1))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "\n",
    "def train_nn(X_train, Y_train, X_val, Y_val, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a neural network with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   NN to be trained\n",
    "    param params:  Parameters to compile and fit the NN\n",
    "    returns:       Nothing     \n",
    "    '''\n",
    "    model.compile(\n",
    "        optimizer=params.get(\"optimizer\"),\n",
    "        loss=params.get(\"loss\"),\n",
    "        metrics=params.get(\"metrics\"),\n",
    "    )\n",
    "    model.fit(\n",
    "        x=X_train,\n",
    "        y=Y_train,\n",
    "        batch_size=params.get(\"batch_size\"),\n",
    "        epochs=params.get(\"epochs\"),\n",
    "        #verbose=params.get(\"verbose\"),\n",
    "    )\n",
    "    Y_pred=model.predict(X_val)\n",
    "    r2= r2_score(Y_val, Y_pred)\n",
    "    MSE= mean_absolute_error(Y_val, Y_pred)\n",
    "    print(f\"R-squared {r2}\")\n",
    "    print(f\"Mean Squared Error {MSE}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_svm(X_train, Y_train, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a SVM with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   SVM to be trained\n",
    "    param params:  Parameters to train the SVM\n",
    "    returns:       Nothing    \n",
    "    '''\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_date_list(delta):\n",
    "    '''\n",
    "    This method creates a list of dates (2018-2019) \n",
    "    depending on the delta\n",
    "    params delta: determines break between dates\n",
    "    returns:      list of dates \n",
    "    '''\n",
    "    # create a startdate\n",
    "    start = pd.to_datetime(\"2018-01-01\", format=\"%Y-%m-%d\")\n",
    "    # create the enddate\n",
    "    end = pd.to_datetime(\"2018-12-31\", format=\"%Y-%m-%d\")\n",
    "    # create timedelta to increase days\n",
    "    next_day = timedelta(days=delta)\n",
    "    list_dates=[]\n",
    "    \n",
    "    while start <= end:\n",
    "        \n",
    "        # add date to list\n",
    "        list_dates.append(start)\n",
    "        # increase date by one day\n",
    "        start = start + next_day\n",
    "    \n",
    "    return list_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Train split Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_k_fold_validation(k, X, Y, model, train_model, params):\n",
    "    '''\n",
    "    Method that trains and validate the model using k-fold validation\n",
    "    However, this method can disrupt the temporal patterns and data leakage occurs \n",
    "    due to the lagged time features\n",
    "    param k:      Number of folds (iterations)\n",
    "    param x:      Feature data\n",
    "    param y:      Target data\n",
    "    param model:  Model to be trained ()\n",
    "    returns:      Nothing\n",
    "    '''\n",
    "    # initialize the folds\n",
    "    k_fold = KFold(n_splits= k, random_state=47, shuffle=True)\n",
    "    # iteratre through all folds\n",
    "    for train_index, val_index in k_fold.split(X,Y):\n",
    "        # prepare data and get splits\n",
    "        X_train, Y_train, X_val, Y_val = prepare_data(X,Y, train_index, val_index)\n",
    "        # train & validate the model\n",
    "        train_model(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=model, params=params)\n",
    "\n",
    "        \n",
    "def batch_split_cross_validation(k, time_bin, X, Y, model, train_model, params):\n",
    "    '''\n",
    "    This method split the entire dataset into batches. Direct Data Leakage is avoided by disrupting\n",
    "    the chain of the lagged time feature between test and training set. Batches are cut on week and day level \n",
    "    depending on the granularity of the time bins.\n",
    "    param X:              feature data \n",
    "    param Y:              target data\n",
    "    param k_folds:        number of folds for k-fold validation\n",
    "    param time_bin:       granularity of the time bins\n",
    "    returns index_dict:   contains indicies to split for cross validation\n",
    "    '''\n",
    "    # if 24 split batches by weeks\n",
    "    if time_bin == 24:\n",
    "        # create date list\n",
    "        date_list = create_date_list(7)    \n",
    "    # else split by days\n",
    "    else:\n",
    "        # create date list\n",
    "        date_list = create_date_list(1)\n",
    "    # number of batches that are included in the test set\n",
    "    size_test_split = len(date_list) // k\n",
    "    # iterate through each fold\n",
    "    for fold in range(0, k):\n",
    "        # make a copy of X and Y\n",
    "        X_copy = X.copy()\n",
    "        Y_copy = Y.copy()\n",
    "        # pick random choices for the test data set\n",
    "        if fold==k-1:\n",
    "            arr = np.random.choice(range(1, len(date_list)), len(date_list)-1, replace=False)\n",
    "        else: \n",
    "            arr = np.random.choice(range(1, len(date_list)), size_test_split, replace=False)\n",
    "        # Create two empty DataFrames (test sets)\n",
    "        X_test = pd.DataFrame()\n",
    "        Y_test = pd.DataFrame()\n",
    "        # Fill the test sets with data\n",
    "        #for element in arr:\n",
    "            \n",
    "        arr = sorted(arr, reverse=True)\n",
    "        # Delete the elements from the date_list that have been already used for the test set\n",
    "        for element in arr:\n",
    "            del date_list[element]\n",
    "        print(date_list)\n",
    "    return None\n",
    "\n",
    "\n",
    "def sliding_window_cross_validation(X,Y,n_windows):\n",
    "    '''\n",
    "    This method uses sliding window technique to the split the data. The user specifies the size of the window and\n",
    "    the model is trained sequentially with each window until the last window. Future data is used to the evaluate the\n",
    "    model. This way ensures no data leakage into the test set\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "\n",
    "def sorted_train_test_split(X, Y, model, train_model, params):\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2018-01-01 00:00:00'), Timestamp('2018-01-08 00:00:00'), Timestamp('2018-01-22 00:00:00'), Timestamp('2018-01-29 00:00:00'), Timestamp('2018-02-05 00:00:00'), Timestamp('2018-02-12 00:00:00'), Timestamp('2018-02-19 00:00:00'), Timestamp('2018-03-05 00:00:00'), Timestamp('2018-03-12 00:00:00'), Timestamp('2018-03-19 00:00:00'), Timestamp('2018-03-26 00:00:00'), Timestamp('2018-04-02 00:00:00'), Timestamp('2018-04-09 00:00:00'), Timestamp('2018-04-16 00:00:00'), Timestamp('2018-04-23 00:00:00'), Timestamp('2018-04-30 00:00:00'), Timestamp('2018-05-07 00:00:00'), Timestamp('2018-05-14 00:00:00'), Timestamp('2018-05-21 00:00:00'), Timestamp('2018-05-28 00:00:00'), Timestamp('2018-06-11 00:00:00'), Timestamp('2018-06-25 00:00:00'), Timestamp('2018-07-02 00:00:00'), Timestamp('2018-07-09 00:00:00'), Timestamp('2018-07-16 00:00:00'), Timestamp('2018-07-30 00:00:00'), Timestamp('2018-08-13 00:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-08-27 00:00:00'), Timestamp('2018-09-03 00:00:00'), Timestamp('2018-09-10 00:00:00'), Timestamp('2018-09-24 00:00:00'), Timestamp('2018-10-01 00:00:00'), Timestamp('2018-10-08 00:00:00'), Timestamp('2018-10-15 00:00:00'), Timestamp('2018-10-22 00:00:00'), Timestamp('2018-10-29 00:00:00'), Timestamp('2018-11-05 00:00:00'), Timestamp('2018-11-19 00:00:00'), Timestamp('2018-12-17 00:00:00')]\n",
      "[Timestamp('2018-01-01 00:00:00'), Timestamp('2018-01-08 00:00:00'), Timestamp('2018-01-22 00:00:00'), Timestamp('2018-02-05 00:00:00'), Timestamp('2018-02-12 00:00:00'), Timestamp('2018-02-19 00:00:00'), Timestamp('2018-03-05 00:00:00'), Timestamp('2018-03-19 00:00:00'), Timestamp('2018-03-26 00:00:00'), Timestamp('2018-04-16 00:00:00'), Timestamp('2018-04-30 00:00:00'), Timestamp('2018-05-21 00:00:00'), Timestamp('2018-06-11 00:00:00'), Timestamp('2018-06-25 00:00:00'), Timestamp('2018-07-02 00:00:00'), Timestamp('2018-07-09 00:00:00'), Timestamp('2018-07-30 00:00:00'), Timestamp('2018-08-13 00:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-09-10 00:00:00'), Timestamp('2018-09-24 00:00:00'), Timestamp('2018-10-01 00:00:00'), Timestamp('2018-10-08 00:00:00'), Timestamp('2018-10-22 00:00:00'), Timestamp('2018-11-05 00:00:00'), Timestamp('2018-11-19 00:00:00'), Timestamp('2018-12-17 00:00:00')]\n",
      "[Timestamp('2018-01-01 00:00:00'), Timestamp('2018-01-22 00:00:00'), Timestamp('2018-02-19 00:00:00'), Timestamp('2018-03-05 00:00:00'), Timestamp('2018-03-26 00:00:00'), Timestamp('2018-04-16 00:00:00'), Timestamp('2018-04-30 00:00:00'), Timestamp('2018-06-11 00:00:00'), Timestamp('2018-07-02 00:00:00'), Timestamp('2018-09-24 00:00:00'), Timestamp('2018-10-01 00:00:00'), Timestamp('2018-10-22 00:00:00'), Timestamp('2018-11-05 00:00:00'), Timestamp('2018-12-17 00:00:00')]\n",
      "[Timestamp('2018-01-01 00:00:00')]\n"
     ]
    }
   ],
   "source": [
    "batch_split_cross_validation(4, 24, df[features], df['number_of_trips'], None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>lagged_1h</th>\n",
       "      <th>lagged_1day</th>\n",
       "      <th>h3_res_4_8426641ffffffff</th>\n",
       "      <th>h3_res_4_8426645ffffffff</th>\n",
       "      <th>h3_res_4_842664dffffffff</th>\n",
       "      <th>h3_res_4_8427593ffffffff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>-18.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>-19.374470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2321.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07 15:00:00</th>\n",
       "      <td>-2.592593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.816970</td>\n",
       "      <td>-0.576680</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07 15:00:00</th>\n",
       "      <td>-2.629199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.816970</td>\n",
       "      <td>-0.576680</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>1537.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07 15:00:00</th>\n",
       "      <td>-2.630471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.816970</td>\n",
       "      <td>-0.576680</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07 16:00:00</th>\n",
       "      <td>-2.025372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.942261</td>\n",
       "      <td>-0.334880</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07 16:00:00</th>\n",
       "      <td>-2.026667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.942261</td>\n",
       "      <td>-0.334880</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      temperature  precipitation  hour_sin  hour_cos  \\\n",
       "trip_start_timestamp                                                   \n",
       "2018-01-01 00:00:00    -20.555556            0.0  0.000000  1.000000   \n",
       "2018-01-01 00:00:00    -20.555556            0.0  0.000000  1.000000   \n",
       "2018-01-01 00:00:00    -20.555556            0.0  0.000000  1.000000   \n",
       "2018-01-01 01:00:00    -18.333333            0.0  0.269797  0.962917   \n",
       "2018-01-01 01:00:00    -19.374470            0.0  0.269797  0.962917   \n",
       "...                           ...            ...       ...       ...   \n",
       "2018-01-07 15:00:00     -2.592593            0.0 -0.816970 -0.576680   \n",
       "2018-01-07 15:00:00     -2.629199            0.0 -0.816970 -0.576680   \n",
       "2018-01-07 15:00:00     -2.630471            0.0 -0.816970 -0.576680   \n",
       "2018-01-07 16:00:00     -2.025372            0.0 -0.942261 -0.334880   \n",
       "2018-01-07 16:00:00     -2.026667            0.0 -0.942261 -0.334880   \n",
       "\n",
       "                       weekday_sin  weekday_cos  lagged_1h  lagged_1day  \\\n",
       "trip_start_timestamp                                                      \n",
       "2018-01-01 00:00:00  -2.449294e-16          1.0        NaN          NaN   \n",
       "2018-01-01 00:00:00  -2.449294e-16          1.0        NaN          NaN   \n",
       "2018-01-01 00:00:00  -2.449294e-16          1.0        NaN          NaN   \n",
       "2018-01-01 01:00:00  -2.449294e-16          1.0        2.0          NaN   \n",
       "2018-01-01 01:00:00  -2.449294e-16          1.0     2321.0          NaN   \n",
       "...                            ...          ...        ...          ...   \n",
       "2018-01-07 15:00:00  -2.449294e-16          1.0        2.0          4.0   \n",
       "2018-01-07 15:00:00  -2.449294e-16          1.0     1048.0       1537.0   \n",
       "2018-01-07 15:00:00  -2.449294e-16          1.0      119.0        119.0   \n",
       "2018-01-07 16:00:00  -2.449294e-16          1.0     1032.0       1581.0   \n",
       "2018-01-07 16:00:00  -2.449294e-16          1.0      132.0        115.0   \n",
       "\n",
       "                      h3_res_4_8426641ffffffff  h3_res_4_8426645ffffffff  \\\n",
       "trip_start_timestamp                                                       \n",
       "2018-01-01 00:00:00                        0.0                       1.0   \n",
       "2018-01-01 00:00:00                        0.0                       0.0   \n",
       "2018-01-01 00:00:00                        0.0                       0.0   \n",
       "2018-01-01 01:00:00                        0.0                       1.0   \n",
       "2018-01-01 01:00:00                        0.0                       0.0   \n",
       "...                                        ...                       ...   \n",
       "2018-01-07 15:00:00                        0.0                       1.0   \n",
       "2018-01-07 15:00:00                        0.0                       0.0   \n",
       "2018-01-07 15:00:00                        0.0                       0.0   \n",
       "2018-01-07 16:00:00                        0.0                       0.0   \n",
       "2018-01-07 16:00:00                        0.0                       0.0   \n",
       "\n",
       "                      h3_res_4_842664dffffffff  h3_res_4_8427593ffffffff  \n",
       "trip_start_timestamp                                                      \n",
       "2018-01-01 00:00:00                        0.0                       0.0  \n",
       "2018-01-01 00:00:00                        1.0                       0.0  \n",
       "2018-01-01 00:00:00                        0.0                       1.0  \n",
       "2018-01-01 01:00:00                        0.0                       0.0  \n",
       "2018-01-01 01:00:00                        1.0                       0.0  \n",
       "...                                        ...                       ...  \n",
       "2018-01-07 15:00:00                        0.0                       0.0  \n",
       "2018-01-07 15:00:00                        1.0                       0.0  \n",
       "2018-01-07 15:00:00                        0.0                       1.0  \n",
       "2018-01-07 16:00:00                        1.0                       0.0  \n",
       "2018-01-07 16:00:00                        0.0                       1.0  \n",
       "\n",
       "[456 rows x 12 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ltf_batch_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1572\\710699311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mltf_batch_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number_of_trips'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ltf_batch_split' is not defined"
     ]
    }
   ],
   "source": [
    "ltf_batch_split(df[features],df['number_of_trips'],4, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models with different time/spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare the results of the different time/spaitla bin combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyize the models with XAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models with different time/spatial bins and different test/train spilt techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of  the different time/spaitla bin combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the different test/train split techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aanalyize the models with XAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply different NN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent NN (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Networks (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernalized NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare SVM and NN models in terms of predictive performance and computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(12, input_shape=(12,), activation='relu'))\n",
    "nn_model.add(Dense(8, activation='relu'))\n",
    "nn_model.add(Dense(4, activation='relu'))\n",
    "nn_model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"optimizer\":\"RMSPROP\",\n",
    "    \"loss\":\"mse\",\n",
    "    \"loss_weights\":None,\n",
    "    \"metrics\":['mae'],\n",
    "    \"weighted_metrics\":None,\n",
    "    \"run_eagerly\":False,\n",
    "    #\"steps_per_execution\":1,\n",
    "    \"jit_compile\":\"auto\",\n",
    "    \"auto_scale_loss\":True,\n",
    "    \"x\":None,\n",
    "    \"y\":None,\n",
    "    \"batch_size\":50,\n",
    "    \"epochs\":100,\n",
    "    \"verbose\":0,\n",
    "    \"callbacks\":None,\n",
    "    \"validation_split\":0.0,\n",
    "    \"validation_data\":None,\n",
    "    \"shuffle\":True,\n",
    "    \"class_weight\":None,\n",
    "    \"sample_weight\":None,\n",
    "    \"initial_epoch\":0,\n",
    "    \"steps_per_epoch\":None,\n",
    "    \"validation_steps\":None,\n",
    "    #\"validation_batch_size\":None,\n",
    "    \"validation_freq\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_fold_validation(4, df[features], df['number_of_trips'], nn_model, train_nn, params=params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
