{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#List-available-GPUs\" data-toc-modified-id=\"List-available-GPUs-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>List available GPUs</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Transformations\" data-toc-modified-id=\"Data-Transformations-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Data Transformations</a></span></li><li><span><a href=\"#Test/Train-split-Techniques\" data-toc-modified-id=\"Test/Train-split-Techniques-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Test/Train split Techniques</a></span></li></ul></li><li><span><a href=\"#Additional-Pre-Processing\" data-toc-modified-id=\"Additional-Pre-Processing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Additional Pre-Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deploy-One-Hot-Encoding\" data-toc-modified-id=\"Deploy-One-Hot-Encoding-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Deploy One-Hot-Encoding</a></span></li><li><span><a href=\"#Create-Feature_Sets\" data-toc-modified-id=\"Create-Feature_Sets-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Create Feature_Sets</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Support Vector Machines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Perform-Grid-search\" data-toc-modified-id=\"Perform-Grid-search-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Perform Grid search</a></span></li><li><span><a href=\"#Train-the-models-with-different-time/spatial\" data-toc-modified-id=\"Train-the-models-with-different-time/spatial-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Train the models with different time/spatial</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-k-fold-Validation\" data-toc-modified-id=\"Simple-k-fold-Validation-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Simple k-fold Validation</a></span></li><li><span><a href=\"#Batch-Split-k-fold-Cross-Validation\" data-toc-modified-id=\"Batch-Split-k-fold-Cross-Validation-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Batch Split k-fold Cross Validation</a></span></li><li><span><a href=\"#Sliding-Window-cross-Validation\" data-toc-modified-id=\"Sliding-Window-cross-Validation-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Sliding Window cross Validation</a></span></li><li><span><a href=\"#Expanding-Window-Split\" data-toc-modified-id=\"Expanding-Window-Split-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Expanding Window Split</a></span></li><li><span><a href=\"#Start-End-Split\" data-toc-modified-id=\"Start-End-Split-6.2.5\"><span class=\"toc-item-num\">6.2.5&nbsp;&nbsp;</span>Start End Split</a></span></li></ul></li><li><span><a href=\"#Compare-the-results-of-the-different-time/spaitla-bin-combinations\" data-toc-modified-id=\"Compare-the-results-of-the-different-time/spaitla-bin-combinations-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Compare the results of the different time/spaitla bin combinations</a></span></li><li><span><a href=\"#Analyize-the-models-with-XAI-methods\" data-toc-modified-id=\"Analyize-the-models-with-XAI-methods-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Analyize the models with XAI methods</a></span></li></ul></li><li><span><a href=\"#Neural-Networks\" data-toc-modified-id=\"Neural-Networks-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Grid Search</a></span></li><li><span><a href=\"#Train-the-models-with-different-time/spatial-bins-and-different-test/train-spilt-techniques\" data-toc-modified-id=\"Train-the-models-with-different-time/spatial-bins-and-different-test/train-spilt-techniques-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Train the models with different time/spatial bins and different test/train spilt techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-k-fold-Validation\" data-toc-modified-id=\"Simple-k-fold-Validation-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Simple k-fold Validation</a></span></li><li><span><a href=\"#Batch-Split-k-fold-Cross-Validation\" data-toc-modified-id=\"Batch-Split-k-fold-Cross-Validation-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Batch Split k-fold Cross Validation</a></span></li><li><span><a href=\"#Sliding-Window-cross-Validation\" data-toc-modified-id=\"Sliding-Window-cross-Validation-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>Sliding Window cross Validation</a></span></li><li><span><a href=\"#Expanding-Window-Split\" data-toc-modified-id=\"Expanding-Window-Split-7.2.4\"><span class=\"toc-item-num\">7.2.4&nbsp;&nbsp;</span>Expanding Window Split</a></span></li><li><span><a href=\"#Start-End-Split\" data-toc-modified-id=\"Start-End-Split-7.2.5\"><span class=\"toc-item-num\">7.2.5&nbsp;&nbsp;</span>Start End Split</a></span></li></ul></li><li><span><a href=\"#Compare-the-results-of--the-different-time/spaitla-bin-combinations\" data-toc-modified-id=\"Compare-the-results-of--the-different-time/spaitla-bin-combinations-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Compare the results of  the different time/spaitla bin combinations</a></span></li><li><span><a href=\"#Compare-the-different-test/train-split-techniques\" data-toc-modified-id=\"Compare-the-different-test/train-split-techniques-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Compare the different test/train split techniques</a></span></li><li><span><a href=\"#Aanalyize-the-models-with-XAI-methods\" data-toc-modified-id=\"Aanalyize-the-models-with-XAI-methods-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Aanalyize the models with XAI methods</a></span></li><li><span><a href=\"#Apply-different-NN-architectures\" data-toc-modified-id=\"Apply-different-NN-architectures-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Apply different NN architectures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convolutional-NN-(CNN)\" data-toc-modified-id=\"Convolutional-NN-(CNN)-7.6.1\"><span class=\"toc-item-num\">7.6.1&nbsp;&nbsp;</span>Convolutional NN (CNN)</a></span></li><li><span><a href=\"#Recurrent-NN-(RNN)\" data-toc-modified-id=\"Recurrent-NN-(RNN)-7.6.2\"><span class=\"toc-item-num\">7.6.2&nbsp;&nbsp;</span>Recurrent NN (RNN)</a></span></li><li><span><a href=\"#Long-Short-Term-Memory-Networks-(LSTM)\" data-toc-modified-id=\"Long-Short-Term-Memory-Networks-(LSTM)-7.6.3\"><span class=\"toc-item-num\">7.6.3&nbsp;&nbsp;</span>Long Short-Term Memory Networks (LSTM)</a></span></li><li><span><a href=\"#Kernalized-NN\" data-toc-modified-id=\"Kernalized-NN-7.6.4\"><span class=\"toc-item-num\">7.6.4&nbsp;&nbsp;</span>Kernalized NN</a></span></li><li><span><a href=\"#Comparison-of-the-results\" data-toc-modified-id=\"Comparison-of-the-results-7.6.5\"><span class=\"toc-item-num\">7.6.5&nbsp;&nbsp;</span>Comparison of the results</a></span></li></ul></li></ul></li><li><span><a href=\"#Compare-SVM-and-NN-models-in-terms-of-predictive-performance-and-computation-time\" data-toc-modified-id=\"Compare-SVM-and-NN-models-in-terms-of-predictive-performance-and-computation-time-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Compare SVM and NN models in terms of predictive performance and computation time</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from dask_ml.model_selection import GridSearchCV as DaskGridSearchCV\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import time\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "\n",
    "import joblib\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#print(\"Num GPUs Available:\", len([x for x in device_lib.list_local_devices() if x.device_type == 'GPU']))\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if not gpus:\n",
    "    print(\"No GPU found.\")\n",
    "else:\n",
    "    print(f\"GPUs found: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(gpu.device_type)\n",
    "        details = tf.config.experimental.get_memory_info(gpu.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods = [1, 2, 6, 24] # time bins we want to predict the demand for\n",
    "resolution = ['h3_res_4', 'h3_res_6', 'h3_res_8'] # spatial resolution we want to predict the demand for\n",
    "\n",
    "prediction_data={}\n",
    "for period in time_periods:\n",
    "    res_data={}\n",
    "    for res in resolution:\n",
    "        res_data[res]=pd.read_csv(f'../data/{period}hours_{res}.csv', \n",
    "                                  parse_dates=['trip_start_timestamp'],\n",
    "                                  #index_col=\"trip_start_timestamp\"\n",
    "                                 )\n",
    "    prediction_data[period]=res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>h3_res_4</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>number_of_trips</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>lagged_1h</th>\n",
       "      <th>lagged_1day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>8426645ffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2321</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>8427593ffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>8426645ffffffff</td>\n",
       "      <td>-18.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>-19.374470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2321.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>2018-01-20 13:00:00</td>\n",
       "      <td>8426645ffffffff</td>\n",
       "      <td>6.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.398401</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>2018-01-20 13:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>6.674568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1828</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.398401</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1647.0</td>\n",
       "      <td>3201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>2018-01-20 13:00:00</td>\n",
       "      <td>8427593ffffffff</td>\n",
       "      <td>6.591154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.398401</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>2018-01-20 14:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>7.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1828.0</td>\n",
       "      <td>3020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>2018-01-20 14:00:00</td>\n",
       "      <td>8427593ffffffff</td>\n",
       "      <td>7.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.631088</td>\n",
       "      <td>-0.775711</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>203.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1377 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trip_start_timestamp         h3_res_4  temperature  precipitation  \\\n",
       "0     2018-01-01 00:00:00  8426645ffffffff   -20.555556            0.0   \n",
       "1     2018-01-01 00:00:00  842664dffffffff   -20.555556            0.0   \n",
       "2     2018-01-01 00:00:00  8427593ffffffff   -20.555556            0.0   \n",
       "3     2018-01-01 01:00:00  8426645ffffffff   -18.333333            0.0   \n",
       "4     2018-01-01 01:00:00  842664dffffffff   -19.374470            0.0   \n",
       "...                   ...              ...          ...            ...   \n",
       "1372  2018-01-20 13:00:00  8426645ffffffff     6.111111            0.0   \n",
       "1373  2018-01-20 13:00:00  842664dffffffff     6.674568            0.0   \n",
       "1374  2018-01-20 13:00:00  8427593ffffffff     6.591154            0.0   \n",
       "1375  2018-01-20 14:00:00  842664dffffffff     7.222222            0.0   \n",
       "1376  2018-01-20 14:00:00  8427593ffffffff     7.222222            0.0   \n",
       "\n",
       "      number_of_trips  weekday  month  hour  hour_sin  hour_cos   weekday_sin  \\\n",
       "0                   2      1.0    1.0   0.0  0.000000  1.000000 -2.449294e-16   \n",
       "1                2321      1.0    1.0   0.0  0.000000  1.000000 -2.449294e-16   \n",
       "2                  35      1.0    1.0   0.0  0.000000  1.000000 -2.449294e-16   \n",
       "3                   2      1.0    1.0   1.0  0.269797  0.962917 -2.449294e-16   \n",
       "4                4192      1.0    1.0   1.0  0.269797  0.962917 -2.449294e-16   \n",
       "...               ...      ...    ...   ...       ...       ...           ...   \n",
       "1372                4     20.0    1.0  13.0 -0.398401 -0.917211 -2.449294e-16   \n",
       "1373             1828     20.0    1.0  13.0 -0.398401 -0.917211 -2.449294e-16   \n",
       "1374              206     20.0    1.0  13.0 -0.398401 -0.917211 -2.449294e-16   \n",
       "1375              192     20.0    1.0  14.0 -0.631088 -0.775711 -2.449294e-16   \n",
       "1376               21     20.0    1.0  14.0 -0.631088 -0.775711 -2.449294e-16   \n",
       "\n",
       "      weekday_cos  lagged_1h  lagged_1day  \n",
       "0             1.0        NaN          NaN  \n",
       "1             1.0        NaN          NaN  \n",
       "2             1.0        NaN          NaN  \n",
       "3             1.0        2.0          NaN  \n",
       "4             1.0     2321.0          NaN  \n",
       "...           ...        ...          ...  \n",
       "1372          1.0        5.0          2.0  \n",
       "1373          1.0     1647.0       3201.0  \n",
       "1374          1.0      160.0        173.0  \n",
       "1375          1.0     1828.0       3020.0  \n",
       "1376          1.0      206.0        203.0  \n",
       "\n",
       "[1377 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data.get(1).get('h3_res_4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, Y_train, X_test, Y_test):\n",
    "    '''\n",
    "    Method that prepares the data for training (split the data/)\n",
    "    param X: feature data to be prepared\n",
    "    param Y: target data to be prepared\n",
    "    param train_index: index that defines the split for trainig data\n",
    "    param val_index: index that defines the split for target data\n",
    "    returns X_train, Y_train, X_val, Y_val: prepared training & validation data\n",
    "    '''\n",
    "    Scaler=StandardScaler()\n",
    "    \n",
    "    X_train = Scaler.fit_transform(X_train)\n",
    "    Y_train = Scaler.fit_transform(Y_train)\n",
    "    \n",
    "    X_test = Scaler.fit_transform(X_test)\n",
    "    Y_test = Scaler.fit_transform(Y_test)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def train_nn(X_train, Y_train, X_val, Y_val, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a neural network with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   NN to be trained\n",
    "    param params:  Parameters to compile and fit the NN\n",
    "    returns:       Nothing     \n",
    "    '''\n",
    "    model.compile(\n",
    "        optimizer=params.get(\"optimizer\"),\n",
    "        loss=params.get(\"loss\"),\n",
    "        metrics=params.get(\"metrics\"),\n",
    "    )\n",
    "    model.fit(\n",
    "        x=X_train,\n",
    "        y=Y_train,\n",
    "        batch_size=params.get(\"batch_size\"),\n",
    "        epochs=params.get(\"epochs\"),\n",
    "        #verbose=params.get(\"verbose\"),\n",
    "    )\n",
    "    Y_pred=model.predict(X_val)\n",
    "    r2 = r2_score(Y_val, Y_pred)\n",
    "    MAE = mean_absolute_error(Y_val, Y_pred)\n",
    "    print(f\"R-squared {r2}\")\n",
    "    print(f\"Mean Squared Error {MAE}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_lsvr(X_train, Y_train, X_val, Y_val, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a SVM with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   SVM to be trained\n",
    "    param params:  Parameters to train the SVM\n",
    "    returns:       Nothing    \n",
    "    '''\n",
    "    model.set_params(\n",
    "              epsilon = params.get('regressor__model__epsilon'),\n",
    "              C = params.get('regressor__model__C'),\n",
    "              max_iter = 5000\n",
    "    )\n",
    "    model.fit(X_train, \n",
    "              Y_train.reshape(len(Y_train))\n",
    "             )\n",
    "    Y_pred = model.predict(X_val)\n",
    "    r2 = r2_score(Y_val, Y_pred)\n",
    "    MAE = mean_absolute_error(Y_val, Y_pred)\n",
    "    print(f\"R-squared {r2}\")\n",
    "    print(f\"Mean Squared Error {MAE}\")\n",
    "\n",
    "\n",
    "def create_date_list(delta):\n",
    "    '''\n",
    "    This method creates a list of dates (2018-2019) \n",
    "    depending on the delta\n",
    "    params delta: determines break between dates\n",
    "    returns:      list of dates \n",
    "    '''\n",
    "    # create a startdate\n",
    "    start = pd.to_datetime(\"2018-01-01\", format=\"%Y-%m-%d\")\n",
    "    # create the enddate\n",
    "    #end = pd.to_datetime(\"2018-12-31\", format=\"%Y-%m-%d\")\n",
    "    end = pd.to_datetime(\"2018-01-20\", format=\"%Y-%m-%d\")\n",
    "    # create timedelta to increase days\n",
    "    next_date = timedelta(days=delta)\n",
    "    list_dates=[]\n",
    "    \n",
    "    while start <= end:\n",
    "        \n",
    "        # add date to list\n",
    "        list_dates.append(start)\n",
    "        # increase date by one day\n",
    "        start = start + next_date\n",
    "    list_dates.append(end)\n",
    "    \n",
    "    return list_dates\n",
    "\n",
    "\n",
    "def create_batch_split(X,Y, date_list, arr):\n",
    "    '''\n",
    "    '''\n",
    "    X_train = X.copy()\n",
    "    Y_train = Y.copy()\n",
    "    X_test = pd.DataFrame()\n",
    "    Y_test = pd.DataFrame()\n",
    "    # Sort the arr to be able to delete entries with the index\n",
    "    arr = sorted(arr, reverse=True)\n",
    "    for element in arr:\n",
    "        # extract batch\n",
    "        batch_x = X.loc[(X.index < date_list[element]) & (X.index >= date_list[element] - timedelta(days=7))]\n",
    "        batch_y = Y.loc[(Y.index < date_list[element]) & (Y.index >= date_list[element] - timedelta(days=7))]\n",
    "        # add to the test sets\n",
    "        X_test = pd.concat([batch_x, X_test])\n",
    "        Y_test = pd.concat([batch_y, Y_test])\n",
    "        # delete from the training data set\n",
    "        X_train.drop(index=batch_x.index, inplace=True)\n",
    "        Y_train.drop(index=batch_y.index, inplace=True)\n",
    "        # Delete the elements from the date_list that have been already used for the test set\n",
    "        del date_list[element]\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def get_columns(df, subset):\n",
    "    features = list(df.columns)\n",
    "    if subset == \"cos\":\n",
    "        features.remove(\"weekday\")\n",
    "        features.remove(\"month\")\n",
    "        features.remove(\"hour\")\n",
    "    return features\n",
    "\n",
    "def create_nn(input_size, act_function, decrease_func, first_layer_factor):\n",
    "    '''\n",
    "    This method returns a neural network architecture based on the given params\n",
    "    param input_size:          number of input features\n",
    "    param act_function:        activation function that should be used for the network\n",
    "    param decrease:            function decrease of neurons from layer to layer \n",
    "                               (determines also number of hidden layers)\n",
    "    param frist_layer_factor:  determines number of neurons in the first layer (multiplied by input_size)\n",
    "    returns:                   neural network\n",
    "    '''\n",
    "    nn = Sequential()\n",
    "    neurons = int(input_size * first_layer_factor)\n",
    "    nn.add(Dense(neurons, input_shape=(input_size,), activation=act_function))\n",
    "    counter = 1\n",
    "    while counter >0:\n",
    "        neurons = decrease_func(factor, neurons, counter)\n",
    "        if neurons ==1:\n",
    "            break\n",
    "        else:\n",
    "            nn.add(Dense(neurons, activation=act_function))\n",
    "            counter +=1\n",
    "    nn.add(Dense(1))\n",
    "    return nn\n",
    "\n",
    "\n",
    "def linear_dec_func(factor, neurons, counter):\n",
    "    '''\n",
    "    function to determin the number of neurons for the next layer (in this case linear)\n",
    "    param factor:  slope of the linear function\n",
    "    param neurons: number of neurons of the last layer\n",
    "    returns:       number of neruons for the next layer\n",
    "    '''\n",
    "    return (int((1-factor *counter) * neurons))\n",
    "    \n",
    "def exp_dec_func(factor, neurons, counter):\n",
    "    '''\n",
    "    function to determin the number of neurons for the next layer (in this case exponential)\n",
    "    param factor:  determins pace of decrease\n",
    "    param neurons: number of neurons of the last layer\n",
    "    returns:       number of neruons for the next layer\n",
    "    '''\n",
    "    return int(math.exp(-counter) * neurons)\n",
    "\n",
    "def get_coumns(df, timefeature):\n",
    "    columns = df.columns\n",
    "    if "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Train split Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_k_fold_validation(k, X, Y, model, train_model, params):\n",
    "    '''\n",
    "    Method that trains and validate the model using k-fold validation\n",
    "    However, this method can disrupt the temporal patterns and data leakage occurs \n",
    "    due to the lagged time features\n",
    "    param k:      Number of folds (iterations)\n",
    "    param x:      Feature data\n",
    "    param y:      Target data\n",
    "    param model:  Model to be trained ()\n",
    "    returns:      Nothing\n",
    "    '''\n",
    "    # initialize the folds\n",
    "    k_fold = KFold(n_splits= k, random_state=47, shuffle=True)\n",
    "    # iteratre through all folds\n",
    "    for train_index, val_index in k_fold.split(X,Y):\n",
    "        # normalize data\n",
    "        X_train, Y_train, X_val, Y_val = normalize_data(\n",
    "            X.iloc[train_index],\n",
    "            Y.iloc[train_index].values.reshape(-1,1), \n",
    "            X.iloc[val_index], \n",
    "            Y.iloc[val_index].values.reshape(-1,1)\n",
    "        )\n",
    "        # train & validate the model\n",
    "        train_model(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=model, params=params)\n",
    "\n",
    "        \n",
    "def batch_split_cross_validation(k, time_bin, X, Y, model, train_model, params):\n",
    "    '''\n",
    "    This method split the entire dataset into batches. Direct Data Leakage is avoided by disrupting\n",
    "    the chain of the lagged time feature between test and training set. Batches are cut on week and day level \n",
    "    depending on the granularity of the time bins.\n",
    "    param X:              feature data \n",
    "    param Y:              target data\n",
    "    param k_folds:        number of folds for k-fold validation\n",
    "    param time_bin:       granularity of the time bins\n",
    "    returns index_dict:   contains indicies to split for cross validation\n",
    "    '''\n",
    "    # if 24 split batches by weeks\n",
    "    if time_bin == 24:\n",
    "        date_list = create_date_list(7)    \n",
    "    # else split by days\n",
    "    else:\n",
    "        date_list = create_date_list(1)\n",
    "    # number of batches that are included in the test set\n",
    "    size_test_split = len(date_list) // k\n",
    "    for fold in range(0, k):\n",
    "        print(fold)\n",
    "        # pick random choices for the test data batches\n",
    "        if fold==k-1:\n",
    "            arr = np.random.choice(range(1, len(date_list)), len(date_list)-1, replace=False)\n",
    "        else: \n",
    "            arr = np.random.choice(range(1, len(date_list)), size_test_split, replace=False)   \n",
    "        # create training and test set with the batches\n",
    "        X_train, Y_train, X_test, Y_test = create_batch_split(X,Y, date_list, arr)\n",
    "        # normalize the data\n",
    "        X_train, Y_train, X_test, Y_test = normalize_data(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test\n",
    "        )\n",
    "        # train & validate the model\n",
    "        train_model(X_train=X_train, Y_train= Y_train, X_val=X_test, Y_val=Y_test,  model=model, params=params)\n",
    "\n",
    "\n",
    "def sliding_window_cross_validation(X,Y,n_windows, model, train_model, params):\n",
    "    '''\n",
    "    This method uses sliding window technique to the split the data. The user specifies the size of the window and\n",
    "    the model is trained sequentially with each window until the last window. Future data is used to the evaluate the\n",
    "    model. This way ensures no data leakage into the test set\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "\n",
    "def sorted_train_test_split(X, Y, test_size, model, train_model, params):\n",
    "    '''\n",
    "    '''\n",
    "    # sort the entries in ascending order\n",
    "    X.sort_index(inplace=True)\n",
    "    Y.sort_index(inplace=True)\n",
    "    # get split index\n",
    "    test_index = int(len(X)*(1-test_size))-1\n",
    "    # normalize data\n",
    "    X_train, Y_train, X_val, Y_val = normalize_data(\n",
    "        X.iloc[:test_index],\n",
    "        Y.iloc[:test_index].values.reshape(-1,1), \n",
    "        X.iloc[test_index:], \n",
    "        Y.iloc[test_index:].values.reshape(-1,1)\n",
    "    )\n",
    "    # train & validate the model\n",
    "    train_model(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=model, params=params)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null_count_column1 = df['lagged_1h'].isnull().sum()\n",
    "print(f\"Null values in 'Column1': {null_count_column1}\")\n",
    "null_count_column1 = df['lagged_1day'].isnull().sum()\n",
    "print(f\"Null values in 'Column1': {null_count_column1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "# encode the hexagons in dummy variables\n",
    "\n",
    "for period in time_periods:\n",
    "    for res in resolution:\n",
    "        df = prediction_data.get(period).get(res)\n",
    "\n",
    "        df['lagged_1day'].fillna(df['number_of_trips'].mean(), inplace= True)\n",
    "        if ~period==24:\n",
    "            df['lagged_1h'].fillna(df['number_of_trips'].mean(), inplace= True)\n",
    "        \n",
    "        encoded_data = encoder.fit_transform(df[[res]])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out([res]))\n",
    "\n",
    "        #df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "        df = pd.concat([df, encoded_df], axis=1).drop(res, axis=1)\n",
    "        df.set_index(\"trip_start_timestamp\", inplace=True);\n",
    "        \n",
    "        prediction_data[period][res] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature_Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_linear = LinearSVR()\n",
    "svr_kernelized = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline_lsvr = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', svr_linear)\n",
    "])\n",
    "ttr_lsvr = TransformedTargetRegressor(\n",
    "    regressor = pipeline_lsvr,\n",
    "    transformer = scaler\n",
    ")\n",
    "pipline_ksvr = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', svr_kernelized)\n",
    "])\n",
    "ttr_ksvr = TransformedTargetRegressor(\n",
    "    regressor = pipline_ksvr,\n",
    "    transformer = scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_linear = {\n",
    "    'regressor__model__C': [0.1, 1, 10, 100, 150],\n",
    "    'regressor__model__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "}\n",
    "\n",
    "param_grid_k = {\n",
    "    'regressor__model__C': [0.1, 1, 10, 100],\n",
    "    'regressor__model__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    #'regressor__model__kernel': ['rbf','sigmoid', 'poly'],\n",
    "    'regressor__model__kernel': ['rbf', 'sigmoid'],\n",
    "    'regressor__model__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'regressor__model__coef0': [0, 0.1, 0.5, 1],  # Relevant for 'poly' and 'sigmoid' kernels\n",
    "    #'regressor__model__degree': [2, 3, 4]  # Only relevant for 'poly' kernel\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_linear_svr = DaskGridSearchCV(\n",
    "    estimator=ttr_lsvr, \n",
    "    param_grid=param_grid_linear, \n",
    "    cv=5, \n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "gs_ksvr = DaskGridSearchCV(\n",
    "    estimator=ttr_ksvr, \n",
    "    param_grid=param_grid_k, \n",
    "    cv=5, \n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8196\\2135244943.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs_linear_svr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number_of_trips'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "gs_linear_svr.fit(df[features], df['number_of_trips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with Client() as client:\n",
    "\n",
    "        start_time = time.time() # time computation time for the gridsearch\n",
    "        warnings.filterwarnings('ignore', message='Liblinear failed to converge, increase the number of iterations.')\n",
    "        gs_linear_svr.fit(df[features], df['number_of_trips'])\n",
    "        end_time = time.time()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Exception {e} occured, swtich to running only one job\")\n",
    "    gs_linear_svr.n_jobs=1\n",
    "    \n",
    "    start_time = time.time() # time computation time for the gridsearch\n",
    "    warnings.filterwarnings('ignore', message='Liblinear failed to converge, increase the number of iterations.')\n",
    "    gs_linear_svr.fit(df[features], df['number_of_trips'])\n",
    "    end_time = time.time()\n",
    "\n",
    "print(f\"GridSearch took: {(end_time-start_time)/60} minuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with Client() as client:\n",
    "\n",
    "        start_time = time.time() # time computation time for the gridsearch\n",
    "        gs_ksvr.fit(df[features], df['number_of_trips'])\n",
    "        end_time = time.time()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Exception {e} occured, swtich to running only one job\")\n",
    "    gs_ksvr.n_jobs=1\n",
    "    \n",
    "    start_time = time.time() # time computation time for the gridsearch\n",
    "    gs_ksvr.fit(df[features], df['number_of_trips'])\n",
    "    end_time = time.time()\n",
    "\n",
    "print(f\"GridSearch took: {(end_time-start_time)/60} minuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVR with linear kernel\")\n",
    "print(f\"best params: {gs_linear_svr.best_params_}\")\n",
    "print(f\"best score:  {gs_linear_svr.best_score_}\")\n",
    "\n",
    "print(\"\\nKernelized SVR\")\n",
    "print(f\"best params: {gs_ksvr.best_params_}\")\n",
    "print(f\"best score:  {gs_ksvr.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gs_ksvr.best_estimator_, 'Models/gs_ksvr_model.pkl')\n",
    "joblib.dump(gs_ksvr.best_estimator_, 'Models/gs_ksvr_params.pkl')\n",
    "\n",
    "joblib.dump(gs_ksvr.best_estimator_, 'Models/gs_linear_svr_model.pkl')\n",
    "joblib.dump(gs_ksvr.best_estimator_, 'Models/gs_linear_svr_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models with different time/spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best models and params from the gridsearch\n",
    "\n",
    "# load the best models\n",
    "joblib.load('Models/gs_linear_svr_model.pkl')\n",
    "joblib.load('Models/gs_ksvr_model.pkl')\n",
    "\n",
    "# load the best params\n",
    "joblib.load('Models/gs_ksvr_params.pkl')\n",
    "joblib.load('Models/gs_linear_svr_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple k-fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"Liblinear failed to converge.*\")\n",
    "\n",
    "simple_k_fold_validation(4, df[features], df['number_of_trips'], svr_linear, train_lsvr, params=best_params_lsvr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Split k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Window Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start End Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare the results of the different time/spaitla bin combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyize the models with XAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['temperature', 'precipitation', 'number_of_trips', 'weekday', 'month',\n",
       "       'hour', 'weekday_sin', 'weekday_cos', 'lagged_1day',\n",
       "       'h3_res_8_8826641915fffff',\n",
       "       ...\n",
       "       'h3_res_8_882664d9bdfffff', 'h3_res_8_882664d9d7fffff',\n",
       "       'h3_res_8_882759340bfffff', 'h3_res_8_8827593433fffff',\n",
       "       'h3_res_8_88275934cdfffff', 'h3_res_8_88275934edfffff',\n",
       "       'h3_res_8_8827593699fffff', 'h3_res_8_88275936b1fffff',\n",
       "       'h3_res_8_88275936bbfffff', 'h3_res_8_88275936d5fffff'],\n",
       "      dtype='object', length=250)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(12, input_shape=(13,), activation='relu'))\n",
    "nn_model.add(Dense(8, activation='relu'))\n",
    "nn_model.add(Dense(4, activation='relu'))\n",
    "nn_model.add(Dense(1))\n",
    "nn_model.save_weights('initial_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"optimizer\":\"RMSPROP\",\n",
    "    \"loss\":\"mse\",\n",
    "    \"loss_weights\":None,\n",
    "    \"metrics\":['mae'],\n",
    "    \"weighted_metrics\":None,\n",
    "    \"run_eagerly\":False,\n",
    "    #\"steps_per_execution\":1,\n",
    "    \"jit_compile\":\"auto\",\n",
    "    \"auto_scale_loss\":True,\n",
    "    \"x\":None,\n",
    "    \"y\":None,\n",
    "    \"batch_size\":50,\n",
    "    \"epochs\":200,\n",
    "    \"verbose\":0,\n",
    "    \"callbacks\":None,\n",
    "    \"validation_split\":0.0,\n",
    "    \"validation_data\":None,\n",
    "    \"shuffle\":True,\n",
    "    \"class_weight\":None,\n",
    "    \"sample_weight\":None,\n",
    "    \"initial_epoch\":0,\n",
    "    \"steps_per_epoch\":None,\n",
    "    \"validation_steps\":None,\n",
    "    #\"validation_batch_size\":None,\n",
    "    \"validation_freq\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models with different time/spatial bins and different test/train spilt techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple k-fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_model.load_weights('initial_weights')\n",
    "simple_k_fold_validation(4, df[features], df['number_of_trips'], nn_model, train_nn, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Split k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.load_weights('initial_weights')\n",
    "batch_split_cross_validation(3, 24, df[features], df[['number_of_trips']], nn_model, train_nn, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Window Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start End Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['temperature', 'precipitation', 'number_of_trips', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos', 'lagged_1h', 'lagged_1day', 'h3_res_4_8426641ffffffff', 'h3_res_4_8426645ffffffff', 'h3_res_4_842664dffffffff', 'h3_res_4_8427593ffffffff']\n",
      "Epoch 1/200\n",
      "1238/1238 [==============================] - 0s 222us/sample - loss: 1.0675 - mean_absolute_error: 0.6989\n",
      "Epoch 2/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.8799 - mean_absolute_error: 0.6236\n",
      "Epoch 3/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.8138 - mean_absolute_error: 0.5514\n",
      "Epoch 4/200\n",
      "1238/1238 [==============================] - 0s 48us/sample - loss: 0.7521 - mean_absolute_error: 0.4743\n",
      "Epoch 5/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.7222 - mean_absolute_error: 0.4367\n",
      "Epoch 6/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.7004 - mean_absolute_error: 0.4153\n",
      "Epoch 7/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.6806 - mean_absolute_error: 0.3982\n",
      "Epoch 8/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.6623 - mean_absolute_error: 0.3842\n",
      "Epoch 9/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.6446 - mean_absolute_error: 0.3712\n",
      "Epoch 10/200\n",
      "1238/1238 [==============================] - 0s 45us/sample - loss: 0.6279 - mean_absolute_error: 0.3623\n",
      "Epoch 11/200\n",
      "1238/1238 [==============================] - 0s 50us/sample - loss: 0.6121 - mean_absolute_error: 0.3537\n",
      "Epoch 12/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.5967 - mean_absolute_error: 0.3459\n",
      "Epoch 13/200\n",
      "1238/1238 [==============================] - 0s 48us/sample - loss: 0.5820 - mean_absolute_error: 0.3386\n",
      "Epoch 14/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.5669 - mean_absolute_error: 0.3321\n",
      "Epoch 15/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.5521 - mean_absolute_error: 0.3247\n",
      "Epoch 16/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.5377 - mean_absolute_error: 0.3182\n",
      "Epoch 17/200\n",
      "1238/1238 [==============================] - 0s 47us/sample - loss: 0.5235 - mean_absolute_error: 0.3125\n",
      "Epoch 18/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.5098 - mean_absolute_error: 0.3060\n",
      "Epoch 19/200\n",
      "1238/1238 [==============================] - 0s 49us/sample - loss: 0.4971 - mean_absolute_error: 0.3011\n",
      "Epoch 20/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.4837 - mean_absolute_error: 0.2944\n",
      "Epoch 21/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.4709 - mean_absolute_error: 0.2895\n",
      "Epoch 22/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.4581 - mean_absolute_error: 0.2838\n",
      "Epoch 23/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.4460 - mean_absolute_error: 0.2794\n",
      "Epoch 24/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.4340 - mean_absolute_error: 0.2730\n",
      "Epoch 25/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.4220 - mean_absolute_error: 0.2679\n",
      "Epoch 26/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.4100 - mean_absolute_error: 0.2630\n",
      "Epoch 27/200\n",
      "1238/1238 [==============================] - 0s 28us/sample - loss: 0.3989 - mean_absolute_error: 0.2584\n",
      "Epoch 28/200\n",
      "1238/1238 [==============================] - 0s 42us/sample - loss: 0.3878 - mean_absolute_error: 0.2531\n",
      "Epoch 29/200\n",
      "1238/1238 [==============================] - 0s 43us/sample - loss: 0.3768 - mean_absolute_error: 0.2485\n",
      "Epoch 30/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.3664 - mean_absolute_error: 0.2440\n",
      "Epoch 31/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.3556 - mean_absolute_error: 0.2386\n",
      "Epoch 32/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.3449 - mean_absolute_error: 0.2339\n",
      "Epoch 33/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.3348 - mean_absolute_error: 0.2291\n",
      "Epoch 34/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.3249 - mean_absolute_error: 0.2244\n",
      "Epoch 35/200\n",
      "1238/1238 [==============================] - 0s 30us/sample - loss: 0.3156 - mean_absolute_error: 0.2212\n",
      "Epoch 36/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.3059 - mean_absolute_error: 0.2158\n",
      "Epoch 37/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.2965 - mean_absolute_error: 0.2119\n",
      "Epoch 38/200\n",
      "1238/1238 [==============================] - 0s 42us/sample - loss: 0.2873 - mean_absolute_error: 0.2062\n",
      "Epoch 39/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.2785 - mean_absolute_error: 0.2027\n",
      "Epoch 40/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.2698 - mean_absolute_error: 0.1985\n",
      "Epoch 41/200\n",
      "1238/1238 [==============================] - 0s 44us/sample - loss: 0.2611 - mean_absolute_error: 0.1941\n",
      "Epoch 42/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.2529 - mean_absolute_error: 0.1901\n",
      "Epoch 43/200\n",
      "1238/1238 [==============================] - 0s 44us/sample - loss: 0.2447 - mean_absolute_error: 0.1854\n",
      "Epoch 44/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.2373 - mean_absolute_error: 0.1818\n",
      "Epoch 45/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.2299 - mean_absolute_error: 0.1784\n",
      "Epoch 46/200\n",
      "1238/1238 [==============================] - 0s 46us/sample - loss: 0.2222 - mean_absolute_error: 0.1743\n",
      "Epoch 47/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.2150 - mean_absolute_error: 0.1705\n",
      "Epoch 48/200\n",
      "1238/1238 [==============================] - 0s 30us/sample - loss: 0.2080 - mean_absolute_error: 0.1666\n",
      "Epoch 49/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.2008 - mean_absolute_error: 0.1629\n",
      "Epoch 50/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.1942 - mean_absolute_error: 0.1597\n",
      "Epoch 51/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.1879 - mean_absolute_error: 0.1561\n",
      "Epoch 52/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.1816 - mean_absolute_error: 0.1530\n",
      "Epoch 53/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.1754 - mean_absolute_error: 0.1495\n",
      "Epoch 54/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.1692 - mean_absolute_error: 0.1466\n",
      "Epoch 55/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.1632 - mean_absolute_error: 0.1442\n",
      "Epoch 56/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.1575 - mean_absolute_error: 0.1396\n",
      "Epoch 57/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.1520 - mean_absolute_error: 0.1364\n",
      "Epoch 58/200\n",
      "1238/1238 [==============================] - 0s 62us/sample - loss: 0.1464 - mean_absolute_error: 0.1323\n",
      "Epoch 59/200\n",
      "1238/1238 [==============================] - 0s 31us/sample - loss: 0.1412 - mean_absolute_error: 0.1312\n",
      "Epoch 60/200\n",
      "1238/1238 [==============================] - 0s 27us/sample - loss: 0.1360 - mean_absolute_error: 0.1266\n",
      "Epoch 61/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.1313 - mean_absolute_error: 0.1250\n",
      "Epoch 62/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.1265 - mean_absolute_error: 0.1218\n",
      "Epoch 63/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.1219 - mean_absolute_error: 0.1187\n",
      "Epoch 64/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.1172 - mean_absolute_error: 0.1163\n",
      "Epoch 65/200\n",
      "1238/1238 [==============================] - 0s 31us/sample - loss: 0.1124 - mean_absolute_error: 0.1131\n",
      "Epoch 66/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.1080 - mean_absolute_error: 0.1106\n",
      "Epoch 67/200\n",
      "1238/1238 [==============================] - 0s 30us/sample - loss: 0.1038 - mean_absolute_error: 0.1075\n",
      "Epoch 68/200\n",
      "1238/1238 [==============================] - 0s 46us/sample - loss: 0.0996 - mean_absolute_error: 0.1056\n",
      "Epoch 69/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.0957 - mean_absolute_error: 0.1023\n",
      "Epoch 70/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0920 - mean_absolute_error: 0.1013\n",
      "Epoch 71/200\n",
      "1238/1238 [==============================] - 0s 46us/sample - loss: 0.0883 - mean_absolute_error: 0.0958\n",
      "Epoch 72/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.0849 - mean_absolute_error: 0.0970\n",
      "Epoch 73/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0813 - mean_absolute_error: 0.0927\n",
      "Epoch 74/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0780 - mean_absolute_error: 0.0910\n",
      "Epoch 75/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.0747 - mean_absolute_error: 0.0889\n",
      "Epoch 76/200\n",
      "1238/1238 [==============================] - 0s 42us/sample - loss: 0.0716 - mean_absolute_error: 0.0868\n",
      "Epoch 77/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0688 - mean_absolute_error: 0.0843\n",
      "Epoch 78/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.0658 - mean_absolute_error: 0.0818\n",
      "Epoch 79/200\n",
      "1238/1238 [==============================] - 0s 31us/sample - loss: 0.0631 - mean_absolute_error: 0.0810\n",
      "Epoch 80/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0604 - mean_absolute_error: 0.0786\n",
      "Epoch 81/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.0579 - mean_absolute_error: 0.0755\n",
      "Epoch 82/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0555 - mean_absolute_error: 0.0744\n",
      "Epoch 83/200\n",
      "1238/1238 [==============================] - 0s 31us/sample - loss: 0.0531 - mean_absolute_error: 0.0728\n",
      "Epoch 84/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0506 - mean_absolute_error: 0.0707\n",
      "Epoch 85/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0483 - mean_absolute_error: 0.0688\n",
      "Epoch 86/200\n",
      "1238/1238 [==============================] - 0s 42us/sample - loss: 0.0460 - mean_absolute_error: 0.0661\n",
      "Epoch 87/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0439 - mean_absolute_error: 0.0655\n",
      "Epoch 88/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0418 - mean_absolute_error: 0.0617\n",
      "Epoch 89/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0399 - mean_absolute_error: 0.0625\n",
      "Epoch 90/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.0381 - mean_absolute_error: 0.0589\n",
      "Epoch 91/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0365 - mean_absolute_error: 0.0584\n",
      "Epoch 92/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0349 - mean_absolute_error: 0.0581\n",
      "Epoch 93/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0333 - mean_absolute_error: 0.0553\n",
      "Epoch 94/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0318 - mean_absolute_error: 0.0552\n",
      "Epoch 95/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0303 - mean_absolute_error: 0.0530\n",
      "Epoch 96/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0287 - mean_absolute_error: 0.0503\n",
      "Epoch 97/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0274 - mean_absolute_error: 0.0505\n",
      "Epoch 98/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.0260 - mean_absolute_error: 0.0490\n",
      "Epoch 99/200\n",
      "1238/1238 [==============================] - 0s 31us/sample - loss: 0.0249 - mean_absolute_error: 0.0480\n",
      "Epoch 100/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0237 - mean_absolute_error: 0.0477\n",
      "Epoch 101/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0227 - mean_absolute_error: 0.0465\n",
      "Epoch 102/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0216 - mean_absolute_error: 0.0434\n",
      "Epoch 103/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0206 - mean_absolute_error: 0.0441\n",
      "Epoch 104/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0197 - mean_absolute_error: 0.0435\n",
      "Epoch 105/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0186 - mean_absolute_error: 0.0414\n",
      "Epoch 106/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0176 - mean_absolute_error: 0.0403\n",
      "Epoch 107/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0168 - mean_absolute_error: 0.0396\n",
      "Epoch 108/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.0159 - mean_absolute_error: 0.0377\n",
      "Epoch 109/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0151 - mean_absolute_error: 0.0366\n",
      "Epoch 110/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0145 - mean_absolute_error: 0.0380\n",
      "Epoch 111/200\n",
      "1238/1238 [==============================] - 0s 29us/sample - loss: 0.0137 - mean_absolute_error: 0.0364\n",
      "Epoch 112/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0131 - mean_absolute_error: 0.0361\n",
      "Epoch 113/200\n",
      "1238/1238 [==============================] - 0s 54us/sample - loss: 0.0124 - mean_absolute_error: 0.0333\n",
      "Epoch 114/200\n",
      "1238/1238 [==============================] - 0s 30us/sample - loss: 0.0120 - mean_absolute_error: 0.0360\n",
      "Epoch 115/200\n",
      "1238/1238 [==============================] - 0s 31us/sample - loss: 0.0113 - mean_absolute_error: 0.0328\n",
      "Epoch 116/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0109 - mean_absolute_error: 0.0337\n",
      "Epoch 117/200\n",
      "1238/1238 [==============================] - 0s 42us/sample - loss: 0.0104 - mean_absolute_error: 0.0329\n",
      "Epoch 118/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.0099 - mean_absolute_error: 0.0309\n",
      "Epoch 119/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0095 - mean_absolute_error: 0.0306\n",
      "Epoch 120/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.0091 - mean_absolute_error: 0.0317\n",
      "Epoch 121/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.0087 - mean_absolute_error: 0.0294\n",
      "Epoch 122/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0083 - mean_absolute_error: 0.0280\n",
      "Epoch 123/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0080 - mean_absolute_error: 0.0296\n",
      "Epoch 124/200\n",
      "1238/1238 [==============================] - 0s 29us/sample - loss: 0.0077 - mean_absolute_error: 0.0282\n",
      "Epoch 125/200\n",
      "1238/1238 [==============================] - ETA: 0s - loss: 2.9263e-04 - mean_absolute_error: 0.013 - 0s 34us/sample - loss: 0.0075 - mean_absolute_error: 0.0289\n",
      "Epoch 126/200\n",
      "1238/1238 [==============================] - 0s 44us/sample - loss: 0.0071 - mean_absolute_error: 0.0269\n",
      "Epoch 127/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.0069 - mean_absolute_error: 0.0282\n",
      "Epoch 128/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0067 - mean_absolute_error: 0.0270\n",
      "Epoch 129/200\n",
      "1238/1238 [==============================] - 0s 44us/sample - loss: 0.0064 - mean_absolute_error: 0.0257\n",
      "Epoch 130/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0062 - mean_absolute_error: 0.0270\n",
      "Epoch 131/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0060 - mean_absolute_error: 0.0264\n",
      "Epoch 132/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0059 - mean_absolute_error: 0.0265\n",
      "Epoch 133/200\n",
      "1238/1238 [==============================] - 0s 31us/sample - loss: 0.0057 - mean_absolute_error: 0.0263\n",
      "Epoch 134/200\n",
      "1238/1238 [==============================] - 0s 44us/sample - loss: 0.0055 - mean_absolute_error: 0.0249\n",
      "Epoch 135/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0053 - mean_absolute_error: 0.0238\n",
      "Epoch 136/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0052 - mean_absolute_error: 0.0264\n",
      "Epoch 137/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0050 - mean_absolute_error: 0.0259\n",
      "Epoch 138/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0048 - mean_absolute_error: 0.0249\n",
      "Epoch 139/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0047 - mean_absolute_error: 0.0247\n",
      "Epoch 140/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.0046 - mean_absolute_error: 0.0250\n",
      "Epoch 141/200\n",
      "1238/1238 [==============================] - 0s 46us/sample - loss: 0.0045 - mean_absolute_error: 0.0246\n",
      "Epoch 142/200\n",
      "1238/1238 [==============================] - 0s 36us/sample - loss: 0.0043 - mean_absolute_error: 0.0233\n",
      "Epoch 143/200\n",
      "1238/1238 [==============================] - 0s 44us/sample - loss: 0.0042 - mean_absolute_error: 0.0241\n",
      "Epoch 144/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0040 - mean_absolute_error: 0.0223\n",
      "Epoch 145/200\n",
      "1238/1238 [==============================] - 0s 37us/sample - loss: 0.0039 - mean_absolute_error: 0.0220\n",
      "Epoch 146/200\n",
      "1238/1238 [==============================] - 0s 30us/sample - loss: 0.0039 - mean_absolute_error: 0.0241\n",
      "Epoch 147/200\n",
      "1238/1238 [==============================] - 0s 48us/sample - loss: 0.0037 - mean_absolute_error: 0.0209\n",
      "Epoch 148/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.0036 - mean_absolute_error: 0.0217\n",
      "Epoch 149/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0036 - mean_absolute_error: 0.0229\n",
      "Epoch 150/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0034 - mean_absolute_error: 0.0194\n",
      "Epoch 151/200\n",
      "1238/1238 [==============================] - 0s 30us/sample - loss: 0.0035 - mean_absolute_error: 0.0242\n",
      "Epoch 152/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0033 - mean_absolute_error: 0.0225\n",
      "Epoch 153/200\n",
      "1238/1238 [==============================] - 0s 34us/sample - loss: 0.0032 - mean_absolute_error: 0.0221\n",
      "Epoch 154/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0032 - mean_absolute_error: 0.0225\n",
      "Epoch 155/200\n",
      "1238/1238 [==============================] - 0s 45us/sample - loss: 0.0030 - mean_absolute_error: 0.0214\n",
      "Epoch 156/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0031 - mean_absolute_error: 0.0228\n",
      "Epoch 157/200\n",
      "1238/1238 [==============================] - 0s 44us/sample - loss: 0.0029 - mean_absolute_error: 0.0204\n",
      "Epoch 158/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0030 - mean_absolute_error: 0.0230\n",
      "Epoch 159/200\n",
      "1238/1238 [==============================] - 0s 43us/sample - loss: 0.0027 - mean_absolute_error: 0.0191\n",
      "Epoch 160/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0028 - mean_absolute_error: 0.0219\n",
      "Epoch 161/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0027 - mean_absolute_error: 0.0219\n",
      "Epoch 162/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0027 - mean_absolute_error: 0.0230\n",
      "Epoch 163/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0026 - mean_absolute_error: 0.0203\n",
      "Epoch 164/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0026 - mean_absolute_error: 0.0219\n",
      "Epoch 165/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0025 - mean_absolute_error: 0.0200\n",
      "Epoch 166/200\n",
      "1238/1238 [==============================] - 0s 48us/sample - loss: 0.0025 - mean_absolute_error: 0.0219\n",
      "Epoch 167/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0024 - mean_absolute_error: 0.0204\n",
      "Epoch 168/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0024 - mean_absolute_error: 0.0215\n",
      "Epoch 169/200\n",
      "1238/1238 [==============================] - 0s 60us/sample - loss: 0.0023 - mean_absolute_error: 0.0197\n",
      "Epoch 170/200\n",
      "1238/1238 [==============================] - 0s 32us/sample - loss: 0.0023 - mean_absolute_error: 0.0216\n",
      "Epoch 171/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0023 - mean_absolute_error: 0.0216\n",
      "Epoch 172/200\n",
      "1238/1238 [==============================] - 0s 47us/sample - loss: 0.0022 - mean_absolute_error: 0.0195\n",
      "Epoch 173/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0023 - mean_absolute_error: 0.0215\n",
      "Epoch 174/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0021 - mean_absolute_error: 0.0176\n",
      "Epoch 175/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0021 - mean_absolute_error: 0.0197\n",
      "Epoch 176/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0021 - mean_absolute_error: 0.0211\n",
      "Epoch 177/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0020 - mean_absolute_error: 0.0197\n",
      "Epoch 178/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0020 - mean_absolute_error: 0.0210\n",
      "Epoch 179/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.0020 - mean_absolute_error: 0.0195\n",
      "Epoch 180/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0019 - mean_absolute_error: 0.0195\n",
      "Epoch 181/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.0019 - mean_absolute_error: 0.0200\n",
      "Epoch 182/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0019 - mean_absolute_error: 0.0187\n",
      "Epoch 183/200\n",
      "1238/1238 [==============================] - 0s 47us/sample - loss: 0.0019 - mean_absolute_error: 0.0208\n",
      "Epoch 184/200\n",
      "1238/1238 [==============================] - 0s 48us/sample - loss: 0.0019 - mean_absolute_error: 0.0215\n",
      "Epoch 185/200\n",
      "1238/1238 [==============================] - 0s 49us/sample - loss: 0.0018 - mean_absolute_error: 0.0196\n",
      "Epoch 186/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0019 - mean_absolute_error: 0.0213\n",
      "Epoch 187/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0017 - mean_absolute_error: 0.0191\n",
      "Epoch 188/200\n",
      "1238/1238 [==============================] - 0s 35us/sample - loss: 0.0017 - mean_absolute_error: 0.0191\n",
      "Epoch 189/200\n",
      "1238/1238 [==============================] - 0s 30us/sample - loss: 0.0018 - mean_absolute_error: 0.0201\n",
      "Epoch 190/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0017 - mean_absolute_error: 0.0205\n",
      "Epoch 191/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0017 - mean_absolute_error: 0.0202\n",
      "Epoch 192/200\n",
      "1238/1238 [==============================] - 0s 42us/sample - loss: 0.0017 - mean_absolute_error: 0.0212\n",
      "Epoch 193/200\n",
      "1238/1238 [==============================] - 0s 47us/sample - loss: 0.0016 - mean_absolute_error: 0.0189\n",
      "Epoch 194/200\n",
      "1238/1238 [==============================] - 0s 41us/sample - loss: 0.0016 - mean_absolute_error: 0.0206\n",
      "Epoch 195/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0015 - mean_absolute_error: 0.0167\n",
      "Epoch 196/200\n",
      "1238/1238 [==============================] - 0s 42us/sample - loss: 0.0016 - mean_absolute_error: 0.0199\n",
      "Epoch 197/200\n",
      "1238/1238 [==============================] - 0s 38us/sample - loss: 0.0016 - mean_absolute_error: 0.0206\n",
      "Epoch 198/200\n",
      "1238/1238 [==============================] - 0s 33us/sample - loss: 0.0015 - mean_absolute_error: 0.0189\n",
      "Epoch 199/200\n",
      "1238/1238 [==============================] - 0s 40us/sample - loss: 0.0015 - mean_absolute_error: 0.0195\n",
      "Epoch 200/200\n",
      "1238/1238 [==============================] - 0s 39us/sample - loss: 0.0014 - mean_absolute_error: 0.0190\n",
      "R-squared 0.9912410396618178\n",
      "Mean Squared Error 0.06910091561004356\n",
      "['temperature', 'precipitation', 'number_of_trips', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos', 'lagged_1h', 'lagged_1day', 'h3_res_6_862664197ffffff', 'h3_res_6_8626641b7ffffff', 'h3_res_6_862664527ffffff', 'h3_res_6_86266452fffffff', 'h3_res_6_862664567ffffff', 'h3_res_6_86266456fffffff', 'h3_res_6_862664577ffffff', 'h3_res_6_862664c17ffffff', 'h3_res_6_862664c1fffffff', 'h3_res_6_862664c87ffffff', 'h3_res_6_862664c8fffffff', 'h3_res_6_862664ca7ffffff', 'h3_res_6_862664cafffffff', 'h3_res_6_862664cb7ffffff', 'h3_res_6_862664cc7ffffff', 'h3_res_6_862664ccfffffff', 'h3_res_6_862664cd7ffffff', 'h3_res_6_862664cdfffffff', 'h3_res_6_862664ce7ffffff', 'h3_res_6_862664cefffffff', 'h3_res_6_862664cf7ffffff', 'h3_res_6_862664d87ffffff', 'h3_res_6_862664d8fffffff', 'h3_res_6_862664d9fffffff', 'h3_res_6_862759347ffffff', 'h3_res_6_86275934fffffff', 'h3_res_6_86275936fffffff']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_4_input to have shape (13,) but got array with shape (36,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19008\\1713616606.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'initial_weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cos\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msorted_train_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cos\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number_of_trips'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19008\\3270986540.py\u001b[0m in \u001b[0;36msorted_train_test_split\u001b[1;34m(X, Y, test_size, model, train_model, params)\u001b[0m\n\u001b[0;32m     89\u001b[0m     )\n\u001b[0;32m     90\u001b[0m     \u001b[1;31m# train & validate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19008\\2699183309.py\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(X_train, Y_train, X_val, Y_val, model, params)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch_size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m#verbose=params.get(\"verbose\"),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     )\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    383\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    386\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_4_input to have shape (13,) but got array with shape (36,)"
     ]
    }
   ],
   "source": [
    "for period in time_periods:\n",
    "    for res in resolution:\n",
    "        df = prediction_data.get(period).get(res) \n",
    "        nn_model.load_weights('initial_weights')\n",
    "        print(get_columns(df, \"cos\"))\n",
    "        sorted_train_test_split(df[get_columns(df, \"cos\")], df[['number_of_trips']], 0.1, nn_model, train_nn, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of  the different time/spaitla bin combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the different test/train split techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aanalyize the models with XAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply different NN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional NN (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent NN (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Networks (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernalized NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare SVM and NN models in terms of predictive performance and computation time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
