{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Transformations\" data-toc-modified-id=\"Data-Transformations-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data Transformations</a></span></li><li><span><a href=\"#Test/Train-split-Techniques\" data-toc-modified-id=\"Test/Train-split-Techniques-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Test/Train split Techniques</a></span></li></ul></li><li><span><a href=\"#Additional-Pre-Processing\" data-toc-modified-id=\"Additional-Pre-Processing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Additional Pre-Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deploy-One-Hot-Encoding\" data-toc-modified-id=\"Deploy-One-Hot-Encoding-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Deploy One-Hot-Encoding</a></span></li><li><span><a href=\"#Create-Feature_Sets\" data-toc-modified-id=\"Create-Feature_Sets-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Create Feature_Sets</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Support Vector Machines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Perform-Grid-search\" data-toc-modified-id=\"Perform-Grid-search-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Perform Grid search</a></span></li><li><span><a href=\"#Train-the-models-with-different-time/spatial\" data-toc-modified-id=\"Train-the-models-with-different-time/spatial-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Train the models with different time/spatial</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-k-fold-Validation\" data-toc-modified-id=\"Simple-k-fold-Validation-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Simple k-fold Validation</a></span></li><li><span><a href=\"#Batch-Split-k-fold-Cross-Validation\" data-toc-modified-id=\"Batch-Split-k-fold-Cross-Validation-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Batch Split k-fold Cross Validation</a></span></li><li><span><a href=\"#Sliding-Window-cross-Validation\" data-toc-modified-id=\"Sliding-Window-cross-Validation-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Sliding Window cross Validation</a></span></li><li><span><a href=\"#Expanding-Window-Split\" data-toc-modified-id=\"Expanding-Window-Split-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Expanding Window Split</a></span></li><li><span><a href=\"#Start-End-Split\" data-toc-modified-id=\"Start-End-Split-5.2.5\"><span class=\"toc-item-num\">5.2.5&nbsp;&nbsp;</span>Start End Split</a></span></li></ul></li><li><span><a href=\"#Compare-the-results-of-the-different-time/spaitla-bin-combinations\" data-toc-modified-id=\"Compare-the-results-of-the-different-time/spaitla-bin-combinations-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Compare the results of the different time/spaitla bin combinations</a></span></li><li><span><a href=\"#Analyize-the-models-with-XAI-methods\" data-toc-modified-id=\"Analyize-the-models-with-XAI-methods-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Analyize the models with XAI methods</a></span></li></ul></li><li><span><a href=\"#Neural-Networks\" data-toc-modified-id=\"Neural-Networks-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Grid Search</a></span></li><li><span><a href=\"#Train-the-models-with-different-time/spatial-bins-and-different-test/train-spilt-techniques\" data-toc-modified-id=\"Train-the-models-with-different-time/spatial-bins-and-different-test/train-spilt-techniques-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Train the models with different time/spatial bins and different test/train spilt techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-k-fold-Validation\" data-toc-modified-id=\"Simple-k-fold-Validation-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Simple k-fold Validation</a></span></li><li><span><a href=\"#Batch-Split-k-fold-Cross-Validation\" data-toc-modified-id=\"Batch-Split-k-fold-Cross-Validation-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Batch Split k-fold Cross Validation</a></span></li><li><span><a href=\"#Sliding-Window-cross-Validation\" data-toc-modified-id=\"Sliding-Window-cross-Validation-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Sliding Window cross Validation</a></span></li><li><span><a href=\"#Expanding-Window-Split\" data-toc-modified-id=\"Expanding-Window-Split-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Expanding Window Split</a></span></li><li><span><a href=\"#Start-End-Split\" data-toc-modified-id=\"Start-End-Split-6.2.5\"><span class=\"toc-item-num\">6.2.5&nbsp;&nbsp;</span>Start End Split</a></span></li></ul></li><li><span><a href=\"#Compare-the-results-of--the-different-time/spaitla-bin-combinations\" data-toc-modified-id=\"Compare-the-results-of--the-different-time/spaitla-bin-combinations-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Compare the results of  the different time/spaitla bin combinations</a></span></li><li><span><a href=\"#Compare-the-different-test/train-split-techniques\" data-toc-modified-id=\"Compare-the-different-test/train-split-techniques-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Compare the different test/train split techniques</a></span></li><li><span><a href=\"#Aanalyize-the-models-with-XAI-methods\" data-toc-modified-id=\"Aanalyize-the-models-with-XAI-methods-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Aanalyize the models with XAI methods</a></span></li><li><span><a href=\"#Apply-different-NN-architectures\" data-toc-modified-id=\"Apply-different-NN-architectures-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Apply different NN architectures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convolutional-NN-(CNN)\" data-toc-modified-id=\"Convolutional-NN-(CNN)-6.6.1\"><span class=\"toc-item-num\">6.6.1&nbsp;&nbsp;</span>Convolutional NN (CNN)</a></span></li><li><span><a href=\"#Recurrent-NN-(RNN)\" data-toc-modified-id=\"Recurrent-NN-(RNN)-6.6.2\"><span class=\"toc-item-num\">6.6.2&nbsp;&nbsp;</span>Recurrent NN (RNN)</a></span></li><li><span><a href=\"#Long-Short-Term-Memory-Networks-(LSTM)\" data-toc-modified-id=\"Long-Short-Term-Memory-Networks-(LSTM)-6.6.3\"><span class=\"toc-item-num\">6.6.3&nbsp;&nbsp;</span>Long Short-Term Memory Networks (LSTM)</a></span></li><li><span><a href=\"#Kernalized-NN\" data-toc-modified-id=\"Kernalized-NN-6.6.4\"><span class=\"toc-item-num\">6.6.4&nbsp;&nbsp;</span>Kernalized NN</a></span></li><li><span><a href=\"#Comparison-of-the-results\" data-toc-modified-id=\"Comparison-of-the-results-6.6.5\"><span class=\"toc-item-num\">6.6.5&nbsp;&nbsp;</span>Comparison of the results</a></span></li></ul></li></ul></li><li><span><a href=\"#Compare-SVM-and-NN-models-in-terms-of-predictive-performance-and-computation-time\" data-toc-modified-id=\"Compare-SVM-and-NN-models-in-terms-of-predictive-performance-and-computation-time-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Compare SVM and NN models in terms of predictive performance and computation time</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from dask_ml.model_selection import GridSearchCV as DaskGridSearchCV\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
<<<<<<< HEAD
    "from datetime import timedelta"
=======
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import time\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "\n",
    "import joblib"
>>>>>>> 8ed4a11604dbfb5d3ec3242fb6e6085c85ce63c6
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 2,
>>>>>>> 8ed4a11604dbfb5d3ec3242fb6e6085c85ce63c6
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods = [1, 2, 6, 24] # time bins we want to predict the demand for\n",
    "resolution = ['h3_res_4', 'h3_res_6', 'h3_res_8'] # spatial resolution we want to predict the demand for\n",
    "\n",
    "prediction_data={}\n",
    "for period in time_periods:\n",
    "    res_data={}\n",
    "    for res in resolution:\n",
<<<<<<< HEAD
    "        res_data[res]=pd.read_csv(f'../data/{period}hours_{res}.csv', index_col=\"trip_start_timestamp\")\n",
    "    prediction_data[period]=res_data"
=======
    "        res_data[res]=pd.read_csv(f'../data/{periods}hours_{res}.csv', \n",
    "                                  parse_dates=['trip_start_timestamp'],\n",
    "                                  index_col=\"trip_start_timestamp\")\n",
    "    prediction_data[periods]=res_data"
>>>>>>> 8ed4a11604dbfb5d3ec3242fb6e6085c85ce63c6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 3,
>>>>>>> 8ed4a11604dbfb5d3ec3242fb6e6085c85ce63c6
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3_res_4</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>number_of_trips</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>lagged_1h</th>\n",
       "      <th>lagged_1day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>8426645ffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2321</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>8427593ffffffff</td>\n",
       "      <td>-20.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>8426645ffffffff</td>\n",
       "      <td>-18.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             h3_res_4  temperature  precipitation  \\\n",
       "trip_start_timestamp                                                \n",
       "2018-01-01 00:00:00   8426645ffffffff   -20.555556            0.0   \n",
       "2018-01-01 00:00:00   842664dffffffff   -20.555556            0.0   \n",
       "2018-01-01 00:00:00   8427593ffffffff   -20.555556            0.0   \n",
       "2018-01-01 01:00:00   8426645ffffffff   -18.333333            0.0   \n",
       "\n",
       "                      number_of_trips  weekday  month  hour  hour_sin  \\\n",
       "trip_start_timestamp                                                    \n",
       "2018-01-01 00:00:00                 2      1.0    1.0   0.0  0.000000   \n",
       "2018-01-01 00:00:00              2321      1.0    1.0   0.0  0.000000   \n",
       "2018-01-01 00:00:00                35      1.0    1.0   0.0  0.000000   \n",
       "2018-01-01 01:00:00                 2      1.0    1.0   1.0  0.269797   \n",
       "\n",
       "                      hour_cos   weekday_sin  weekday_cos  lagged_1h  \\\n",
       "trip_start_timestamp                                                   \n",
       "2018-01-01 00:00:00   1.000000 -2.449294e-16          1.0        NaN   \n",
       "2018-01-01 00:00:00   1.000000 -2.449294e-16          1.0        NaN   \n",
       "2018-01-01 00:00:00   1.000000 -2.449294e-16          1.0        NaN   \n",
       "2018-01-01 01:00:00   0.962917 -2.449294e-16          1.0        2.0   \n",
       "\n",
       "                      lagged_1day  \n",
       "trip_start_timestamp               \n",
       "2018-01-01 00:00:00           NaN  \n",
       "2018-01-01 00:00:00           NaN  \n",
       "2018-01-01 00:00:00           NaN  \n",
       "2018-01-01 01:00:00           NaN  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 8,
=======
     "execution_count": 3,
>>>>>>> 8ed4a11604dbfb5d3ec3242fb6e6085c85ce63c6
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prediction_data.get(1).get('h3_res_4')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, Y_train, X_test, Y_test):\n",
    "    '''\n",
    "    Method that prepares the data for training (split the data/)\n",
    "    param X: feature data to be prepared\n",
    "    param Y: target data to be prepared\n",
    "    param train_index: index that defines the split for trainig data\n",
    "    param val_index: index that defines the split for target data\n",
    "    returns X_train, Y_train, X_val, Y_val: prepared training & validation data\n",
    "    '''\n",
    "    Scaler=StandardScaler()\n",
    "    \n",
    "    X_train = Scaler.fit_transform(X_train)\n",
    "    Y_train = Scaler.fit_transform(Y_train)\n",
    "    \n",
    "    X_test = Scaler.fit_transform(X_test)\n",
    "    Y_test = Scaler.fit_transform(Y_test)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def train_nn(X_train, Y_train, X_val, Y_val, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a neural network with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   NN to be trained\n",
    "    param params:  Parameters to compile and fit the NN\n",
    "    returns:       Nothing     \n",
    "    '''\n",
    "    model.compile(\n",
    "        optimizer=params.get(\"optimizer\"),\n",
    "        loss=params.get(\"loss\"),\n",
    "        metrics=params.get(\"metrics\"),\n",
    "    )\n",
    "    model.fit(\n",
    "        x=X_train,\n",
    "        y=Y_train,\n",
    "        batch_size=params.get(\"batch_size\"),\n",
    "        epochs=params.get(\"epochs\"),\n",
    "        #verbose=params.get(\"verbose\"),\n",
    "    )\n",
    "    Y_pred=model.predict(X_val)\n",
    "    r2 = r2_score(Y_val, Y_pred)\n",
    "    MAE = mean_absolute_error(Y_val, Y_pred)\n",
    "    print(f\"R-squared {r2}\")\n",
    "    print(f\"Mean Squared Error {MAE}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_lsvr(X_train, Y_train, X_val, Y_val, model, params):\n",
    "    '''\n",
    "    This method compiles and trains a SVM with the given data and parameters\n",
    "    param X_train: Training data-set\n",
    "    param Y_train: Target variable for training\n",
    "    param X_val:   Test data-set\n",
    "    param y_val:   Target variable for validation\n",
    "    param model:   SVM to be trained\n",
    "    param params:  Parameters to train the SVM\n",
    "    returns:       Nothing    \n",
    "    '''\n",
    "    model.set_params(\n",
    "              epsilon = params.get('regressor__model__epsilon'),\n",
    "              C = params.get('regressor__model__C'),\n",
    "              max_iter = 5000\n",
    "    )\n",
    "    model.fit(X_train, \n",
    "              Y_train.reshape(len(Y_train))\n",
    "             )\n",
    "    Y_pred = model.predict(X_val)\n",
    "    r2 = r2_score(Y_val, Y_pred)\n",
    "    MAE = mean_absolute_error(Y_val, Y_pred)\n",
    "    print(f\"R-squared {r2}\")\n",
    "    print(f\"Mean Squared Error {MAE}\")\n",
    "\n",
    "\n",
    "def create_date_list(delta):\n",
    "    '''\n",
    "    This method creates a list of dates (2018-2019) \n",
    "    depending on the delta\n",
    "    params delta: determines break between dates\n",
    "    returns:      list of dates \n",
    "    '''\n",
    "    # create a startdate\n",
    "    start = pd.to_datetime(\"2018-01-01\", format=\"%Y-%m-%d\")\n",
    "    # create the enddate\n",
    "    #end = pd.to_datetime(\"2018-12-31\", format=\"%Y-%m-%d\")\n",
    "    end = pd.to_datetime(\"2018-01-20\", format=\"%Y-%m-%d\")\n",
    "    # create timedelta to increase days\n",
    "    next_date = timedelta(days=delta)\n",
    "    list_dates=[]\n",
    "    \n",
    "    while start <= end:\n",
    "        \n",
    "        # add date to list\n",
    "        list_dates.append(start)\n",
    "        # increase date by one day\n",
    "        start = start + next_date\n",
    "    list_dates.append(end)\n",
    "    \n",
    "    return list_dates\n",
    "\n",
    "\n",
    "def create_batch_split(X,Y, date_list, arr):\n",
    "    '''\n",
    "    '''\n",
    "    X_train = X.copy()\n",
    "    Y_train = Y.copy()\n",
    "    X_test = pd.DataFrame()\n",
    "    Y_test = pd.DataFrame()\n",
    "    # Sort the arr to be able to delete entries with the index\n",
    "    arr = sorted(arr, reverse=True)\n",
    "    for element in arr:\n",
    "        # extract batch\n",
    "        batch_x = X.loc[(X.index < date_list[element]) & (X.index >= date_list[element] - timedelta(days=7))]\n",
    "        batch_y = Y.loc[(Y.index < date_list[element]) & (Y.index >= date_list[element] - timedelta(days=7))]\n",
    "        # add to the test sets\n",
    "        X_test = pd.concat([batch_x, X_test])\n",
    "        Y_test = pd.concat([batch_y, Y_test])\n",
    "        # delete from the training data set\n",
    "        X_train.drop(index=batch_x.index, inplace=True)\n",
    "        Y_train.drop(index=batch_y.index, inplace=True)\n",
    "        # Delete the elements from the date_list that have been already used for the test set\n",
    "        del date_list[element]\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Train split Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_k_fold_validation(k, X, Y, model, train_model, params):\n",
    "    '''\n",
    "    Method that trains and validate the model using k-fold validation\n",
    "    However, this method can disrupt the temporal patterns and data leakage occurs \n",
    "    due to the lagged time features\n",
    "    param k:      Number of folds (iterations)\n",
    "    param x:      Feature data\n",
    "    param y:      Target data\n",
    "    param model:  Model to be trained ()\n",
    "    returns:      Nothing\n",
    "    '''\n",
    "    # initialize the folds\n",
    "    k_fold = KFold(n_splits= k, random_state=47, shuffle=True)\n",
    "    # iteratre through all folds\n",
    "    for train_index, val_index in k_fold.split(X,Y):\n",
    "        # normalize data\n",
    "        X_train, Y_train, X_val, Y_val = normalize_data(\n",
    "            X.iloc[train_index],\n",
    "            Y.iloc[train_index].values.reshape(-1,1), \n",
    "            X.iloc[val_index], \n",
    "            Y.iloc[val_index].values.reshape(-1,1)\n",
    "        )\n",
    "        # train & validate the model\n",
    "        train_model(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=model, params=params)\n",
    "\n",
    "        \n",
    "def batch_split_cross_validation(k, time_bin, X, Y, model, train_model, params):\n",
    "    '''\n",
    "    This method split the entire dataset into batches. Direct Data Leakage is avoided by disrupting\n",
    "    the chain of the lagged time feature between test and training set. Batches are cut on week and day level \n",
    "    depending on the granularity of the time bins.\n",
    "    param X:              feature data \n",
    "    param Y:              target data\n",
    "    param k_folds:        number of folds for k-fold validation\n",
    "    param time_bin:       granularity of the time bins\n",
    "    returns index_dict:   contains indicies to split for cross validation\n",
    "    '''\n",
    "    # if 24 split batches by weeks\n",
    "    if time_bin == 24:\n",
    "        date_list = create_date_list(7)    \n",
    "    # else split by days\n",
    "    else:\n",
    "        date_list = create_date_list(1)\n",
    "    # number of batches that are included in the test set\n",
    "    size_test_split = len(date_list) // k\n",
    "    for fold in range(0, k):\n",
    "        print(fold)\n",
    "        # pick random choices for the test data batches\n",
    "        if fold==k-1:\n",
    "            arr = np.random.choice(range(1, len(date_list)), len(date_list)-1, replace=False)\n",
    "        else: \n",
    "            arr = np.random.choice(range(1, len(date_list)), size_test_split, replace=False)   \n",
    "        # create training and test set with the batches\n",
    "        X_train, Y_train, X_test, Y_test = create_batch_split(X,Y, date_list, arr)\n",
    "        # normalize the data\n",
    "        X_train, Y_train, X_test, Y_test = normalize_data(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test\n",
    "        )\n",
    "        # train & validate the model\n",
    "        train_model(X_train=X_train, Y_train= Y_train, X_val=X_test, Y_val=Y_test,  model=model, params=params)\n",
    "\n",
    "\n",
    "def sliding_window_cross_validation(X,Y,n_windows, model, train_model, params):\n",
    "    '''\n",
    "    This method uses sliding window technique to the split the data. The user specifies the size of the window and\n",
    "    the model is trained sequentially with each window until the last window. Future data is used to the evaluate the\n",
    "    model. This way ensures no data leakage into the test set\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "\n",
    "def sorted_train_test_split(X, Y, test_size, model, train_model, params):\n",
    "    '''\n",
    "    '''\n",
    "    # sort the entries in ascending order\n",
    "    X.sort_index(inplace=True)\n",
    "    Y.sort_index(inplace=True)\n",
    "    # get split index\n",
    "    test_index = int(len(X)*(1-test_size))-1\n",
    "    # normalize data\n",
    "    X_train, Y_train, X_val, Y_val = normalize_data(\n",
    "        X.iloc[:test_index],\n",
    "        Y.iloc[:test_index].values.reshape(-1,1), \n",
    "        X.iloc[test_index:], \n",
    "        Y.iloc[test_index:].values.reshape(-1,1)\n",
    "    )\n",
    "    # train & validate the model\n",
    "    train_model(X_train=X_train, Y_train= Y_train, X_val=X_val, Y_val=Y_val,  model=model, params=params)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lagged_1day'].fillna(df['number_of_trips'].mean(), inplace= True)\n",
    "df['lagged_1h'].fillna(df['number_of_trips'].mean(), inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in 'Column1': 0\n",
      "Null values in 'Column1': 0\n"
     ]
    }
   ],
   "source": [
    "null_count_column1 = df['lagged_1h'].isnull().sum()\n",
    "print(f\"Null values in 'Column1': {null_count_column1}\")\n",
    "null_count_column1 = df['lagged_1day'].isnull().sum()\n",
    "print(f\"Null values in 'Column1': {null_count_column1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "encoded_data = encoder.fit_transform(df[['h3_res_4']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['h3_res_4']))\n",
    "\n",
    "df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "df = pd.concat([df, encoded_df], axis=1).drop('h3_res_4', axis=1)\n",
    "\n",
    "df.set_index(\"trip_start_timestamp\", inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature_Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['temperature', 'precipitation', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
    "       'lagged_1h', 'lagged_1day', 'h3_res_4_8426641ffffffff',\n",
    "       'h3_res_4_8426645ffffffff', 'h3_res_4_842664dffffffff',\n",
    "       'h3_res_4_8427593ffffffff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_linear = LinearSVR()\n",
    "svr_kernelized = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline_lsvr = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', svr_linear)\n",
    "])\n",
    "ttr_lsvr = TransformedTargetRegressor(\n",
    "    regressor = pipeline_lsvr,\n",
    "    transformer = scaler\n",
    ")\n",
    "pipline_ksvr = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', svr_kernelized)\n",
    "])\n",
    "ttr_ksvr = TransformedTargetRegressor(\n",
    "    regressor = pipline_ksvr,\n",
    "    transformer = scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_linear = {\n",
    "    'regressor__model__C': [0.1, 1, 10, 100],\n",
    "    'regressor__model__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "}\n",
    "\n",
    "param_grid_k = {\n",
    "    'regressor__model__C': [0.1, 1, 10, 100],\n",
    "    'regressor__model__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    #'regressor__model__kernel': ['rbf','sigmoid', 'poly'],\n",
    "    'regressor__model__kernel': ['rbf', 'sigmoid'],\n",
    "    'regressor__model__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'regressor__model__coef0': [0, 0.1, 0.5, 1],  # Relevant for 'poly' and 'sigmoid' kernels\n",
    "    #'regressor__model__degree': [2, 3, 4]  # Only relevant for 'poly' kernel\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_linear_svr = DaskGridSearchCV(\n",
    "    estimator=ttr_lsvr, \n",
    "    param_grid=param_grid_linear, \n",
    "    cv=5, \n",
    "    scoring=['r2'],\n",
    "    n_jobs=-1,\n",
    ")\n",
    "gs_ksvr = DaskGridSearchCV(\n",
    "    estimator=ttr_ksvr, \n",
    "    param_grid=param_grid_k, \n",
    "    cv=5, \n",
    "    scoring=['r2'],\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch took: 0.12870121002197266min\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with Client() as client:\n",
    "\n",
    "        start_time = time.time() # time computation time for the gridsearch\n",
    "        warnings.filterwarnings('ignore', message='Liblinear failed to converge, increase the number of iterations.')\n",
    "        gs_linear_svr.fit(df[features], df['number_of_trips'])\n",
    "        end_time = time.time()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Exception {e} occured, swtich to running only one job\")\n",
    "    gs_linear_svr.n_jobs=1\n",
    "    \n",
    "    start_time = time.time() # time computation time for the gridsearch\n",
    "    warnings.filterwarnings('ignore', message='Liblinear failed to converge, increase the number of iterations.')\n",
    "    gs_linear_svr.fit(df[features], df['number_of_trips'])\n",
    "    end_time = time.time()\n",
    "\n",
    "print(f\"GridSearch took: {(end_time-start_time)/60} minuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.29271173477173\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with Client() as client:\n",
    "\n",
    "        start_time = time.time() # time computation time for the gridsearch\n",
    "        gs_ksvr.fit(df[features], df['number_of_trips'])\n",
    "        end_time = time.time()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Exception {e} occured, swtich to running only one job)\n",
    "    gs_ksvr.n_jobs=1\n",
    "    \n",
    "    start_time = time.time() # time computation time for the gridsearch\n",
    "    gs_ksvr.fit(df[features], df['number_of_trips'])\n",
    "    end_time = time.time()\n",
    "\n",
    "print(f\"GridSearch took: {(end_time-start_time)/60} minuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with linear kernel\n",
      "best params: {'regressor__model__C': 10, 'regressor__model__epsilon': 0.1}\n",
      "best score:  0.9268351304349299\n",
      "\n",
      "Kernelized SVR\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6624\\2683987855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nKernelized SVR\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"best params: {gs_ksvr.best_params_}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"best score:  {gs_ksvr.best_score_}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\dask_ml\\model_selection\\_search.py\u001b[0m in \u001b[0;36mbest_params_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbest_params_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cv_results_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_refit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best_params_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_index_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1222\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "print(\"SVR with linear kernel\")\n",
    "print(f\"best params: {gs_linear_svr.best_params_}\")\n",
    "print(f\"best score:  {gs_linear_svr.best_score_}\")\n",
    "\n",
    "print(\"\\nKernelized SVR\")\n",
    "print(f\"best params: {gs_ksvr.best_params_}\")\n",
    "print(f\"best score:  {gs_ksvr.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best models\n",
    "if os.path.exists('Models/test.py'):\n",
    "    os.remove()\n",
    "else:\n",
    "    # save the best models\n",
    "    joblib.dump(grid_search_rbf.best_estimator_, 'Models/gs_model_svr_rbf.pkl')\n",
    "    joblib.dump(grid_search_sigmoid.best_estimator_, 'Models/gs_model_svr_sigmoid.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models with different time/spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best models and params from the gridsearch\n",
    "\n",
    "# load the best models\n",
    "#joblib.load('Models/gs_model_svr_poly.pkl')\n",
    "joblib.load('Models/gs_model_svr_rbf.pkl')\n",
    "joblib.load('Models/gs_model_svr_sigmoid.pkl')\n",
    "\n",
    "# load the best params\n",
    "#joblib.load('Models/gs_params_svr_poly.pkl')\n",
    "joblib.load('Models/gs_params_svr_rbf.pkl')\n",
    "joblib.load('Models/gs_params_svr_sigmoid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple k-fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_params_lsvr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6624\\3319639575.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Liblinear failed to converge.*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msimple_k_fold_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number_of_trips'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvr_linear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_lsvr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_params_lsvr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_params_lsvr' is not defined"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"Liblinear failed to converge.*\")\n",
    "\n",
    "simple_k_fold_validation(4, df[features], df['number_of_trips'], svr_linear, train_lsvr, params=best_params_lsvr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Split k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Window Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start End Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare the results of the different time/spaitla bin combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyize the models with XAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(12, input_shape=(12,), activation='relu'))\n",
    "nn_model.add(Dense(8, activation='relu'))\n",
    "nn_model.add(Dense(4, activation='relu'))\n",
    "nn_model.add(Dense(1))\n",
    "nn_model.save_weights('initial_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"optimizer\":\"RMSPROP\",\n",
    "    \"loss\":\"mse\",\n",
    "    \"loss_weights\":None,\n",
    "    \"metrics\":['mae'],\n",
    "    \"weighted_metrics\":None,\n",
    "    \"run_eagerly\":False,\n",
    "    #\"steps_per_execution\":1,\n",
    "    \"jit_compile\":\"auto\",\n",
    "    \"auto_scale_loss\":True,\n",
    "    \"x\":None,\n",
    "    \"y\":None,\n",
    "    \"batch_size\":50,\n",
    "    \"epochs\":100,\n",
    "    \"verbose\":0,\n",
    "    \"callbacks\":None,\n",
    "    \"validation_split\":0.0,\n",
    "    \"validation_data\":None,\n",
    "    \"shuffle\":True,\n",
    "    \"class_weight\":None,\n",
    "    \"sample_weight\":None,\n",
    "    \"initial_epoch\":0,\n",
    "    \"steps_per_epoch\":None,\n",
    "    \"validation_steps\":None,\n",
    "    #\"validation_batch_size\":None,\n",
    "    \"validation_freq\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models with different time/spatial bins and different test/train spilt techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple k-fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_model.load_weights('initial_weights')\n",
    "simple_k_fold_validation(4, df[features], df['number_of_trips'], nn_model, train_nn, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Split k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.load_weights('initial_weights')\n",
    "batch_split_cross_validation(3, 24, df[features], df[['number_of_trips']], nn_model, train_nn, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Window Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start End Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_model.load_weights('initial_weights')\n",
    "sorted_train_test_split(df[features], df['number_of_trips'], 0.1, nn_model, train_nn, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of  the different time/spaitla bin combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the different test/train split techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aanalyize the models with XAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply different NN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional NN (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent NN (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Networks (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernalized NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare SVM and NN models in terms of predictive performance and computation time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
